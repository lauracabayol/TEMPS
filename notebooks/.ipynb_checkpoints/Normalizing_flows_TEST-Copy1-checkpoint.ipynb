{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f65ab0-9aa9-4de1-bffb-2a70a5ebe152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f6f0feae-ce85-4840-ba5d-639c48d0e496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InvertedPlanarFlow(nn.Module):\n",
    "    \"\"\"Implementation of the invertible transformation used in planar flow:\n",
    "        f(z) = z + u * h(dot(w.T, z) + b)\n",
    "    See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims):\n",
    "        super(InvertedPlanarFlow, self).__init__()\n",
    "        self.n_dims=n_dims\n",
    "        \n",
    "    def _assign_params_flow(self, flow_params):\n",
    "        \n",
    "        # Extract the u, w, and b components from flow_params\n",
    "        u = flow_params[:, :self.n_dims]\n",
    "        w = flow_params[:, self.n_dims:(2* self.n_dims)]\n",
    "        b = flow_params[:, -1].unsqueeze(1)\n",
    "        \n",
    "        # Update the parameters with the new values\n",
    "        self.u = nn.Parameter(u)\n",
    "        self.w = nn.Parameter(w)\n",
    "        self.b = nn.Parameter(b)\n",
    "\n",
    "        \n",
    "    def forward(self, x, flow_params):\n",
    "        self._assign_params_flow(flow_params)\n",
    "        activation = torch.tanh(torch.mm(x, self.w.t()) + self.b)\n",
    "        flow = x + torch.mm(activation, self.u)\n",
    "        return flow\n",
    "    \n",
    "    \"\"\"def _forward_log_det_jacobian(self, x, context, flow_params):\n",
    "        u, w, b = flow_params[..., :self.n_dims], flow_params[..., self.n_dims:2*self.n_dims], flow_params[..., -1:]\n",
    "        activation = torch.tanh(torch.mm(x, w.t()) + b)\n",
    "        psi = (1 - torch.mm(w, u.t()) * activation**2) * w\n",
    "        log_det_J = torch.log(torch.abs(1 + torch.mm(psi, u.t())))\n",
    "        return log_det_J\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0a1c3-34a7-4ca3-ab7c-54ce6a5301a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InvertedPlanarFlow(nn.Module):\n",
    "    \"\"\"Implementation of the invertible transformation used in planar flow:\n",
    "        f(z) = z + u * h(dot(w.T, z) + b)\n",
    "    See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims):\n",
    "        super(InvertedPlanarFlow, self).__init__()\n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "    def _assign_params_flow(self, flow_params):\n",
    "        # Extract the u, w, and b components from flow_params\n",
    "        u = flow_params[:, :self.n_dims]\n",
    "        w = flow_params[:, self.n_dims:(2 * self.n_dims)]\n",
    "        b = flow_params[:, -1].unsqueeze(1)\n",
    "        \n",
    "        # Update the parameters with the new values\n",
    "        self.u = nn.Parameter(u)\n",
    "        self.w = nn.Parameter(w)\n",
    "        self.b = nn.Parameter(b)\n",
    "        \n",
    "    def h(self, x):\n",
    "        return torch.tanh(x)\n",
    "        \n",
    "    def forward(self, x, flow_params):\n",
    "        self._assign_params_flow(flow_params)\n",
    "        wTx_plus_b = torch.mm(x, self.w.t()) + self.b\n",
    "        activation = self.h(wTx_plus_b)\n",
    "        flow = x + torch.mm(activation, self.u)\n",
    "        \n",
    "        # Compute the Jacobian of the transformation\n",
    "        batch_size = x.size(0)\n",
    "        dim = x.size(1)\n",
    "        jacobian = torch.zeros(batch_size, dim, dim).to(x.device)\n",
    "        diag = (1 - activation ** 2) * self.w  # Diagonal of the Jacobian\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            jacobian[i] = torch.eye(dim) + torch.mm(diag[i].unsqueeze(1), self.u[i].unsqueeze(0))\n",
    "        \n",
    "        # Compute the determinant of the Jacobian using the property of the determinant of a triangular matrix\n",
    "        det_jacobian = jacobian.det()\n",
    "        \n",
    "        return flow, det_jacobian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8ad6f89b-517c-47bb-a955-c94d80a5e4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConditionalNormalizingFlow(nn.Module):\n",
    "    def __init__(self, n_dims, n_context, n_flows):\n",
    "        super(ConditionalNormalizingFlow, self).__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.n_flows = n_flows\n",
    "        self.n_context = n_context\n",
    "\n",
    "        # Create the flow layers\n",
    "        self.flows = nn.ModuleList([InvertedPlanarFlow(self.n_dims) for _ in range(n_flows)])\n",
    "\n",
    "        # Context encoder network (maps context to flow parameters)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_context, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_flows * (2 * n_dims + 1))  # u, w, b for each flow\n",
    "        )\n",
    "\n",
    "    def forward(self, x, contect_data):\n",
    "        # Compute flow parameters from the context\n",
    "        flow_params = self.encoder(contect_data).view(-1, self.n_flows, 2 * self.n_dims + 1)\n",
    "\n",
    "        # Apply each forward flow transformation\n",
    "        for i in range(self.n_flows):\n",
    "            x = self.flows[i](x,flow_params[:, i, :])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "59b89bbe-aab9-424d-be61-b49ecabbb589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "n_dims = 1\n",
    "n_context=6\n",
    "n_flows = 4\n",
    "batch_size = 32\n",
    "\n",
    "# Define the dimensionality of z\n",
    "z_dim = 1\n",
    "base_distribution = D.Normal(torch.zeros(z_dim), torch.ones(z_dim))\n",
    "\n",
    "data = torch.randn(batch_size, context_dim)\n",
    "s = torch.randn(batch_size,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3a7e8b7b-22ec-497e-a5f0-575a2f9bd590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a conditional normalizing flow\n",
    "flow_model = ConditionalNormalizingFlow(n_dims, n_context, n_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6e99b7b8-7a57-47d0-832d-ad1255c47c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7707],\n",
       "        [-2.0723],\n",
       "        [-1.9680],\n",
       "        [-1.7693],\n",
       "        [-5.3760],\n",
       "        [-1.6102],\n",
       "        [ 1.9134],\n",
       "        [-4.3363],\n",
       "        [-2.8115],\n",
       "        [-0.2120],\n",
       "        [ 1.1797],\n",
       "        [-2.3803],\n",
       "        [-6.1303],\n",
       "        [ 0.6290],\n",
       "        [-4.0855],\n",
       "        [ 1.3080],\n",
       "        [ 1.8476],\n",
       "        [-2.3908],\n",
       "        [-1.1731],\n",
       "        [ 3.3209],\n",
       "        [-5.6039],\n",
       "        [-2.3699],\n",
       "        [-2.2396],\n",
       "        [-0.2545],\n",
       "        [ 2.2664],\n",
       "        [-2.2547],\n",
       "        [ 1.7116],\n",
       "        [-2.0171],\n",
       "        [ 3.7052],\n",
       "        [-1.8661],\n",
       "        [ 1.1254],\n",
       "        [-1.6376]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_model(s,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d462c6f-db0c-4439-ac84-c41ff5704fb0",
   "metadata": {},
   "source": [
    "## second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0e937f99-22c8-4d7b-8db2-a2bba14b6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "75c1b13e-bef8-4ad1-9af5-acd09be079c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability of the sample in the base distribution:\n",
      "tensor([-90.7244])\n"
     ]
    }
   ],
   "source": [
    "# Sample from the base Gaussian distribution\n",
    "z_sample = torch.Tensor([-13.4019])\n",
    "\n",
    "# Compute log probability for a given sample\n",
    "log_prob_z = base_distribution.log_prob(z_sample)\n",
    "print(\"Log probability of the sample in the base distribution:\")\n",
    "print(log_prob_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b72d9238-ff08-4071-93e0-be175676dadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9709e-40])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(log_prob_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d6adc-0be2-45d0-8be1-adc1662c0374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26503a10-7780-485b-907c-54b03d1ee868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class InvertedPlanarTransform(nn.Module):\n",
    "    \"\"\"Implementation of the invertible transformation used in planar flow:\n",
    "        f(z) = z + u * h(dot(w.T, z) + b)\n",
    "    See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"Initialise weights and bias.\n",
    "        \n",
    "        Args:\n",
    "            dim: Dimensionality of the distribution to be estimated.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
    "        self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
    "        self.u = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            self.get_u_hat()\n",
    "\n",
    "        return z + self.u * nn.Tanh()(torch.mm(z, self.w.T) + self.b)\n",
    "    \n",
    "    def log_det_J(self, z: Tensor) -> Tensor:\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            self.get_u_hat()\n",
    "        a = torch.mm(z, self.w.T) + self.b\n",
    "        psi = (1 - nn.Tanh()(a) ** 2) * self.w\n",
    "        abs_det = (1 + torch.mm(self.u, psi.T)).abs()\n",
    "        log_det = torch.log(1e-4 + abs_det)\n",
    "\n",
    "        return log_det\n",
    "\n",
    "    def get_u_hat(self) -> None:\n",
    "        \"\"\"Enforce w^T u >= -1. When using h(.) = tanh(.), this is a sufficient condition \n",
    "        for invertibility of the transformation f(z). See Appendix A.1.\n",
    "        \"\"\"\n",
    "        wtu = torch.mm(self.u, self.w.T)\n",
    "        m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n",
    "        self.u.data = (\n",
    "            self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2\n",
    "        )\n",
    "\n",
    "    def inverse(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the inverse transformation.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Inverse transformed tensor.\n",
    "        \"\"\"\n",
    "        # Compute inverse of tanh using atanh (inverse hyperbolic tangent)\n",
    "        inverse_h = torch.atanh((x - self.u) / torch.mm(self.w, self.w.T))\n",
    "\n",
    "        # Compute the inverse transformation\n",
    "        inverse_z = torch.mm(inverse_h - self.b, torch.inverse(self.w))\n",
    "\n",
    "        return inverse_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9e463-5c22-428c-8548-08182fb77c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConditionalPlanarFlowModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, target_dim=1, num_flows=1):\n",
    "        super(ConditionalPlanarFlowModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.target_dim = target_dim\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        # Create a list to hold the InvertedPlanarTransform instances\n",
    "        self.flow_list = nn.ModuleList([InvertedPlanarTransform(dim=input_dim) for _ in range(num_flows)])\n",
    "\n",
    "        # Linear layer to predict the CPD for the target variable\n",
    "        self.linear_layer = nn.Linear(input_dim, target_dim)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        # Apply all the InvertedPlanarTransform instances sequentially\n",
    "        transformed_features = input_features\n",
    "        for flow in self.flow_list:\n",
    "            transformed_features = flow(transformed_features)\n",
    "\n",
    "        # Predict the CPD for the target variable\n",
    "        predicted_cpd = self.linear_layer(transformed_features)\n",
    "\n",
    "        return predicted_cpd\n",
    "\n",
    "\n",
    "def train_model(model, input_data, target_data, num_epochs=100, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_cpd = model(input_data)\n",
    "        loss = criterion(predicted_cpd, target_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Step 5: Inference\n",
    "def predict_cpd(model, input_features):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        transformed_features = model.feature_transform.inverse(input_features)\n",
    "        predicted_cpd = model.linear_layer(transformed_features)\n",
    "    return predicted_cpd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbcc9d-8661-41e6-a3e8-c31441e0491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define your loss function (e.g., negative log-likelihood)\n",
    "criterion = nn.NLLLoss()  # Use a suitable loss function based on the nature of your CPD\n",
    "\n",
    "# Step 4: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92ade13-b05c-44d2-a5fb-7cbe61dc63e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples=10\n",
    "input_dim=6\n",
    "target_dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa8e26a-007e-461c-8fb9-e4cd103e363a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have your input_data and target_data as torch tensors:\n",
    "input_data = torch.randn(num_samples, input_dim)  # Replace with your actual data\n",
    "target_data = torch.randn(num_samples, target_dim)  # Replace with your actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802c6a15-5109-4ad5-ad42-bec6dc3dd838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, input_data, target_data, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m predicted_cpd \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[0;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_cpd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/nn/modules/loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/nn/functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "train_model(model, input_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7296cbf5-10f7-4dc6-a30a-96bf70b7e577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m target_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_samples, target_dim)  \u001b[38;5;66;03m# Replace with your actual data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 2: Create the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mConditionalPlanarFlowModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 4: Train the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_model(model, input_data, target_data)\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36mConditionalPlanarFlowModel.__init__\u001b[0;34m(self, input_dim, target_dim)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dim \u001b[38;5;241m=\u001b[39m target_dim\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Feature transformation using InvertedPlanarTransform\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_transform \u001b[38;5;241m=\u001b[39m \u001b[43mInvertedPlanarTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Linear layer to predict the CPD for the target variable\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(input_dim, target_dim)\n",
      "Cell \u001b[0;32mIn[34], line 19\u001b[0m, in \u001b[0;36mInvertedPlanarTransform.__init__\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialise weights and bias.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    dim: Dimensionality of the distribution to be estimated.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, dim)\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: randn(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2"
     ]
    }
   ],
   "source": [
    "# Assuming you have your input_data and target_data as torch tensors:\n",
    "input_data = torch.randn(num_samples, input_dim)  # Replace with your actual data\n",
    "target_data = torch.randn(num_samples, target_dim)  # Replace with your actual data\n",
    "\n",
    "# Step 2: Create the model\n",
    "\n",
    "# Step 4: Train the model\n",
    "\n",
    "\n",
    "# Step 5: Predict the CPD for new input features\n",
    "new_input_features = torch.randn(num_samples, input_dim)  # Replace with your new data\n",
    "predicted_cpd = predict_cpd(model, new_input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe0b227d-9aa5-4c99-91a4-0e558795328c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
       "\n",
       "\n",
       "Returns a tensor filled with random numbers from a normal distribution\n",
       "with mean `0` and variance `1` (also called the standard normal\n",
       "distribution).\n",
       "\n",
       ".. math::\n",
       "    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
       "\n",
       "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
       "\n",
       "Args:\n",
       "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
       "        Can be a variable number of arguments or a collection like a list or tuple.\n",
       "\n",
       "Keyword args:\n",
       "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
       "    out (Tensor, optional): the output tensor.\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
       "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
       "        Default: ``torch.strided``.\n",
       "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
       "        Default: if ``None``, uses the current device for the default tensor type\n",
       "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
       "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "    pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
       "        the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.randn(4)\n",
       "    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
       "    >>> torch.randn(2, 3)\n",
       "    tensor([[ 1.5954,  2.8929, -1.0923],\n",
       "            [ 1.1719, -0.4709, -0.1996]])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e45c5-79fa-43bc-b4a5-3bb633880230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9af6a-e0e9-4171-b46f-ca21255e253d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd133-21b6-40b1-a264-c1ba88cc8b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8c8a08-854d-44f7-83eb-23a6a9e0cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, dim: int = 2, K: int = 6):\n",
    "        \"\"\"Make a planar flow by stacking planar transformations in sequence.\n",
    "\n",
    "        Args:\n",
    "            dim: Dimensionality of the distribution to be estimated.\n",
    "            K: Number of transformations in the flow. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = [InvertedPlanarTransform(dim) for _ in range(K)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n",
    "        log_det_J = 0\n",
    "\n",
    "        for layer in self.layers:\n",
    "            log_det_J += layer.log_det_J(z)\n",
    "            z = layer(z)\n",
    "\n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "789351d3-262f-43b8-8132-89d053681ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PlanarFlow(dim=2, K=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf722c0-6d93-437e-9679-1a4704831d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlanarFlow(\n",
       "  (model): Sequential(\n",
       "    (0): InvertedPlanarTransform()\n",
       "    (1): InvertedPlanarTransform()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a9b31f-f3e3-49d7-8b1a-6cfe426bbbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m identity_flow \u001b[38;5;241m=\u001b[39m \u001b[43mIdentityFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EUCLID/INSIGHT/NF/flows/IdentityFlow.py:14\u001b[0m, in \u001b[0;36mIdentityFlow.__init__\u001b[0;34m(self, params, n_dims, name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, n_dims, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdentityFlow\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    :param params: shape (?, 1), this will become alpha and define the slow of ReLU for x < 0\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    :param n_dims: Dimension of the distribution that's being transformed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIdentityFlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d50e1e-5adc-4bdd-87cb-32c78c883b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "planar_flow = InvertedPlanarFlow(params=torch.zeros((1, 2 * 1 + 1)), n_dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb981f0f-113e-45f2-a027-2e003065bdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvertedPlanarFlow()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planar_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74688079-9a3f-4f87-b93f-4a191325e6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29cd37-0b15-4cf9-a55c-da73f38d67ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9fbda-6377-4df2-9caa-3fe741ec3bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d73f2-51a8-4775-850a-da4981fd7d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1bd67f-8253-4ebd-b69e-16a1418c6e31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 07:26:44.817779: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-03 07:26:47.725659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 07:27:10.847442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-03 07:27:53.690091: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class BaseEstimator(tf.keras.Sequential):\n",
    "    x_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "    y_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def __init__(self, layers, noise_fn_type=\"fixed_rate\", noise_scale_factor=0.0, random_seed=22):\n",
    "        tf.random.set_seed(random_seed)\n",
    "        self.noise_fn_type = noise_fn_type\n",
    "        self.noise_scale_factor = noise_scale_factor\n",
    "\n",
    "        super().__init__(layers)\n",
    "\n",
    "    def fit(self, x, y, batch_size=None, epochs=None, verbose=1, **kwargs):\n",
    "        self._assign_data_normalization(x, y)\n",
    "        assert len(x.shape) == len(y.shape) == 2, \"Please pass a matrix not a vector\"\n",
    "        self._assign_noise_regularisation(n_dims=x.shape[1] + y.shape[1], n_datapoints=x.shape[0])\n",
    "        super().fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            callbacks=[tf.keras.callbacks.TerminateOnNaN()],\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _assign_noise_regularisation(self, n_dims, n_datapoints):\n",
    "        assert self.noise_fn_type in [\"rule_of_thumb\", \"fixed_rate\"]\n",
    "        if self.noise_fn_type == \"rule_of_thumb\":\n",
    "            noise_std = self.noise_scale_factor * (n_datapoints + 1) ** (-1 / (4 + n_dims))\n",
    "            self.x_noise_std.assign(noise_std)\n",
    "            self.y_noise_std.assign(noise_std)\n",
    "        elif self.noise_fn_type == \"fixed_rate\":\n",
    "            self.x_noise_std.assign(self.noise_scale_factor)\n",
    "            self.y_noise_std.assign(self.noise_scale_factor)\n",
    "\n",
    "    def score(self, x_data, y_data):\n",
    "        x_data = x_data.astype(np.float32)\n",
    "        y_data = y_data.astype(np.float32)\n",
    "        nll = self._get_neg_log_likelihood()\n",
    "        return -nll(y_data, self.call(x_data, training=False)).numpy().mean()\n",
    "\n",
    "    def _assign_data_normalization(self, x, y):\n",
    "        self.x_mean = np.mean(x, axis=0, dtype=np.float32)\n",
    "        self.y_mean = np.mean(y, axis=0, dtype=np.float32)\n",
    "        self.x_std = np.std(x, axis=0, dtype=np.float32)\n",
    "        self.y_std = np.std(y, axis=0, dtype=np.float32)\n",
    "\n",
    "    def _get_neg_log_likelihood(self):\n",
    "        y_input_model = self._get_input_model()\n",
    "        return lambda y, p_y: -p_y.log_prob(y_input_model(y)) + tf.reduce_sum(\n",
    "            tf.math.log(self.y_std)\n",
    "        )\n",
    "\n",
    "    def _get_input_model(self):\n",
    "        y_input_model = tf.keras.Sequential()\n",
    "        # add data normalization layer\n",
    "        y_input_model.add(\n",
    "            tf.keras.layers.Lambda(lambda y: (y - tf.ones_like(y) * self.y_mean) / self.y_std)\n",
    "        )\n",
    "        # noise will be switched on during training and switched off otherwise automatically\n",
    "        y_input_model.add(tf.keras.layers.GaussianNoise(self.y_noise_std))\n",
    "        return y_input_model\n",
    "\n",
    "    def pdf(self, x, y):\n",
    "        assert x.shape == y.shape\n",
    "        output = self(x)\n",
    "        y_circ = (y - tf.ones_like(y) * self.y_mean) / self.y_std\n",
    "        return output.prob(y_circ) / tf.reduce_prod(self.y_std)\n",
    "\n",
    "    def log_pdf(self, x, y):\n",
    "        x = x.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        assert x.shape == y.shape\n",
    "\n",
    "        output = self(x)\n",
    "        assert output.event_shape == y.shape[-1]\n",
    "\n",
    "        y_circ = (y - tf.ones_like(y) * self.y_mean) / self.y_std\n",
    "        return output.log_prob(y_circ) - tf.reduce_sum(tf.math.log(self.y_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84797c3a-3973-40d8-991d-cf375a88a416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python import tf2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "if not tf2.enabled():\n",
    "    import tensorflow.compat.v2 as tf\n",
    "\n",
    "    tf.enable_v2_behavior()\n",
    "    assert tf2.enabled()\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class MeanFieldLayer(tfp.layers.DistributionLambda):\n",
    "    def __init__(self, n_dims, scale=None, map_mode=False, dtype=None):\n",
    "        \"\"\"\n",
    "        A subclass of Distribution Lambda. A layer that uses it's input to parametrize n_dims-many indepentent normal\n",
    "        distributions (aka mean field)\n",
    "        Requires input size n_dims for fixed scale, 2*n_dims for trainable scale\n",
    "        Mean Field also works for scalars\n",
    "        The input tensors for this layer should be initialized to Zero for a standard normal distribution\n",
    "        :param n_dims: Dimension of the distribution that's being output by the Layer\n",
    "        :param scale: (float) None if scale should be trainable. If not None, specifies the fixed scale of the\n",
    "            independent normals. If map mode is activated, this is ignored and set to 1.0\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.scale = scale\n",
    "\n",
    "        if map_mode:\n",
    "            self.scale = 1.0\n",
    "        convert_ttf = tfd.Distribution.mean if map_mode else tfd.Distribution.sample\n",
    "\n",
    "        make_dist_fn = self._get_distribution_fn(self.n_dims, self.scale)\n",
    "\n",
    "        super().__init__(\n",
    "            make_distribution_fn=make_dist_fn, convert_to_tensor_fn=convert_ttf, dtype=dtype\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_distribution_fn(n_dims, scale=None):\n",
    "        if scale is None:\n",
    "\n",
    "            def dist_fn(t):\n",
    "                assert t.shape[-1] == 2 * n_dims\n",
    "                return tfd.Independent(\n",
    "                    tfd.Normal(\n",
    "                        loc=t[..., 0:n_dims],\n",
    "                        scale=1e-3\n",
    "                        + tf.nn.softplus(\n",
    "                            tf.math.log(tf.math.expm1(1.0)) + 0.05 * t[..., n_dims : 2 * n_dims]\n",
    "                        ),\n",
    "                    ),\n",
    "                    reinterpreted_batch_ndims=1,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            assert scale > 0.0\n",
    "\n",
    "            def dist_fn(t):\n",
    "                assert t.shape[-1] == n_dims\n",
    "                return tfd.Independent(\n",
    "                    tfd.Normal(loc=t[..., 0:n_dims], scale=scale), reinterpreted_batch_ndims=1\n",
    "                )\n",
    "\n",
    "        return dist_fn\n",
    "\n",
    "    def get_total_param_size(self):\n",
    "        return 2 * self.n_dims if self.scale is None else self.n_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af8bca53-9191-4f66-aa78-64747187df98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class AffineFlow(tfp.experimental.joint_distribution_layers.Affine):\n",
    "    def __init__(self, t, n_dims, name=\"AffineFlow\"):\n",
    "        assert t.shape[-1] == 2 * n_dims\n",
    "        super(AffineFlow, self).__init__(\n",
    "            shift=t[..., 0:n_dims], scale_diag=1.0 + t[..., n_dims : 2 * n_dims], name=name\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(n_dims):\n",
    "        \"\"\"\n",
    "        :param n_dims:  The dimension of the distribution to be transformed by the flow\n",
    "        :return: (int) The dimension of the parameter space for the flow\n",
    "        \"\"\"\n",
    "        return 2 * n_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5561b844-a362-4b0c-8ad2-002c68cff427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'out_units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_radial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 46\u001b[0m, in \u001b[0;36mtest_radial\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_radial\u001b[39m():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mflow_dimension_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mradial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 17\u001b[0m, in \u001b[0;36mflow_dimension_testing\u001b[0;34m(flow_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)), dim)\n\u001b[1;32m     16\u001b[0m flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n\u001b[0;32m---> 17\u001b[0m reference \u001b[38;5;241m=\u001b[39m \u001b[43mAffineFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_tensors \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim], [[\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim] \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mforward_min_event_ndims \u001b[38;5;241m==\u001b[39m reference\u001b[38;5;241m.\u001b[39mforward_min_event_ndims\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py:342\u001b[0m, in \u001b[0;36m_DistributionMeta.__new__.<locals>.wrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to have things set in `self` before `__init__` is\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# called, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m \u001b[43mdefault_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to override things set in `self` by subclass\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# `__init__`, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m   \u001b[38;5;66;03m# We prefer subclasses will set `parameters = dict(locals())` because\u001b[39;00m\n\u001b[1;32m    347\u001b[0m   \u001b[38;5;66;03m# this has nearly zero overhead. However, failing to do this, we will\u001b[39;00m\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;66;03m# resolve the input arguments dynamically and only when needed.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m, in \u001b[0;36mAffineFlow.__init__\u001b[0;34m(self, t, n_dims, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, n_dims, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAffineFlow\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n_dims\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_diag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/decorator.py:231\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m--> 231\u001b[0m         args, kw \u001b[38;5;241m=\u001b[39m \u001b[43mfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/decorator.py:203\u001b[0m, in \u001b[0;36mfix\u001b[0;34m(args, kwargs, sig)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfix\u001b[39m(args, kwargs, sig):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Fix args and kwargs to be consistent with the signature\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     ba \u001b[38;5;241m=\u001b[39m \u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     ba\u001b[38;5;241m.\u001b[39mapply_defaults()  \u001b[38;5;66;03m# needed for test_dan_schult\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ba\u001b[38;5;241m.\u001b[39margs, ba\u001b[38;5;241m.\u001b[39mkwargs\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/inspect.py:3185\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/inspect.py:3100\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3098\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3099\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 3100\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3102\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[1;32m   3103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'out_units'"
     ]
    }
   ],
   "source": [
    "test_radial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727a821f-0fc9-4ec9-8ba1-9451c3e2bda7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class PlanarFlow(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Implements a bijector x = y + u * tanh(w_t * y + b)\n",
    "\n",
    "    Args:\n",
    "        params: Tensor shape (?, 2*n_dims+1). This will be split into the parameters\n",
    "            u (?, n_dims), w (?, n_dims), b (?, 1).\n",
    "            Furthermore u will be constrained to assure the invertability of the flow\n",
    "        n_dims: The dimension of the distribution that will be transformed\n",
    "        name: The name to give this particular flow\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _u, _w, _b = None, None, None\n",
    "\n",
    "    def __init__(self, t, n_dims, name=\"Inverted_Planar_Flow\"):\n",
    "        super().__init__(validate_args=False, name=name, inverse_min_event_ndims=1)\n",
    "        assert t.shape[-1] == 2 * n_dims + 1\n",
    "        u, w, b = (\n",
    "            t[..., 0:n_dims],\n",
    "            # initialize w to 1.0\n",
    "            t[..., n_dims : 2 * n_dims] + 1,\n",
    "            t[..., 2 * n_dims : 2 * n_dims + 1],\n",
    "        )\n",
    "\n",
    "        # constrain u before assigning it\n",
    "        self._u = self._u_circ(u, w)\n",
    "        self._w = w\n",
    "        self._b = b\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(n_dims):\n",
    "        \"\"\"\n",
    "        :param n_dims: The dimension of the distribution to be transformed by the flow\n",
    "        :return: (int array) The dimension of the parameter space for this flow, n_dims + n_dims + 1\n",
    "        \"\"\"\n",
    "        return n_dims + n_dims + 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _u_circ(u, w):\n",
    "        \"\"\"\n",
    "        To ensure invertibility of the flow, the following condition needs to hold: w_t * u >= -1\n",
    "        :return: The transformed u\n",
    "        \"\"\"\n",
    "        wtu = tf.math.reduce_sum(w * u, 1, keepdims=True)\n",
    "        # add constant to make it more numerically stable\n",
    "        m_wtu = -1.0 + tf.nn.softplus(wtu) + 1e-5\n",
    "        norm_w_squared = tf.math.reduce_sum(w ** 2, 1, keepdims=True) + 1e-9\n",
    "        return u + (m_wtu - wtu) * (w / norm_w_squared)\n",
    "\n",
    "    def _wzb(self, z):\n",
    "        \"\"\"\n",
    "        Computes w_t * z + b\n",
    "        \"\"\"\n",
    "        return tf.math.reduce_sum(self._w * z, 1, keepdims=True) + self._b\n",
    "\n",
    "    @staticmethod\n",
    "    def _der_tanh(z):\n",
    "        \"\"\"\n",
    "        Computes the derivative of hyperbolic tangent\n",
    "        \"\"\"\n",
    "        return 1.0 - tf.math.tanh(z) ** 2\n",
    "\n",
    "    def _forward(self, z):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the bijector\n",
    "        \"\"\"\n",
    "        return z + self._u * tf.math.tanh(self._wzb(z))\n",
    "\n",
    "    def _forward_log_det_jacobian(self, z):\n",
    "        \"\"\"\n",
    "        Computes the ln of the absolute determinant of the jacobian\n",
    "        \"\"\"\n",
    "        psi = self._der_tanh(self._wzb(z)) * self._w\n",
    "        det_grad = 1.0 + tf.math.reduce_sum(self._u * psi, 1)\n",
    "        return tf.math.log(tf.math.abs(det_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3aa80cb-8d33-429a-bf44-17228f745289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class RadialFlow(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Implements a bijector x = y + (alpha * beta * (y - y_0)) / (alpha + abs(y - y_0)).\n",
    "    Args:\n",
    "        params: Tensor shape (?, n_dims+2). This will be split into the parameters\n",
    "            alpha (?, 1), beta (?, 1), gamma (?, n_dims).\n",
    "            Furthermore alpha will be constrained to assure the invertability of the flow\n",
    "        n_dims: The dimension of the distribution that will be transformed\n",
    "        name: The name to give this particular flow\n",
    "    \"\"\"\n",
    "\n",
    "    _alpha = None\n",
    "    _beta = None\n",
    "    _gamma = None\n",
    "\n",
    "    def __init__(self, t, n_dims, name='RadialFlow'):\n",
    "        super().__init__(validate_args=False, name=name, inverse_min_event_ndims=1)\n",
    "\n",
    "        assert t.shape[-1] == n_dims + 2\n",
    "        alpha = t[..., 0:1]\n",
    "        beta = t[..., 1:2]\n",
    "        gamma = t[..., 2 : n_dims + 2]\n",
    "\n",
    "        # constraining the parameters before they are assigned to ensure invertibility.\n",
    "        # slightly shift alpha, softplus(zero centered input - 2) = small\n",
    "        self._alpha = self._alpha_circ(0.3 * alpha - 2.0)\n",
    "        # slightly shift beta, softplus(zero centered input + ln(e - 1)) = 0\n",
    "        self._beta = self._beta_circ(0.1 * beta + tf.math.log(tf.math.expm1(1.0)))\n",
    "        self._gamma = gamma\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(n_dims):\n",
    "        \"\"\"\n",
    "        :param n_dims:  The dimension of the distribution to be transformed by the flow\n",
    "        :return: (int) The dimension of the parameter space for the flow\n",
    "        \"\"\"\n",
    "        return 1 + 1 + n_dims\n",
    "\n",
    "    def _r(self, z):\n",
    "        return tf.math.reduce_sum(tf.abs(z - self._gamma), 1, keepdims=True)\n",
    "\n",
    "    def _h(self, r):\n",
    "        return 1.0 / (self._alpha + r)\n",
    "\n",
    "    def _forward(self, z):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the bijector\n",
    "        \"\"\"\n",
    "        r = self._r(z)\n",
    "        h = self._h(r)\n",
    "        return z + (self._alpha * self._beta * h) * (z - self._gamma)\n",
    "\n",
    "    def _forward_log_det_jacobian(self, z):\n",
    "        \"\"\"\n",
    "        Computes the ln of the absolute determinant of the jacobian\n",
    "        \"\"\"\n",
    "        r = self._r(z)\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(r)\n",
    "            h = self._h(r)\n",
    "        der_h = g.gradient(h, r)\n",
    "        ab = self._alpha * self._beta\n",
    "        det = (1.0 + ab * h) ** (self.n_dims - 1) * (1.0 + ab * h + ab * der_h * r)\n",
    "        det = tf.squeeze(det, axis=-1)\n",
    "        return tf.math.log(det)\n",
    "\n",
    "    @staticmethod\n",
    "    def _alpha_circ(alpha):\n",
    "        \"\"\"\n",
    "        Method for constraining the alpha parameter to meet the invertibility requirements\n",
    "        \"\"\"\n",
    "        return tf.nn.softplus(alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def _beta_circ(beta):\n",
    "        \"\"\"\n",
    "        Method for constraining the beta parameter to meet the invertibility requirements\n",
    "        \"\"\"\n",
    "        return tf.nn.softplus(beta) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f74ef9a1-83b6-45ba-ac25-5f0f5ec5665c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class BayesianNNEstimator(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dist_layer,\n",
    "        kl_weight_scale,\n",
    "        kl_use_exact=True,\n",
    "        hidden_sizes=(10,),\n",
    "        activation=\"tanh\",\n",
    "        learning_rate=3e-2,\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        trainable_prior=False,\n",
    "        map_mode=False,\n",
    "        prior_scale=1.0,\n",
    "        random_seed=22,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A bayesian net parametrizing a normalizing flow distribution\n",
    "        :param dist_layer: A Tfp Distribution Lambda Layer that converts the neural net output into a distribution\n",
    "        :param kl_weight_scale: Scales how much KL(posterior|prior) influences the loss\n",
    "        :param hidden_sizes: size and depth of net\n",
    "        :param noise_reg: Tuple with (type_of_reg, scale_factor)\n",
    "        :param trainable_prior: empirical bayes\n",
    "        :param map_mode: If true, will use the mean of the posterior instead of a sample. Default False\n",
    "        :param prior_scale: The scale of the zero centered priors\n",
    "\n",
    "        A note on kl_weight_scale: Keras calculates the loss per sample and not for the full dataset. Therefore,\n",
    "        we need to scale the KL(q||p) loss down to a single sample, which means setting kl_weight_scale = 1/n_datapoints\n",
    "        \"\"\"\n",
    "        self.x_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "        self.y_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "        self.map_mode = map_mode\n",
    "\n",
    "        posterior = self._get_posterior_fn(map_mode=map_mode)\n",
    "        prior = self._get_prior_fn(trainable_prior, prior_scale)\n",
    "        dense_layers = self._get_dense_layers(\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=dist_layer.get_total_param_size(),\n",
    "            posterior=posterior,\n",
    "            prior=prior,\n",
    "            kl_weight_scale=kl_weight_scale,\n",
    "            kl_use_exact=kl_use_exact,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            dense_layers + [dist_layer],\n",
    "            noise_fn_type=noise_reg[0],\n",
    "            noise_scale_factor=noise_reg[1],\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "\n",
    "        self.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate), loss=self._get_neg_log_likelihood()\n",
    "        )\n",
    "\n",
    "    def score(self, x_data, y_data):\n",
    "        x_data = x_data.astype(np.float32)\n",
    "        y_data = y_data.astype(np.float32)\n",
    "\n",
    "        scores = None\n",
    "        nll = self._get_neg_log_likelihood()\n",
    "        posterior_draws = 1 if self.map_mode else 50\n",
    "        for _ in range(posterior_draws):\n",
    "            res = tf.expand_dims(-nll(y_data, self.call(x_data, training=False)), axis=0)\n",
    "            scores = res if scores is None else tf.concat([scores, res], axis=0)\n",
    "        logsumexp = tf.math.reduce_logsumexp(scores, axis=0).numpy() - np.log(posterior_draws)\n",
    "        return logsumexp.mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_prior_fn(trainable=False, prior_scale=1.0):\n",
    "        def prior_fn(kernel_size, bias_size=0, dtype=None):\n",
    "            size = kernel_size + bias_size\n",
    "            layers = [\n",
    "                tfp.layers.VariableLayer(\n",
    "                    shape=size, initializer=\"zeros\", dtype=dtype, trainable=trainable\n",
    "                ),\n",
    "                MeanFieldLayer(size, scale=prior_scale, map_mode=False, dtype=dtype),\n",
    "            ]\n",
    "            return tf.keras.Sequential(layers)\n",
    "\n",
    "        return prior_fn\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_posterior_fn(map_mode=False):\n",
    "        def posterior_fn(kernel_size, bias_size=0, dtype=None):\n",
    "            size = kernel_size + bias_size\n",
    "            layers = [\n",
    "                tfp.layers.VariableLayer(\n",
    "                    size if map_mode else 2 * size,\n",
    "                    initializer=\"normal\",\n",
    "                    dtype=dtype,\n",
    "                    trainable=True,\n",
    "                ),\n",
    "                MeanFieldLayer(size, scale=None, map_mode=map_mode, dtype=dtype),\n",
    "            ]\n",
    "            return tf.keras.Sequential(layers)\n",
    "\n",
    "        return posterior_fn\n",
    "\n",
    "    def _get_dense_layers(\n",
    "        self,\n",
    "        hidden_sizes,\n",
    "        output_size,\n",
    "        posterior,\n",
    "        prior,\n",
    "        kl_weight_scale=1.0,\n",
    "        kl_use_exact=True,\n",
    "        activation=\"relu\",\n",
    "    ):\n",
    "        assert type(hidden_sizes) == tuple or type(hidden_sizes) == list\n",
    "        assert kl_weight_scale <= 1.0\n",
    "\n",
    "        # these values are assigned once fit is called\n",
    "        normalization = [tf.keras.layers.Lambda(lambda x: (x - self.x_mean) / (self.x_std + 1e-8))]\n",
    "        noise_reg = [tf.keras.layers.GaussianNoise(self.x_noise_std)]\n",
    "        hidden = [\n",
    "            tfp.layers.DenseVariational(\n",
    "                units=size,\n",
    "                make_posterior_fn=posterior,\n",
    "                make_prior_fn=prior,\n",
    "                kl_weight=kl_weight_scale,\n",
    "                kl_use_exact=kl_use_exact,\n",
    "                activation=activation,\n",
    "            )\n",
    "            for size in hidden_sizes\n",
    "        ]\n",
    "        output = [\n",
    "            tfp.layers.DenseVariational(\n",
    "                units=output_size,\n",
    "                make_posterior_fn=posterior,\n",
    "                make_prior_fn=prior,\n",
    "                kl_weight=kl_weight_scale,\n",
    "                kl_use_exact=kl_use_exact,\n",
    "                activation=\"linear\",\n",
    "            )\n",
    "        ]\n",
    "        return normalization + noise_reg + hidden + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881c0b13-71fc-4a27-88a6-590de016cdea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InverseNormalizingFlowLayer(tfp.layers.DistributionLambda):\n",
    "    _flow_types = None\n",
    "    _trainable_base_dist = None\n",
    "    _n_dims = None\n",
    "\n",
    "    def __init__(self, flow_types, n_dims, trainable_base_dist=False):\n",
    "        \"\"\"\n",
    "        Subclass of a DistributionLambda. A layer that uses it's input to parametrize a normalizing flow\n",
    "        that transforms a base normal distribution. The Normalizing flows are inverted to enable fast likelihood\n",
    "        calculation of externally provided data. This is useful for density estimation.\n",
    "        As a result, sampling from this layer is not possible.\n",
    "        This layer does not work for scalars!\n",
    "        :param flow_types: Types of flows to use, applied in order from base_dist -> transformed_dist\n",
    "        :param n_dims: dimension of the underlying distribution being transformed\n",
    "        :param trainable_base_dist: whether the base normal distribution should have trainable loc and scale diag\n",
    "        \"\"\"\n",
    "        assert all([flow_type in FLOWS for flow_type in flow_types])\n",
    "\n",
    "        self._flow_types = flow_types\n",
    "        self._trainable_base_dist = trainable_base_dist\n",
    "        self._n_dims = n_dims\n",
    "\n",
    "        # as keras transforms tensors, this layer needs to have an tensor-like output\n",
    "        # therefore a function needs to be provided that transforms a distribution into a tensor\n",
    "        # per default the .sample() function is used, but our reversed flows cannot perform that operation\n",
    "        convert_ttfn = lambda d: tf.constant([0.0])\n",
    "        make_flow_dist = self._get_distribution_fn(n_dims, flow_types, trainable_base_dist)\n",
    "        super().__init__(make_distribution_fn=make_flow_dist, convert_to_tensor_fn=convert_ttfn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_distribution_fn(n_dims, flow_types, trainable_base_dist):\n",
    "        return lambda t: tfd.TransformedDistribution(\n",
    "            distribution=InverseNormalizingFlowLayer._get_base_dist(\n",
    "                t, n_dims, trainable_base_dist\n",
    "            ),\n",
    "            bijector=tfp.bijectors.Invert(\n",
    "                InverseNormalizingFlowLayer._get_bijector(\n",
    "                    (t[..., 2 * n_dims :] if trainable_base_dist else t), flow_types, n_dims\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_total_param_size(self):\n",
    "        \"\"\"\n",
    "        :return: The total number of parameters to specify this distribution\n",
    "        \"\"\"\n",
    "        num_flow_params = sum(\n",
    "            [FLOWS[flow_type].get_param_size(self._n_dims) for flow_type in self._flow_types]\n",
    "        )\n",
    "        base_dist_params = 2 * self._n_dims if self._trainable_base_dist else 0\n",
    "        return num_flow_params + base_dist_params\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_bijector(t, flow_types, n_dims):\n",
    "        # intuitively, we want to flows to go from base_dist -> transformed dist\n",
    "        flow_types = list(reversed(flow_types))\n",
    "        param_sizes = [FLOWS[flow_type].get_param_size(n_dims) for flow_type in flow_types]\n",
    "        assert sum(param_sizes) == t.shape[-1]\n",
    "        split_beginnings = [sum(param_sizes[0:i]) for i in range(len(param_sizes))]\n",
    "        chain = [\n",
    "            FLOWS[flow_type](t[..., begin : begin + size], n_dims)\n",
    "            for begin, size, flow_type in zip(split_beginnings, param_sizes, flow_types)\n",
    "        ]\n",
    "        return tfp.bijectors.Chain(chain)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_base_dist(t, n_dims, trainable):\n",
    "        if trainable:\n",
    "            return tfd.MultivariateNormalDiag(\n",
    "                loc=t[..., 0:n_dims],\n",
    "                scale_diag=1e-3\n",
    "                + tf.math.softplus(\n",
    "                    tf.math.log(tf.math.expm1(1.0)) + 0.1 * t[..., n_dims : 2 * n_dims]\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # we still need to know the batch size, therefore we need t for reference\n",
    "            return tfd.MultivariateNormalDiag(\n",
    "                loc=tf.zeros_like(t[..., 0:n_dims]), scale_diag=tf.ones_like(t[..., 0:n_dims])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b62f5ad-f94a-4a4d-955a-76f75adaee76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class BayesNormalizingFlowNetwork(BayesianNNEstimator):\n",
    "    def __init__(self, n_dims, kl_weight_scale, n_flows=2, trainable_base_dist=True, **kwargs):\n",
    "        \"\"\"\n",
    "        A bayesian net parametrizing a normalizing flow distribution\n",
    "        :param n_dims: The dimension of the output distribution\n",
    "        :param kl_weight_scale: Scales how much KL(posterior|prior) influences the loss\n",
    "        :param n_flows: The number of flows to use\n",
    "        :param hidden_sizes: size and depth of net\n",
    "        :param trainable_base_dist: whether to train the base normal dist\n",
    "        :param noise_reg: Tuple with (type_of_reg, scale_factor)\n",
    "        :param trainable_prior: empirical bayes\n",
    "        :param map_mode: If true, will use the mean of the posterior instead of a sample. Default False\n",
    "        :param prior_scale: The scale of the zero centered priors\n",
    "\n",
    "        A note on kl_weight_scale: Keras calculates the loss per sample and not for the full dataset. Therefore,\n",
    "        we need to scale the KL(q||p) loss down to a single sample, which means setting kl_weight_scale = 1/n_datapoints\n",
    "        \"\"\"\n",
    "        dist_layer = InverseNormalizingFlowLayer(\n",
    "            flow_types=[\"radial\"] * n_flows, n_dims=n_dims, trainable_base_dist=trainable_base_dist\n",
    "        )\n",
    "        super().__init__(dist_layer, kl_weight_scale, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_function(\n",
    "        n_dims,\n",
    "        kl_weight_scale,\n",
    "        n_flows=2,\n",
    "        trainable_base_dist=True,\n",
    "        kl_use_exact=True,\n",
    "        hidden_sizes=(10,),\n",
    "        activation=\"tanh\",\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        learning_rate=2e-2,\n",
    "        trainable_prior=False,\n",
    "        map_mode=False,\n",
    "        prior_scale=1.0,\n",
    "    ):\n",
    "        # this is necessary, else there'll be processes hanging around hogging memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        return BayesNormalizingFlowNetwork(\n",
    "            n_dims=n_dims,\n",
    "            kl_weight_scale=kl_weight_scale,\n",
    "            n_flows=n_flows,\n",
    "            trainable_base_dist=trainable_base_dist,\n",
    "            kl_use_exact=kl_use_exact,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            activation=activation,\n",
    "            noise_reg=noise_reg,\n",
    "            learning_rate=learning_rate,\n",
    "            trainable_prior=trainable_prior,\n",
    "            map_mode=map_mode,\n",
    "            prior_scale=prior_scale,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65992896-1ca4-4c63-8775-7485fe274321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaximumLikelihoodNNEstimator(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dist_layer,\n",
    "        hidden_sizes=(16, 16),\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        learning_rate=3e-3,\n",
    "        activation=\"relu\",\n",
    "        random_seed=22,\n",
    "    ):\n",
    "        assert len(noise_reg) == 2\n",
    "\n",
    "        dense_layers = self._get_dense_layers(\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=dist_layer.get_total_param_size(),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            dense_layers + [dist_layer],\n",
    "            noise_fn_type=noise_reg[0],\n",
    "            noise_scale_factor=noise_reg[1],\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "\n",
    "        self.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate), loss=self._get_neg_log_likelihood()\n",
    "        )\n",
    "\n",
    "    def _get_dense_layers(self, hidden_sizes, output_size, activation):\n",
    "        assert type(hidden_sizes) == tuple or type(hidden_sizes) == list\n",
    "        # the data normalization values are assigned once fit is called\n",
    "        normalization = [tf.keras.layers.Lambda(lambda x: (x - self.x_mean) / (self.x_std + 1e-8))]\n",
    "        noise_reg = [tf.keras.layers.GaussianNoise(self.x_noise_std)]\n",
    "        hidden = [tf.keras.layers.Dense(size, activation=activation) for size in hidden_sizes]\n",
    "        output = [tf.keras.layers.Dense(output_size, activation=\"linear\")]\n",
    "        return normalization + noise_reg + hidden + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a97995fc-130a-4b21-a8dd-98b39f940647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class NormalizingFlowNetwork(MaximumLikelihoodNNEstimator):\n",
    "    def __init__(self, n_dims, n_flows=10, trainable_base_dist=True, **kwargs):\n",
    "        \"\"\"\n",
    "        :param n_dims: Dimensionsion of Y. The dimension of X is automatically inferred from the data\n",
    "        :param n_flows: The number of radial flows to use.\n",
    "        :param trainable_base_dist: Whether the standard normal base dist has trainable mean + diag. convariance\n",
    "        \"\"\"\n",
    "        dist_layer = InverseNormalizingFlowLayer(\n",
    "            flow_types=[\"radial\"] * n_flows, n_dims=n_dims, trainable_base_dist=trainable_base_dist\n",
    "        )\n",
    "        super().__init__(dist_layer, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_function(\n",
    "        n_dims=1,\n",
    "        n_flows=3,\n",
    "        hidden_sizes=(16, 16),\n",
    "        trainable_base_dist=True,\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        learning_rate=3e-3,\n",
    "        activation=\"tanh\",\n",
    "    ):\n",
    "        # this is necessary, else there'll be processes hanging around hogging memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        return NormalizingFlowNetwork(\n",
    "            n_dims=n_dims,\n",
    "            n_flows=n_flows,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            trainable_base_dist=trainable_base_dist,\n",
    "            noise_reg=noise_reg,\n",
    "            learning_rate=learning_rate,\n",
    "            activation=activation,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024be78-1775-48f0-a82b-c35059acab44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94580e6-639e-4e81-8e00-5d05fdf42067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d66b5d-ffc9-42e6-82e3-909245a0907b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "640b9386-fbe4-488a-a15c-7cf1596d369f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import pytest\n",
    "\n",
    "\n",
    "def flow_dimension_testing(flow_name):\n",
    "    FLOWS = {\"planar\": PlanarFlow, \"radial\": RadialFlow, \"affine\": AffineFlow}\n",
    "    # all are tested against Affine Flow, since that's the reference implementation\n",
    "    batch_size = 10\n",
    "    for dim in [1, 4]:\n",
    "        flow_class = FLOWS[flow_name]\n",
    "        # test dimension of parameter space\n",
    "        with pytest.raises(AssertionError):\n",
    "            flow = flow_class(tf.ones((batch_size, flow_class.get_param_size(dim) + 1)), dim)\n",
    "\n",
    "        flow = flow_class(tf.ones((batch_size, flow_class.get_param_size(dim))), dim)\n",
    "        reference = AffineFlow(tf.ones((batch_size, AffineFlow.get_param_size(dim))), dim)\n",
    "\n",
    "        test_tensors = [[[0.0] * dim], [[1.0] * dim] * batch_size]\n",
    "        assert flow.forward_min_event_ndims == reference.forward_min_event_ndims\n",
    "        for tensor in test_tensors:\n",
    "            assert flow.forward(tensor).shape == reference.forward(tensor).shape\n",
    "            assert (\n",
    "                flow._forward_log_det_jacobian(tensor).shape\n",
    "                == reference._forward_log_det_jacobian(tensor).shape\n",
    "            )\n",
    "\n",
    "        tensor = [[1.0] * dim] + ([[0.0] * dim] * (batch_size - 2)) + [[1.0] * dim]\n",
    "        res = flow.forward(tensor).numpy()\n",
    "        assert res[0] == pytest.approx(res[-1], rel=1e-5)\n",
    "        assert res[1] == pytest.approx(res[-2], rel=1e-5)\n",
    "        assert not all(res[0] == res[1])\n",
    "\n",
    "        tensor = [[1.0] * dim] + ([[0.0] * dim] * (batch_size - 2)) + [[1.0] * dim]\n",
    "        res = flow._forward_log_det_jacobian(tensor).numpy()\n",
    "        assert res[0] == pytest.approx(res[-1], rel=1e-5)\n",
    "        assert res[1] == pytest.approx(res[-2], rel=1e-5)\n",
    "        assert not res[0] == pytest.approx(res[1])\n",
    "\n",
    "\n",
    "def test_planar():\n",
    "    flow_dimension_testing(\"planar\")\n",
    "\n",
    "\n",
    "def test_radial():\n",
    "    flow_dimension_testing(\"radial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d08e49a-50a5-4f01-8af9-c61d07a7630a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Affine.__init__() got an unexpected keyword argument 'shift'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_radial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 46\u001b[0m, in \u001b[0;36mtest_radial\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_radial\u001b[39m():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mflow_dimension_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mradial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 17\u001b[0m, in \u001b[0;36mflow_dimension_testing\u001b[0;34m(flow_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)), dim)\n\u001b[1;32m     16\u001b[0m flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n\u001b[0;32m---> 17\u001b[0m reference \u001b[38;5;241m=\u001b[39m \u001b[43mAffineFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_tensors \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim], [[\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim] \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mforward_min_event_ndims \u001b[38;5;241m==\u001b[39m reference\u001b[38;5;241m.\u001b[39mforward_min_event_ndims\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mAffineFlow.__init__\u001b[0;34m(self, t, n_dims, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, n_dims, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAffineFlow\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n_dims\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_diag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Affine.__init__() got an unexpected keyword argument 'shift'"
     ]
    }
   ],
   "source": [
    "test_radial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97946900-bc4f-4bca-8faf-e7cfbf238d61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "name for name_scope must be a string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mflow_dimension_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPlanarFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m, in \u001b[0;36mflow_dimension_testing\u001b[0;34m(flow_class)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# test dimension of parameter space\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pytest\u001b[38;5;241m.\u001b[39mraises(\u001b[38;5;167;01mAssertionError\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m         flow \u001b[38;5;241m=\u001b[39m \u001b[43mflow_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n\u001b[1;32m     15\u001b[0m     reference \u001b[38;5;241m=\u001b[39m AffineFlow(tf\u001b[38;5;241m.\u001b[39mones((batch_size, AffineFlow\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:913\u001b[0m, in \u001b[0;36mBijector.__call__\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Bijector):\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mChain([\u001b[38;5;28mself\u001b[39m, value], name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:1326\u001b[0m, in \u001b[0;36mBijector.forward\u001b[0;34m(self, x, name, **kwargs)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the forward `Bijector` evaluation, i.e., X = g(Y).\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;124;03m    NotImplementedError: if `_forward` is not implemented.\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:1300\u001b[0m, in \u001b[0;36mBijector._call_forward\u001b[0;34m(self, x, name, **kwargs)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Wraps call to _forward, allowing extra shared logic.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1300\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(name):\n\u001b[1;32m   1301\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_dtype(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1302\u001b[0m     x \u001b[38;5;241m=\u001b[39m nest_util\u001b[38;5;241m.\u001b[39mconvert_to_nested_tensor(\n\u001b[1;32m   1303\u001b[0m         x, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype_hint\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1304\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m SKIP_DTYPE_CHECKS \u001b[38;5;28;01melse\u001b[39;00m dtype,\n\u001b[1;32m   1305\u001b[0m         allow_packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:1816\u001b[0m, in \u001b[0;36mBijector._name_and_control_scope\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to standardize op scope.\"\"\"\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m name_util\u001b[38;5;241m.\u001b[39minstance_scope(\n\u001b[1;32m   1814\u001b[0m     instance_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1815\u001b[0m     constructor_name_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_name_scope):\n\u001b[0;32m-> 1816\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m name_scope:\n\u001b[1;32m   1817\u001b[0m     deps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defer_all_assertions:\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:6423\u001b[0m, in \u001b[0;36mname_scope_v2.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6414\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the context manager.\u001b[39;00m\n\u001b[1;32m   6415\u001b[0m \n\u001b[1;32m   6416\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6420\u001b[0m \u001b[38;5;124;03m  ValueError: If name is not a string.\u001b[39;00m\n\u001b[1;32m   6421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 6423\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname for name_scope must be a string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m   6425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exit_fns \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: name for name_scope must be a string."
     ]
    }
   ],
   "source": [
    "flow_dimension_testing(PlanarFlow(t=t, n_dims=ndims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c58f187a-4d63-45e9-bd80-8349e94edd63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = tf.random.normal(shape=(1, 5))\n",
    "ndims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41869a9e-26ea-4729-a19f-3463eca5f4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6c787-4142-4992-8a39-4f358aa43bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Implements a bijector x = y + (alpha * beta * (y - y_0)) / (alpha + abs(y - y_0)).\n",
    "    Args:\n",
    "        params: Tensor shape (?, n_dims+2). This will be split into the parameters\n",
    "            alpha (?, 1), beta (?, 1), gamma (?, n_dims).\n",
    "            Furthermore alpha will be constrained to assure the invertability of the flow\n",
    "        n_dims: The dimension of the distribution that will be transformed\n",
    "        name: The name to give this particular flow\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215fb29-b609-4def-afa5-a610b188fadf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFlows",
   "language": "python",
   "name": "nflows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

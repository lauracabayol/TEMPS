{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f65ab0-9aa9-4de1-bffb-2a70a5ebe152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "\n",
    "from torch.utils.data import DataLoader, dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25375e3d-9f96-48fb-84fe-093d14f386ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../insight')\n",
    "from archive import archive \n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22153c3-c83b-4f3e-b25a-3a8a034c17fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parent_dir = '/data/astro/scratch/lcabayol/Euclid/NNphotozs/Euclid_EXT_MER_PHZ_DC2_v1.5'\n",
    "\n",
    "photoz_archive = archive(path = parent_dir, Qz_cut=0.5)\n",
    "f, ferr, specz, specqz = photoz_archive.get_training_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a316118-9704-4ea9-a35e-7959e0b83ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dset = TensorDataset(torch.Tensor(f),torch.Tensor(specz))\n",
    "loader = DataLoader(dset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ceab5e-d789-4873-a818-bda038896145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def MMD(x, y, kernel):\n",
    "    \"\"\"Emprical maximum mean discrepancy. The lower the result\n",
    "       the more evidence that distributions are the same.\n",
    "\n",
    "    Args:\n",
    "        x: first sample, distribution P\n",
    "        y: second sample, distribution Q\n",
    "        kernel: kernel type such as \"multiscale\" or \"rbf\"\n",
    "    \"\"\"\n",
    "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "\n",
    "    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
    "    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
    "    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n",
    "\n",
    "    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device))\n",
    "\n",
    "    if kernel == \"multiscale\":\n",
    "\n",
    "        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n",
    "        for a in bandwidth_range:\n",
    "            XX += a**2 * (a**2 + dxx)**-1\n",
    "            YY += a**2 * (a**2 + dyy)**-1\n",
    "            XY += a**2 * (a**2 + dxy)**-1\n",
    "\n",
    "    if kernel == \"rbf\":\n",
    "\n",
    "        bandwidth_range = [10, 15, 20, 50]\n",
    "        for a in bandwidth_range:\n",
    "            XX += torch.exp(-0.5*dxx/a)\n",
    "            YY += torch.exp(-0.5*dyy/a)\n",
    "            XY += torch.exp(-0.5*dxy/a)\n",
    "\n",
    "\n",
    "    return torch.mean(XX + YY - 2. * XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f0feae-ce85-4840-ba5d-639c48d0e496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InvertedPlanarFlow(nn.Module):\n",
    "    \"\"\"Implementation of the invertible transformation used in planar flow:\n",
    "        f(z) = z + u * h(dot(w.T, z) + b)\n",
    "    See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims):\n",
    "        super(InvertedPlanarFlow, self).__init__()\n",
    "        self.n_dims=n_dims\n",
    "        \n",
    "    def _assign_params_flow(self, flow_params):\n",
    "        \n",
    "        # Extract the u, w, and b components from flow_params\n",
    "        u = flow_params[:, :self.n_dims]\n",
    "        w = flow_params[:, self.n_dims:(2* self.n_dims)]\n",
    "        b = flow_params[:, -1].unsqueeze(1)\n",
    "        \n",
    "        # Update the parameters with the new values\n",
    "        self.u = nn.Parameter(u)\n",
    "        self.w = nn.Parameter(w)\n",
    "        self.b = nn.Parameter(b)\n",
    "        \n",
    "    \n",
    "    def forward(self, y, flow_params):\n",
    "        self._assign_params_flow(flow_params)\n",
    "        activation = y + self.w*y + self.b\n",
    "        flow =  y + self.u * torch.tanh(activation)\n",
    "        psi = (1 - torch.tanh(activation) ** 2) * self.w\n",
    "        jacobian = 1 + psi * self.u\n",
    "        det_jacobian = torch.abs(jacobian)\n",
    "        return flow, det_jacobian\n",
    "    \n",
    "    \n",
    "    def inverse(self, z):\n",
    "        activation = z + self.w*z + self.b\n",
    "        flow =  z - self.u * torch.tanh(activation)\n",
    "        psi = (1 - torch.tanh(activation) ** 2) * self.w\n",
    "        jacobian = 1 - psi* self.u\n",
    "        det_jacobian = torch.abs(jacobian)\n",
    "        return flow, det_jacobian\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ad6f89b-517c-47bb-a955-c94d80a5e4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConditionalNormalizingFlow(nn.Module):\n",
    "    def __init__(self, n_dims, n_context, n_flows):\n",
    "        super(ConditionalNormalizingFlow, self).__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.n_flows = n_flows\n",
    "        self.n_context = n_context\n",
    "        self.base_distribution=D.Normal(torch.zeros(self.n_dims), torch.ones(self.n_dims))\n",
    "\n",
    "        # Create the flow layers\n",
    "        self.flows = nn.ModuleList([InvertedPlanarFlow(self.n_dims) for _ in range(n_flows)])\n",
    "\n",
    "        # Context encoder network (maps context to flow parameters)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_context, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_flows * (2 * n_dims + 1))  # u, w, b for each flow\n",
    "        )\n",
    "           \n",
    "\n",
    "    def forward(self, y, context_data):\n",
    "        # Compute flow parameters from the context\n",
    "        flow_params = self.encoder(context_data).view(-1, self.n_flows, 2 * self.n_dims + 1)\n",
    "        \n",
    "        #forward prop:\n",
    "        logdetjac_sum = 0\n",
    "        for i in range(self.n_flows):\n",
    "            if i == 0:\n",
    "                z, det_jacobian = self.flows[i](y,flow_params[:, i, :])\n",
    "            else:\n",
    "                z, det_jacobian = self.flows[i](z,flow_params[:, i, :])\n",
    "                \n",
    "            logdetjac_sum += torch.log(det_jacobian)\n",
    "            \n",
    "        for i in range(self.n_flows):\n",
    "            if i == 0:\n",
    "                yp, det_jacobian = self.flows[i].inverse(z)\n",
    "                yp = z*det_jacobian\n",
    "            else:\n",
    "                yp, det_jacobian = self.flows[i].inverse(yp)\n",
    "                yp = yp*det_jacobian            \n",
    "            \n",
    "        return z, yp, logdetjac_sum\n",
    "    \n",
    "    def predict(self, z, context_data):\n",
    "        # Compute flow parameters from the context\n",
    "        flow_params = self.encoder(context_data).view(-1, self.n_flows, 2 * self.n_dims + 1)\n",
    "            \n",
    "        for i in range(self.n_flows):\n",
    "            if i == 0:\n",
    "                self.flows[i]._assign_params_flow(flow_params[:, i, :])\n",
    "                yp, det_jacobian = self.flows[i].inverse(z)\n",
    "                yp = yp*det_jacobian\n",
    "            else:\n",
    "                self.flows[i]._assign_params_flow(flow_params[:, i, :])\n",
    "                yp, det_jacobian = self.flows[i].inverse(yp)\n",
    "                yp = yp*det_jacobian            \n",
    "            \n",
    "        return yp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59b89bbe-aab9-424d-be61-b49ecabbb589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "n_dims = 1\n",
    "n_context=6\n",
    "n_flows = 20\n",
    "nepochs=200\n",
    "# Define the dimensionality of z\n",
    "z_dim = 1\n",
    "\n",
    "base_distribution = D.Normal(torch.zeros(z_dim), torch.ones(z_dim))\n",
    "#base_distribution_perturbation = D.Normal(torch.zeros(z_dim), epsilon**2*torch.ones(z_dim))\n",
    "\n",
    "#data = torch.randn(batch_size, n_context)\n",
    "#y = torch.randn(batch_size,1)\n",
    "#mmd = MMDLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6567aace-64e6-4ad6-ac75-44b4c8ed63a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m randomN_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnan_to_num(z)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m zloss \u001b[38;5;241m=\u001b[39m ((yp\u001b[38;5;241m-\u001b[39my\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m logloss \u001b[38;5;241m=\u001b[39m  MMD(z,randomN_input, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m-\u001b[39mbase_distribution\u001b[38;5;241m.\u001b[39mlog_prob(z\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m logdet_jacb\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m zloss\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#flow_model = ConditionalNormalizingFlow(n_dims, n_context, n_flows).to(device)\n",
    "optimizer = optim.Adam(flow_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "for e in range(nepochs):\n",
    "    \n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z, yp, logdet_jacb = flow_model(y.unsqueeze(1).to(device),x.to(device))\n",
    "        randomN_input = torch.randn(z.size(0), z.size(1)).to(device)\n",
    "        z = torch.nan_to_num(z)\n",
    "        assert False\n",
    "\n",
    "        \n",
    "        zloss = ((yp-y.to(device))**2).mean(1)\n",
    "                \n",
    "        logloss =  MMD(z,randomN_input, kernel='rbf')-base_distribution.log_prob(z.cpu()).to(device).mean() + logdet_jacb.mean() + zloss.mean()\n",
    "        #logloss = -logloss.mean()\n",
    "\n",
    "        logloss.backward()\n",
    "        optimizer.step() \n",
    "    print('epoch',e,logloss)\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee7fd02a-4612-4da8-85bd-27504a74d486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "93d1f507-e28d-4488-af13-bcfc649f5e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  4.,\n",
       "        24., 16., 20.,  9.,  6.,  4.,  4.,  2.,  0.,  0.,  2.,  3.,  3.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
       " array([-5.        , -4.80000019, -4.5999999 , -4.4000001 , -4.19999981,\n",
       "        -4.        , -3.79999995, -3.5999999 , -3.4000001 , -3.20000005,\n",
       "        -3.        , -2.79999995, -2.5999999 , -2.4000001 , -2.20000005,\n",
       "        -2.        , -1.79999995, -1.60000002, -1.39999998, -1.20000005,\n",
       "        -1.        , -0.80000001, -0.60000002, -0.40000001, -0.2       ,\n",
       "         0.        ,  0.2       ,  0.40000001,  0.60000002,  0.80000001,\n",
       "         1.        ,  1.20000005,  1.39999998,  1.60000002,  1.79999995,\n",
       "         2.        ,  2.20000005,  2.4000001 ,  2.5999999 ,  2.79999995,\n",
       "         3.        ,  3.20000005,  3.4000001 ,  3.5999999 ,  3.79999995,\n",
       "         4.        ,  4.19999981,  4.4000001 ,  4.5999999 ,  4.80000019,\n",
       "         5.        ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(z.detach().cpu().numpy(), bins = 50, range = (-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96747917-9254-4a5e-85e2-b524922c6cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2950, 0.3657, 0.4153, 0.3495, 0.6173, 0.3576, 1.1720, 0.7188, 0.5605,\n",
       "        0.3393])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "509e83b7-cd93-4e8b-999c-442955936005",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1834],\n",
       "        [0.2864],\n",
       "        [0.3292],\n",
       "        [0.2728],\n",
       "        [0.5064],\n",
       "        [0.2797],\n",
       "        [1.0441],\n",
       "        [0.6005],\n",
       "        [0.4555],\n",
       "        [0.2642]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1572b201-1427-4aba-93ef-11dcf7b6130e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ntest=1\n",
    "zs = base_distribution.sample(sample_shape=torch.Size([1000]))\n",
    "ppz = np.zeros(shape=(Ntest,1000))\n",
    "for ii in range(Ntest):\n",
    "    for jj,z in enumerate(zs):\n",
    "        ypred = flow_model.predict(z.unsqueeze(0).to(device),x[ii].unsqueeze(0).to(device))\n",
    "        ppz[ii,jj] = ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "617fa82d-f6d0-4c2c-8561-df6268c2475f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 17.,  49., 135., 202., 251., 193., 100.,  40.,  10.,   3.]),\n",
       " array([-2.72178793, -2.09639964, -1.47101135, -0.84562306, -0.22023478,\n",
       "         0.40515351,  1.0305418 ,  1.65593009,  2.28131838,  2.90670667,\n",
       "         3.53209496]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ppz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86b0fd71-ad00-48d1-9a66-c8d799e2689a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RadialFlow(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RadialFlow, self).__init__()\n",
    "        self.n_dims=n_dims\n",
    "        \n",
    "    def _assign_params_flow(self, flow_params):\n",
    "        \n",
    "        # Extract the u, w, and b components from flow_params\n",
    "        alpha = flow_params[:, :self.n_dims]\n",
    "        beta = flow_params[:, self.n_dims:(2* self.n_dims)]\n",
    "        gamma = flow_params[:, -1].unsqueeze(1)\n",
    "        \n",
    "        # Update the parameters with the new values\n",
    "        self.alpha = nn.Parameter(alpha)\n",
    "        self.beta = nn.Parameter(beta)\n",
    "        self.gamma = nn.Parameter(gamma)\n",
    "\n",
    "    def forward(self, z, flow_params):\n",
    "        \n",
    "        self._assign_params_flow(flow_params)\n",
    "\n",
    "        \n",
    "        num = softplus(self.alpha) * (torch.exp(self.beta)-1) * (z - self.gamma) \n",
    "        den = softplus(self.alpha) + (z - self.gamma) \n",
    "        \n",
    "        z_flow = z + num / den\n",
    "        \n",
    "        r = torch.abs(z-self.gamma)\n",
    "        h = 1 / (softplus(self.alpha) + r)\n",
    "        \n",
    "        term1 = (1 + softplus(self.alpha) * (torch.exp(self.beta)-1) * h)**(self.n_dims -1)\n",
    "        term2 = 1 + softplus(self.alpha) * (torch.exp(self.beta)-1) * h + softplus(self.alpha) * (torch.exp(self.beta)-1) * r *h**2\n",
    "        \n",
    "        det_jacobian = term1*term2\n",
    "        \n",
    "        return z_flow, det_jacobian\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        \n",
    "        num = softplus(self.alpha) * (torch.exp(self.beta)-1) * (z - self.gamma) \n",
    "        den = softplus(self.alpha) + (z - self.gamma) \n",
    "        \n",
    "        z_flow = z - num / den\n",
    "        \n",
    "        r = torch.abs(z-self.gamma)\n",
    "        h = 1 / (softplus(self.alpha) + r)\n",
    "        \n",
    "        term1 = (1 + softplus(self.alpha) * (torch.exp(self.beta)-1) * h)**(self.n_dims -1)\n",
    "        term2 = 1 - softplus(self.alpha) * (torch.exp(self.beta)-1) * h - softplus(self.alpha) * (torch.exp(self.beta)-1) * r *h**2\n",
    "        \n",
    "        det_jacobian = term1*term2\n",
    "        \n",
    "        return z_flow, det_jacobian\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e87e76e0-4ece-4b21-9b65-ea46b50e6100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConditionalNormalizingFlow(nn.Module):\n",
    "    def __init__(self, n_dims, n_context, n_flows):\n",
    "        super(ConditionalNormalizingFlow, self).__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.n_flows = n_flows\n",
    "        self.n_context = n_context\n",
    "        self.base_distribution=D.Normal(torch.zeros(self.n_dims), torch.ones(self.n_dims))\n",
    "\n",
    "        # Create the flow layers\n",
    "        self.flows = nn.ModuleList([RadialFlow(self.n_dims) for _ in range(n_flows)])\n",
    "\n",
    "        # Context encoder network (maps context to flow parameters)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_context, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_flows * (2 * n_dims + 1))  # u, w, b for each flow\n",
    "        )\n",
    "           \n",
    "\n",
    "    def forward(self, y, context_data):\n",
    "        # Compute flow parameters from the context\n",
    "        flow_params = self.encoder(context_data).view(-1, self.n_flows, 2 * self.n_dims + 1)\n",
    "        \n",
    "        #forward prop:\n",
    "        logdetjac_sum = 0\n",
    "        for i in range(self.n_flows):\n",
    "            if i == 0:\n",
    "                z, det_jacobian = self.flows[i](y,flow_params[:, i, :])\n",
    "            else:\n",
    "                z, det_jacobian = self.flows[i](z,flow_params[:, i, :])\n",
    "                \n",
    "            logdetjac_sum += torch.log(det_jacobian)\n",
    "            \n",
    "        for i in range(self.n_flows):\n",
    "            if i == 0:\n",
    "                yp, det_jacobian = self.flows[i].inverse(z)\n",
    "                yp = z*det_jacobian\n",
    "            else:\n",
    "                yp, det_jacobian = self.flows[i].inverse(yp)\n",
    "                yp = yp*det_jacobian            \n",
    "            \n",
    "        return z, yp, logdetjac_sum\n",
    "    \n",
    "    def predict(self, z, context_data):\n",
    "        # Compute flow parameters from the context\n",
    "        flow_params = self.encoder(context_data).view(-1, self.n_flows, 2 * self.n_dims + 1)\n",
    "            \n",
    "        for i in range(self.n_flows):\n",
    "            if i == 0:\n",
    "                self.flows[i]._assign_params_flow(flow_params[:, i, :])\n",
    "                yp, det_jacobian = self.flows[i].inverse(z)\n",
    "                yp = yp*det_jacobian\n",
    "            else:\n",
    "                self.flows[i]._assign_params_flow(flow_params[:, i, :])\n",
    "                yp, det_jacobian = self.flows[i].inverse(yp)\n",
    "                yp = yp*det_jacobian            \n",
    "            \n",
    "        return yp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d9f70e1-419b-4d2d-9bdc-44450d84bd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (100, 1)) to be within the support (Real()) of the distribution Normal(loc: tensor([0.]), scale: tensor([1.])), but found invalid values:\ntensor([[0.8334],\n        [1.0070],\n        [1.9326],\n        [0.6259],\n        [0.3777],\n        [0.4443],\n        [0.7886],\n        [0.7467],\n        [0.7462],\n        [0.7054],\n        [1.1618],\n        [0.5967],\n        [0.5120],\n        [0.6465],\n        [0.0662],\n        [0.2656],\n        [0.8282],\n        [0.7042],\n        [0.3526],\n        [0.4975],\n        [2.7807],\n        [0.4749],\n        [0.8635],\n        [0.4452],\n        [1.1419],\n        [0.4767],\n        [0.7106],\n        [0.5398],\n        [0.7557],\n        [0.6760],\n        [0.7165],\n        [1.1008],\n        [0.6860],\n        [3.5272],\n        [0.6977],\n        [0.3678],\n        [1.4739],\n        [0.8516],\n        [1.0473],\n        [1.0079],\n        [0.5887],\n        [1.1655],\n        [0.8932],\n        [0.4380],\n        [0.6055],\n        [0.8969],\n        [0.7425],\n        [0.6864],\n        [0.9037],\n        [0.5603],\n        [2.1866],\n        [0.3644],\n        [0.1954],\n        [1.1353],\n        [0.3222],\n        [3.0116],\n        [3.8900],\n        [0.9592],\n        [2.2661],\n        [2.4230],\n        [0.7257],\n        [   nan],\n        [0.7326],\n        [0.9051],\n        [0.5102],\n        [0.5501],\n        [0.8331],\n        [0.9098],\n        [1.1535],\n        [3.0428],\n        [0.7206],\n        [0.4672],\n        [0.6801],\n        [0.6882],\n        [0.4479],\n        [0.9945],\n        [0.3963],\n        [1.9218],\n        [0.7609],\n        [0.2332],\n        [0.8465],\n        [0.7598],\n        [0.8371],\n        [0.3823],\n        [1.5190],\n        [0.1409],\n        [2.0572],\n        [0.8513],\n        [0.2356],\n        [0.4411],\n        [0.4399],\n        [1.0175],\n        [1.0340],\n        [0.8210],\n        [0.1267],\n        [0.8957],\n        [0.9660],\n        [0.2468],\n        [1.8487],\n        [0.9320]], grad_fn=<ToCopyBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m randomN_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m zloss \u001b[38;5;241m=\u001b[39m ((yp\u001b[38;5;241m-\u001b[39my\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m logloss \u001b[38;5;241m=\u001b[39m  MMD(z,randomN_input, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[43mbase_distribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m logdet_jacb\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m zloss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#logloss = -logloss.mean()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m logloss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/distributions/normal.py:79\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# compute the variance\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     var \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/distributions/distribution.py:300\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    298\u001b[0m valid \u001b[38;5;241m=\u001b[39m support\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be within the support (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(support)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (100, 1)) to be within the support (Real()) of the distribution Normal(loc: tensor([0.]), scale: tensor([1.])), but found invalid values:\ntensor([[0.8334],\n        [1.0070],\n        [1.9326],\n        [0.6259],\n        [0.3777],\n        [0.4443],\n        [0.7886],\n        [0.7467],\n        [0.7462],\n        [0.7054],\n        [1.1618],\n        [0.5967],\n        [0.5120],\n        [0.6465],\n        [0.0662],\n        [0.2656],\n        [0.8282],\n        [0.7042],\n        [0.3526],\n        [0.4975],\n        [2.7807],\n        [0.4749],\n        [0.8635],\n        [0.4452],\n        [1.1419],\n        [0.4767],\n        [0.7106],\n        [0.5398],\n        [0.7557],\n        [0.6760],\n        [0.7165],\n        [1.1008],\n        [0.6860],\n        [3.5272],\n        [0.6977],\n        [0.3678],\n        [1.4739],\n        [0.8516],\n        [1.0473],\n        [1.0079],\n        [0.5887],\n        [1.1655],\n        [0.8932],\n        [0.4380],\n        [0.6055],\n        [0.8969],\n        [0.7425],\n        [0.6864],\n        [0.9037],\n        [0.5603],\n        [2.1866],\n        [0.3644],\n        [0.1954],\n        [1.1353],\n        [0.3222],\n        [3.0116],\n        [3.8900],\n        [0.9592],\n        [2.2661],\n        [2.4230],\n        [0.7257],\n        [   nan],\n        [0.7326],\n        [0.9051],\n        [0.5102],\n        [0.5501],\n        [0.8331],\n        [0.9098],\n        [1.1535],\n        [3.0428],\n        [0.7206],\n        [0.4672],\n        [0.6801],\n        [0.6882],\n        [0.4479],\n        [0.9945],\n        [0.3963],\n        [1.9218],\n        [0.7609],\n        [0.2332],\n        [0.8465],\n        [0.7598],\n        [0.8371],\n        [0.3823],\n        [1.5190],\n        [0.1409],\n        [2.0572],\n        [0.8513],\n        [0.2356],\n        [0.4411],\n        [0.4399],\n        [1.0175],\n        [1.0340],\n        [0.8210],\n        [0.1267],\n        [0.8957],\n        [0.9660],\n        [0.2468],\n        [1.8487],\n        [0.9320]], grad_fn=<ToCopyBackward0>)"
     ]
    }
   ],
   "source": [
    "flow_model = ConditionalNormalizingFlow(n_dims, n_context, n_flows).to(device)\n",
    "optimizer = optim.Adam(flow_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "for e in range(nepochs):\n",
    "    \n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z, yp, logdet_jacb = flow_model(y.unsqueeze(1).to(device),x.to(device))\n",
    "        randomN_input = torch.randn(z.size(0), z.size(1)).to(device)\n",
    "\n",
    "        \n",
    "        zloss = ((yp-y.to(device))**2).mean(1)\n",
    "                \n",
    "        logloss =  MMD(z,randomN_input, kernel='rbf')-base_distribution.log_prob(z.cpu()).to(device).mean() + logdet_jacb.mean() + zloss.mean()\n",
    "        #logloss = -logloss.mean()\n",
    "\n",
    "        logloss.backward()\n",
    "        optimizer.step() \n",
    "    print('epoch',e,logloss)\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7beb3634-1cbe-429d-947c-e9be606ce090",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8484],\n",
       "        [ 1.6182],\n",
       "        [ 0.8343],\n",
       "        [-1.9827],\n",
       "        [ 0.9152],\n",
       "        [ 0.7381],\n",
       "        [ 0.0629],\n",
       "        [-0.8316],\n",
       "        [ 2.1748],\n",
       "        [-0.3499],\n",
       "        [ 0.3545],\n",
       "        [ 0.2066],\n",
       "        [-0.2118],\n",
       "        [ 1.2085],\n",
       "        [-1.8986],\n",
       "        [-1.0616],\n",
       "        [ 0.7481],\n",
       "        [ 0.1565],\n",
       "        [-0.4269],\n",
       "        [-1.6495],\n",
       "        [-0.3292],\n",
       "        [ 0.8947],\n",
       "        [ 0.7817],\n",
       "        [ 0.7382],\n",
       "        [-0.6586],\n",
       "        [ 0.8902],\n",
       "        [ 0.0948],\n",
       "        [ 0.1384],\n",
       "        [-0.9999],\n",
       "        [-0.4162],\n",
       "        [-0.2117],\n",
       "        [-0.6847],\n",
       "        [-0.3855],\n",
       "        [-1.1827],\n",
       "        [ 0.5822],\n",
       "        [-1.6971],\n",
       "        [-0.9910],\n",
       "        [ 0.7619],\n",
       "        [-0.8828],\n",
       "        [-0.1368],\n",
       "        [-0.7045],\n",
       "        [-0.1798],\n",
       "        [ 1.1066],\n",
       "        [-0.5498],\n",
       "        [-1.2881],\n",
       "        [ 1.1565],\n",
       "        [ 1.1648],\n",
       "        [-0.6986],\n",
       "        [-1.8623],\n",
       "        [ 0.9912],\n",
       "        [ 0.3955],\n",
       "        [ 0.4971],\n",
       "        [ 0.9061],\n",
       "        [-0.5090],\n",
       "        [ 0.0095],\n",
       "        [ 1.0226],\n",
       "        [-1.4518],\n",
       "        [-0.5620],\n",
       "        [ 1.7518],\n",
       "        [ 0.2183],\n",
       "        [ 0.1641],\n",
       "        [-1.0863],\n",
       "        [-2.1012],\n",
       "        [ 1.3582],\n",
       "        [ 0.0400],\n",
       "        [ 1.1156],\n",
       "        [ 0.0281],\n",
       "        [ 2.5893],\n",
       "        [-1.1186],\n",
       "        [-0.4407],\n",
       "        [-1.3903],\n",
       "        [ 0.4520],\n",
       "        [ 1.4107],\n",
       "        [-0.2401],\n",
       "        [-0.8214],\n",
       "        [-0.3933],\n",
       "        [ 1.6437],\n",
       "        [ 0.3437],\n",
       "        [-1.7400],\n",
       "        [-1.6789],\n",
       "        [ 1.6124],\n",
       "        [ 0.2401],\n",
       "        [ 1.2815],\n",
       "        [ 0.3185],\n",
       "        [ 0.4684],\n",
       "        [-1.0326],\n",
       "        [ 0.0446],\n",
       "        [-1.3375],\n",
       "        [ 1.4807],\n",
       "        [-1.5241],\n",
       "        [-0.4639],\n",
       "        [ 0.0416],\n",
       "        [ 1.0517],\n",
       "        [ 0.4896],\n",
       "        [ 0.3838],\n",
       "        [-0.1845],\n",
       "        [-0.0876],\n",
       "        [ 1.1864],\n",
       "        [-0.9631],\n",
       "        [-1.9050],\n",
       "        [-1.2937],\n",
       "        [ 0.9693],\n",
       "        [ 1.7783],\n",
       "        [ 0.1779],\n",
       "        [-0.5898],\n",
       "        [ 0.1853],\n",
       "        [ 0.4726],\n",
       "        [ 1.4247],\n",
       "        [-0.9633],\n",
       "        [ 2.4807],\n",
       "        [ 0.4720],\n",
       "        [ 1.6540],\n",
       "        [ 1.4077],\n",
       "        [-0.1462],\n",
       "        [ 0.3137],\n",
       "        [ 0.5545],\n",
       "        [ 1.4232],\n",
       "        [ 1.2829],\n",
       "        [ 2.3770],\n",
       "        [-1.8780],\n",
       "        [-0.1885],\n",
       "        [-0.7368],\n",
       "        [ 1.9480],\n",
       "        [ 1.0137],\n",
       "        [ 0.1715],\n",
       "        [-0.2915],\n",
       "        [ 2.1046],\n",
       "        [-0.3610],\n",
       "        [ 0.0489],\n",
       "        [-0.9548],\n",
       "        [ 0.0087],\n",
       "        [-0.7630],\n",
       "        [ 2.0795],\n",
       "        [ 0.5805],\n",
       "        [ 0.1057],\n",
       "        [-0.4684],\n",
       "        [-0.6578],\n",
       "        [ 1.7787],\n",
       "        [-1.0371],\n",
       "        [ 0.8826],\n",
       "        [-0.7278],\n",
       "        [ 1.0947],\n",
       "        [-1.5316],\n",
       "        [ 0.3948],\n",
       "        [-0.9081],\n",
       "        [-1.2862],\n",
       "        [ 2.2802],\n",
       "        [-0.4428],\n",
       "        [ 1.5194],\n",
       "        [ 0.5744],\n",
       "        [-0.9943],\n",
       "        [-0.6302],\n",
       "        [-1.3358],\n",
       "        [ 0.0834],\n",
       "        [-1.2864],\n",
       "        [ 0.9505],\n",
       "        [ 1.4664],\n",
       "        [ 0.8021],\n",
       "        [-3.2288],\n",
       "        [ 1.0043],\n",
       "        [-1.5168],\n",
       "        [ 0.7103],\n",
       "        [-1.1284],\n",
       "        [ 0.5888],\n",
       "        [ 1.2163],\n",
       "        [ 0.3103],\n",
       "        [-1.0108],\n",
       "        [ 0.1485],\n",
       "        [-2.1098],\n",
       "        [-0.3611],\n",
       "        [-0.5909],\n",
       "        [ 0.8019],\n",
       "        [-0.4823],\n",
       "        [ 0.2759],\n",
       "        [ 2.7019],\n",
       "        [ 0.5493],\n",
       "        [-1.2281],\n",
       "        [-0.6216],\n",
       "        [ 0.8920],\n",
       "        [ 0.5475],\n",
       "        [-1.5573],\n",
       "        [-2.5593],\n",
       "        [ 0.7151],\n",
       "        [-1.2309],\n",
       "        [ 1.1923],\n",
       "        [-0.7318],\n",
       "        [-0.1184],\n",
       "        [ 1.2953],\n",
       "        [-0.0813],\n",
       "        [-0.1665],\n",
       "        [ 2.3250],\n",
       "        [ 2.1487],\n",
       "        [-0.4384],\n",
       "        [ 0.5207],\n",
       "        [-0.4189],\n",
       "        [-1.2742],\n",
       "        [ 0.3463],\n",
       "        [-0.8962],\n",
       "        [-0.1095],\n",
       "        [ 1.3079],\n",
       "        [-0.3902],\n",
       "        [-0.1738],\n",
       "        [-1.0716],\n",
       "        [-0.3681],\n",
       "        [-2.3060],\n",
       "        [-0.3853],\n",
       "        [ 0.6688],\n",
       "        [ 1.4002],\n",
       "        [-0.1124],\n",
       "        [-1.1049],\n",
       "        [ 1.0707],\n",
       "        [ 0.3457],\n",
       "        [-0.7880],\n",
       "        [-0.3253],\n",
       "        [-0.1266],\n",
       "        [-0.5428],\n",
       "        [-0.3276],\n",
       "        [-0.6233],\n",
       "        [-1.9073],\n",
       "        [-0.7494],\n",
       "        [ 1.4387],\n",
       "        [-0.5146],\n",
       "        [-0.7635],\n",
       "        [-0.4909],\n",
       "        [ 1.6064],\n",
       "        [ 0.2278],\n",
       "        [ 0.3243],\n",
       "        [ 0.2031],\n",
       "        [-1.5865],\n",
       "        [ 0.7786],\n",
       "        [-0.1413],\n",
       "        [-1.2022],\n",
       "        [-0.9837],\n",
       "        [ 0.4362],\n",
       "        [ 1.6154],\n",
       "        [-0.1199],\n",
       "        [-0.0664],\n",
       "        [ 0.0472],\n",
       "        [-0.1505],\n",
       "        [-0.1991],\n",
       "        [ 2.2513],\n",
       "        [ 0.8745],\n",
       "        [ 0.6841],\n",
       "        [-0.1732],\n",
       "        [-0.2757],\n",
       "        [-0.9060],\n",
       "        [-0.3234],\n",
       "        [ 0.7810],\n",
       "        [-0.0444],\n",
       "        [-1.3418],\n",
       "        [-1.1832],\n",
       "        [ 1.7351],\n",
       "        [-0.3090],\n",
       "        [ 0.3704],\n",
       "        [-0.3049],\n",
       "        [ 0.0453],\n",
       "        [ 0.2731],\n",
       "        [ 0.2806],\n",
       "        [ 1.0438],\n",
       "        [-0.0344],\n",
       "        [-0.4080],\n",
       "        [-0.3858],\n",
       "        [ 0.1113],\n",
       "        [ 0.3823],\n",
       "        [ 1.9601],\n",
       "        [-0.4075],\n",
       "        [ 1.0364],\n",
       "        [-0.2932],\n",
       "        [-0.6234],\n",
       "        [-0.9265],\n",
       "        [ 0.1855],\n",
       "        [-1.1804],\n",
       "        [-1.2523],\n",
       "        [-0.2447],\n",
       "        [ 0.3975],\n",
       "        [-1.5798],\n",
       "        [ 0.9913],\n",
       "        [ 0.4032],\n",
       "        [ 0.3002],\n",
       "        [-1.6637],\n",
       "        [-1.9582],\n",
       "        [-0.1222],\n",
       "        [ 0.1790],\n",
       "        [ 1.1305],\n",
       "        [-0.8171],\n",
       "        [ 1.3106],\n",
       "        [-1.0320],\n",
       "        [ 0.0206],\n",
       "        [-0.1214],\n",
       "        [ 0.4119],\n",
       "        [ 0.8203],\n",
       "        [-1.0025],\n",
       "        [ 1.2579],\n",
       "        [ 0.2053],\n",
       "        [-0.8662],\n",
       "        [-1.6397],\n",
       "        [ 1.0495],\n",
       "        [-0.6104],\n",
       "        [ 1.4331],\n",
       "        [ 0.6375],\n",
       "        [-0.2168],\n",
       "        [-0.3101],\n",
       "        [-0.2008],\n",
       "        [-0.9112],\n",
       "        [ 0.4122],\n",
       "        [-0.0483],\n",
       "        [ 1.0281],\n",
       "        [ 1.0719],\n",
       "        [ 0.2373],\n",
       "        [ 0.0212],\n",
       "        [ 0.5707],\n",
       "        [ 1.6138],\n",
       "        [ 0.8172],\n",
       "        [ 0.3461],\n",
       "        [ 0.0128],\n",
       "        [ 0.3050],\n",
       "        [ 1.3341],\n",
       "        [ 0.7692],\n",
       "        [-0.1687],\n",
       "        [ 1.2077],\n",
       "        [-0.1087],\n",
       "        [ 0.9714],\n",
       "        [ 0.6303],\n",
       "        [ 0.5722],\n",
       "        [-1.1819],\n",
       "        [ 0.1692],\n",
       "        [ 1.2250],\n",
       "        [-1.0731],\n",
       "        [ 0.6335],\n",
       "        [ 2.2093],\n",
       "        [-0.7465],\n",
       "        [ 0.0166],\n",
       "        [ 0.5410],\n",
       "        [-2.8078],\n",
       "        [-0.0488],\n",
       "        [ 1.1682],\n",
       "        [-0.5070],\n",
       "        [-1.1633],\n",
       "        [-0.4779],\n",
       "        [ 0.5795],\n",
       "        [-0.6407],\n",
       "        [-0.1174],\n",
       "        [ 0.1414],\n",
       "        [-2.6810],\n",
       "        [-1.0488],\n",
       "        [-0.8054],\n",
       "        [-1.2615],\n",
       "        [ 0.1315],\n",
       "        [ 0.2922],\n",
       "        [ 0.5277],\n",
       "        [ 0.6547],\n",
       "        [ 1.5397],\n",
       "        [-1.8371],\n",
       "        [-2.0627],\n",
       "        [-0.1351],\n",
       "        [-0.2179],\n",
       "        [ 0.6396],\n",
       "        [ 0.3243],\n",
       "        [-0.5590],\n",
       "        [ 0.5037],\n",
       "        [ 0.7505],\n",
       "        [-1.3132],\n",
       "        [ 0.5287],\n",
       "        [ 1.0842],\n",
       "        [ 0.2646],\n",
       "        [ 1.3829],\n",
       "        [ 0.4475],\n",
       "        [ 1.3442],\n",
       "        [-0.7452],\n",
       "        [ 1.2135],\n",
       "        [ 0.1138],\n",
       "        [ 0.4037],\n",
       "        [-0.7675],\n",
       "        [ 2.3560],\n",
       "        [ 0.1491],\n",
       "        [ 0.8971],\n",
       "        [-0.9873],\n",
       "        [-1.5648],\n",
       "        [ 0.6061],\n",
       "        [ 0.4657],\n",
       "        [-0.2207],\n",
       "        [-1.2698],\n",
       "        [ 0.7112],\n",
       "        [ 0.6240],\n",
       "        [-2.0512],\n",
       "        [ 1.5672],\n",
       "        [-1.9298],\n",
       "        [ 0.4563],\n",
       "        [ 1.6955],\n",
       "        [ 1.6558],\n",
       "        [ 0.5164],\n",
       "        [-0.7062],\n",
       "        [ 0.4874],\n",
       "        [-0.2535],\n",
       "        [ 1.7271],\n",
       "        [ 0.9901],\n",
       "        [ 0.2829],\n",
       "        [-1.3487],\n",
       "        [-0.7403],\n",
       "        [ 1.3535],\n",
       "        [-0.8901],\n",
       "        [ 2.0288],\n",
       "        [-0.5798],\n",
       "        [-1.6997],\n",
       "        [-0.5378],\n",
       "        [ 0.2290],\n",
       "        [-0.1389],\n",
       "        [ 0.9740],\n",
       "        [-0.5510],\n",
       "        [-0.0816],\n",
       "        [ 0.6632],\n",
       "        [ 1.6692],\n",
       "        [ 0.1024],\n",
       "        [-1.8658],\n",
       "        [ 1.0311],\n",
       "        [-0.7037],\n",
       "        [ 1.8282],\n",
       "        [-0.1249],\n",
       "        [-0.2623],\n",
       "        [-0.1945],\n",
       "        [-1.3351],\n",
       "        [ 0.3040],\n",
       "        [-0.7674],\n",
       "        [ 2.1212],\n",
       "        [-2.3398],\n",
       "        [-0.3086],\n",
       "        [ 0.3988],\n",
       "        [-0.3147],\n",
       "        [ 0.6549],\n",
       "        [ 1.1813],\n",
       "        [ 0.6678],\n",
       "        [-0.4060],\n",
       "        [ 1.2435],\n",
       "        [ 0.2761],\n",
       "        [-0.2256],\n",
       "        [-0.0505],\n",
       "        [-1.0280],\n",
       "        [-0.4732],\n",
       "        [ 1.0582],\n",
       "        [ 2.9294],\n",
       "        [-0.4778],\n",
       "        [-0.7488],\n",
       "        [ 0.6736],\n",
       "        [ 0.6852],\n",
       "        [ 1.5694],\n",
       "        [-0.2367],\n",
       "        [-0.4071],\n",
       "        [-1.9901],\n",
       "        [-1.5835],\n",
       "        [-1.7664],\n",
       "        [ 2.2332],\n",
       "        [ 1.2876],\n",
       "        [ 1.1543],\n",
       "        [-0.2406],\n",
       "        [ 0.5135],\n",
       "        [ 0.0505],\n",
       "        [ 0.7619],\n",
       "        [-0.0932],\n",
       "        [-0.6752],\n",
       "        [-1.7912],\n",
       "        [-1.7924],\n",
       "        [ 1.5102],\n",
       "        [-0.0879],\n",
       "        [ 0.2008],\n",
       "        [ 0.3200],\n",
       "        [ 0.4841],\n",
       "        [ 0.7664],\n",
       "        [-0.5850],\n",
       "        [ 0.7696],\n",
       "        [ 0.8054],\n",
       "        [ 0.6201],\n",
       "        [-1.0881],\n",
       "        [-0.4274],\n",
       "        [ 0.0041],\n",
       "        [-0.9817],\n",
       "        [-1.3310],\n",
       "        [ 0.2281],\n",
       "        [ 0.0317],\n",
       "        [-0.5253],\n",
       "        [-2.1040],\n",
       "        [-1.8740],\n",
       "        [ 1.7745],\n",
       "        [-1.6375],\n",
       "        [ 0.5859],\n",
       "        [ 0.5821],\n",
       "        [ 1.0172],\n",
       "        [-1.2461],\n",
       "        [ 0.0288],\n",
       "        [-0.3108],\n",
       "        [ 0.5266],\n",
       "        [-0.4236],\n",
       "        [-1.4083],\n",
       "        [ 0.5051],\n",
       "        [ 1.0877],\n",
       "        [ 0.2421],\n",
       "        [-0.9031],\n",
       "        [-0.2515],\n",
       "        [-0.7858],\n",
       "        [ 1.2855],\n",
       "        [-1.3077],\n",
       "        [ 0.1758],\n",
       "        [-2.6858],\n",
       "        [-1.2236],\n",
       "        [-0.4257],\n",
       "        [ 0.3830],\n",
       "        [ 0.4960],\n",
       "        [-1.0802],\n",
       "        [-0.6477],\n",
       "        [ 2.3971],\n",
       "        [ 0.6137],\n",
       "        [-1.3494],\n",
       "        [ 0.6293],\n",
       "        [ 1.5480],\n",
       "        [ 0.0269],\n",
       "        [-0.1519],\n",
       "        [ 0.4690],\n",
       "        [ 0.5525],\n",
       "        [ 0.6399],\n",
       "        [ 0.2694],\n",
       "        [-0.6512],\n",
       "        [ 0.8908],\n",
       "        [ 1.6630],\n",
       "        [-0.8944],\n",
       "        [ 0.6265],\n",
       "        [-1.3063],\n",
       "        [-1.0954],\n",
       "        [ 0.7482],\n",
       "        [ 1.8008],\n",
       "        [-0.3115],\n",
       "        [ 1.0241],\n",
       "        [ 0.6851],\n",
       "        [ 0.3456],\n",
       "        [-0.8985],\n",
       "        [-2.7074],\n",
       "        [-0.7714],\n",
       "        [ 1.3443],\n",
       "        [ 0.0929],\n",
       "        [-2.1514],\n",
       "        [-0.4225],\n",
       "        [-0.0519],\n",
       "        [-0.3225],\n",
       "        [ 0.1089],\n",
       "        [ 0.2726],\n",
       "        [-1.3717],\n",
       "        [ 0.5866],\n",
       "        [ 1.2626],\n",
       "        [-0.2116],\n",
       "        [-1.0426],\n",
       "        [-0.8483],\n",
       "        [ 0.8313],\n",
       "        [ 0.2363],\n",
       "        [-0.6206],\n",
       "        [ 0.1610],\n",
       "        [ 1.9711],\n",
       "        [ 1.5012],\n",
       "        [ 1.1874],\n",
       "        [-0.1874],\n",
       "        [ 0.7828],\n",
       "        [ 0.3814],\n",
       "        [ 0.2977],\n",
       "        [-0.0565],\n",
       "        [ 1.2097],\n",
       "        [ 0.4086],\n",
       "        [ 1.0315],\n",
       "        [ 0.7165],\n",
       "        [ 0.8851],\n",
       "        [ 0.3158],\n",
       "        [ 0.7494],\n",
       "        [ 0.8517],\n",
       "        [ 1.5425],\n",
       "        [ 0.7181],\n",
       "        [ 1.1441],\n",
       "        [-0.8995],\n",
       "        [ 0.6298],\n",
       "        [ 0.7891],\n",
       "        [-0.2911],\n",
       "        [ 0.1373],\n",
       "        [-0.1217],\n",
       "        [ 0.8212],\n",
       "        [ 0.0733],\n",
       "        [ 2.1515],\n",
       "        [-0.9786],\n",
       "        [ 0.6745],\n",
       "        [ 0.6556],\n",
       "        [-0.5731],\n",
       "        [-0.5098],\n",
       "        [-0.9516],\n",
       "        [ 0.1897],\n",
       "        [-0.5442],\n",
       "        [ 0.0604],\n",
       "        [-0.7411],\n",
       "        [ 0.4794],\n",
       "        [ 0.0198],\n",
       "        [-0.6394],\n",
       "        [-0.7729],\n",
       "        [-0.0799],\n",
       "        [ 2.3268],\n",
       "        [-0.1677],\n",
       "        [ 0.1481],\n",
       "        [-0.5329],\n",
       "        [-0.1187],\n",
       "        [-0.8519],\n",
       "        [ 1.8832],\n",
       "        [-0.1509],\n",
       "        [-0.7986],\n",
       "        [ 1.5142],\n",
       "        [-0.8809],\n",
       "        [ 0.7454],\n",
       "        [-0.9878],\n",
       "        [ 0.6007],\n",
       "        [ 1.4745],\n",
       "        [-0.3393],\n",
       "        [-0.3989],\n",
       "        [-0.9402],\n",
       "        [ 0.7090],\n",
       "        [ 1.3637],\n",
       "        [ 0.2148],\n",
       "        [ 1.3627],\n",
       "        [ 0.5403],\n",
       "        [-0.1745],\n",
       "        [-0.3899],\n",
       "        [ 1.3247],\n",
       "        [-1.3939],\n",
       "        [-0.2186],\n",
       "        [-1.0121],\n",
       "        [ 1.7591],\n",
       "        [-0.8580],\n",
       "        [-0.0870],\n",
       "        [ 1.3938],\n",
       "        [-0.4162],\n",
       "        [ 1.2729],\n",
       "        [ 1.2634],\n",
       "        [-2.3685],\n",
       "        [-1.2822],\n",
       "        [-1.3675],\n",
       "        [ 0.8407],\n",
       "        [-0.5122],\n",
       "        [ 1.4306],\n",
       "        [-2.7914],\n",
       "        [-0.3061],\n",
       "        [ 0.4404],\n",
       "        [ 0.5048],\n",
       "        [-0.3665],\n",
       "        [-0.2967],\n",
       "        [ 1.5601],\n",
       "        [ 0.6411],\n",
       "        [-0.1547],\n",
       "        [-0.1089],\n",
       "        [-0.2022],\n",
       "        [-1.3656],\n",
       "        [-0.4375],\n",
       "        [-0.9177],\n",
       "        [ 0.3400],\n",
       "        [-2.4454],\n",
       "        [-0.0372],\n",
       "        [ 0.1133],\n",
       "        [-0.6889],\n",
       "        [ 0.0989],\n",
       "        [-0.8998],\n",
       "        [ 0.6488],\n",
       "        [ 0.2334],\n",
       "        [-0.3779],\n",
       "        [ 2.0730],\n",
       "        [-0.3439],\n",
       "        [ 0.2982],\n",
       "        [-0.9382],\n",
       "        [-1.0531],\n",
       "        [ 0.0895],\n",
       "        [-1.0397],\n",
       "        [-0.0334],\n",
       "        [-0.0656],\n",
       "        [ 1.3556],\n",
       "        [-0.9963],\n",
       "        [-0.2734],\n",
       "        [-0.4409],\n",
       "        [ 1.0912],\n",
       "        [-0.2721],\n",
       "        [ 1.4855],\n",
       "        [-0.8651],\n",
       "        [-0.2906],\n",
       "        [-0.4845],\n",
       "        [-1.4015],\n",
       "        [-0.3124],\n",
       "        [-2.8999],\n",
       "        [-0.9364],\n",
       "        [ 0.3479],\n",
       "        [-1.0670],\n",
       "        [-2.3096],\n",
       "        [ 0.5729],\n",
       "        [-0.2985],\n",
       "        [ 0.9382],\n",
       "        [ 0.0115],\n",
       "        [-0.0177],\n",
       "        [ 1.6096],\n",
       "        [ 0.0627],\n",
       "        [ 1.4500],\n",
       "        [ 0.3364],\n",
       "        [ 1.0698],\n",
       "        [-0.0485],\n",
       "        [ 0.1934],\n",
       "        [ 0.5109],\n",
       "        [ 1.3763],\n",
       "        [ 0.2197],\n",
       "        [ 0.5998],\n",
       "        [-0.3130],\n",
       "        [ 1.2136],\n",
       "        [-1.3719],\n",
       "        [ 0.8770],\n",
       "        [-0.3848],\n",
       "        [ 0.2177],\n",
       "        [-0.0291],\n",
       "        [-0.4609],\n",
       "        [ 0.8566],\n",
       "        [-0.8841],\n",
       "        [ 0.4599],\n",
       "        [ 1.1159],\n",
       "        [-1.6790],\n",
       "        [-1.7559],\n",
       "        [ 0.3858],\n",
       "        [-0.7529],\n",
       "        [-1.1984],\n",
       "        [ 0.9233],\n",
       "        [ 0.1201],\n",
       "        [-0.6119],\n",
       "        [ 0.5814],\n",
       "        [ 0.8219],\n",
       "        [ 0.3823],\n",
       "        [ 0.4205],\n",
       "        [-0.9495],\n",
       "        [ 0.2699],\n",
       "        [ 1.8066],\n",
       "        [ 1.5769],\n",
       "        [-0.2897],\n",
       "        [ 1.3077],\n",
       "        [-1.6017],\n",
       "        [ 0.4260],\n",
       "        [-0.1753],\n",
       "        [ 0.3743],\n",
       "        [ 2.1173],\n",
       "        [-1.0976],\n",
       "        [ 0.2195],\n",
       "        [-0.0274],\n",
       "        [-0.5415],\n",
       "        [-0.3558],\n",
       "        [-1.2152],\n",
       "        [ 0.6339],\n",
       "        [-0.5122],\n",
       "        [-0.0460],\n",
       "        [ 0.6351],\n",
       "        [ 2.1699],\n",
       "        [-0.5391],\n",
       "        [ 0.8199],\n",
       "        [ 0.2247],\n",
       "        [-0.7360],\n",
       "        [ 0.3564],\n",
       "        [-0.2612],\n",
       "        [-0.0213],\n",
       "        [ 0.7207],\n",
       "        [-0.2644],\n",
       "        [ 0.0042],\n",
       "        [-0.7910],\n",
       "        [ 0.8843],\n",
       "        [ 1.2866],\n",
       "        [ 2.9640],\n",
       "        [-1.1800],\n",
       "        [ 2.0579],\n",
       "        [-0.4582],\n",
       "        [-0.5839],\n",
       "        [-0.8870],\n",
       "        [-0.1416],\n",
       "        [ 0.0792],\n",
       "        [ 0.5945],\n",
       "        [-0.5687],\n",
       "        [ 0.5785],\n",
       "        [ 3.0883],\n",
       "        [ 0.2612],\n",
       "        [-0.6597],\n",
       "        [ 0.3947],\n",
       "        [-0.3563],\n",
       "        [ 0.2698],\n",
       "        [-2.1329],\n",
       "        [-0.0665],\n",
       "        [ 0.5893],\n",
       "        [-0.9953],\n",
       "        [ 0.2560],\n",
       "        [ 1.2873],\n",
       "        [ 1.2248],\n",
       "        [ 0.2388],\n",
       "        [ 0.1590],\n",
       "        [-1.7923],\n",
       "        [ 1.2396],\n",
       "        [ 1.6170],\n",
       "        [-0.2990],\n",
       "        [-0.4146],\n",
       "        [-1.3248],\n",
       "        [-1.2056],\n",
       "        [-0.9059],\n",
       "        [-1.7329],\n",
       "        [-0.6472],\n",
       "        [-0.2775],\n",
       "        [ 0.7523],\n",
       "        [-0.4447],\n",
       "        [-0.1093],\n",
       "        [-1.0766],\n",
       "        [-0.2607],\n",
       "        [-2.0443],\n",
       "        [-0.9382],\n",
       "        [ 0.0710],\n",
       "        [ 0.7677],\n",
       "        [ 0.9576],\n",
       "        [ 0.7577],\n",
       "        [-0.5647],\n",
       "        [ 0.0113],\n",
       "        [-1.6190],\n",
       "        [-0.8728],\n",
       "        [-0.9477],\n",
       "        [-0.4772],\n",
       "        [ 1.7269],\n",
       "        [ 1.0626],\n",
       "        [ 0.6498],\n",
       "        [-1.5153],\n",
       "        [-0.7709],\n",
       "        [ 2.2194],\n",
       "        [-1.8108],\n",
       "        [-1.2405],\n",
       "        [-0.4566],\n",
       "        [ 0.5555],\n",
       "        [-1.5957],\n",
       "        [ 0.1971],\n",
       "        [-0.3185],\n",
       "        [-0.0717],\n",
       "        [-0.5015],\n",
       "        [-0.8691],\n",
       "        [-0.1489],\n",
       "        [ 2.9840],\n",
       "        [ 0.5669],\n",
       "        [ 0.2276],\n",
       "        [ 1.1145],\n",
       "        [ 0.6526],\n",
       "        [ 1.6012],\n",
       "        [ 1.2689],\n",
       "        [-1.4017],\n",
       "        [ 0.8526],\n",
       "        [-0.8376],\n",
       "        [ 2.4532],\n",
       "        [ 0.8453],\n",
       "        [ 0.2907],\n",
       "        [ 0.3392],\n",
       "        [-0.6982],\n",
       "        [-0.1664],\n",
       "        [ 0.4856],\n",
       "        [-0.8993],\n",
       "        [-0.9710],\n",
       "        [-0.0720],\n",
       "        [ 0.5251],\n",
       "        [-0.5991],\n",
       "        [-2.6102],\n",
       "        [ 0.7403],\n",
       "        [ 1.1294],\n",
       "        [-0.4721],\n",
       "        [-0.2221],\n",
       "        [ 0.1149],\n",
       "        [-0.9092],\n",
       "        [-1.2388],\n",
       "        [-2.2578],\n",
       "        [ 0.7914],\n",
       "        [-1.3435],\n",
       "        [ 1.7365],\n",
       "        [-1.7691],\n",
       "        [ 2.2052],\n",
       "        [ 0.0533],\n",
       "        [-0.9368],\n",
       "        [ 0.0165],\n",
       "        [-0.3708],\n",
       "        [ 0.2248],\n",
       "        [-0.7995],\n",
       "        [-0.6166],\n",
       "        [-1.8534],\n",
       "        [-0.3888],\n",
       "        [ 0.1096],\n",
       "        [ 0.0443],\n",
       "        [-0.6100],\n",
       "        [-0.3480],\n",
       "        [ 1.1450],\n",
       "        [ 0.0991],\n",
       "        [ 1.1639],\n",
       "        [ 1.6290],\n",
       "        [ 0.7965],\n",
       "        [ 1.4303],\n",
       "        [-1.6421],\n",
       "        [ 1.3805],\n",
       "        [-0.8967],\n",
       "        [ 0.8531],\n",
       "        [ 0.4328],\n",
       "        [-1.4195],\n",
       "        [ 0.5335],\n",
       "        [ 0.6816],\n",
       "        [ 0.5978],\n",
       "        [-1.3339],\n",
       "        [ 0.4236],\n",
       "        [ 1.1282],\n",
       "        [ 1.1214],\n",
       "        [-1.1396],\n",
       "        [-0.3442],\n",
       "        [ 0.8335],\n",
       "        [ 0.6062],\n",
       "        [ 0.3056],\n",
       "        [-0.9032],\n",
       "        [ 0.5305],\n",
       "        [-0.8229],\n",
       "        [ 0.6300],\n",
       "        [-0.6948],\n",
       "        [-0.0697],\n",
       "        [-1.2181],\n",
       "        [-0.9327],\n",
       "        [-0.3206],\n",
       "        [ 1.3385],\n",
       "        [ 0.1788],\n",
       "        [ 0.5470],\n",
       "        [ 0.7427],\n",
       "        [ 0.6966],\n",
       "        [-1.9724],\n",
       "        [-0.0947],\n",
       "        [-0.7459],\n",
       "        [-1.6420],\n",
       "        [ 2.0779],\n",
       "        [ 0.0671],\n",
       "        [-1.4921],\n",
       "        [ 1.2030],\n",
       "        [ 0.1127],\n",
       "        [ 0.1111],\n",
       "        [ 0.9835],\n",
       "        [-0.4681],\n",
       "        [-0.1926],\n",
       "        [ 0.7899],\n",
       "        [ 0.1099],\n",
       "        [-0.2606],\n",
       "        [ 1.4206],\n",
       "        [ 0.3239],\n",
       "        [ 1.3200],\n",
       "        [ 0.9881],\n",
       "        [ 0.1133],\n",
       "        [ 0.6585],\n",
       "        [ 1.1794],\n",
       "        [ 0.6981],\n",
       "        [-0.2303],\n",
       "        [ 1.4779],\n",
       "        [-1.0493],\n",
       "        [-1.0736],\n",
       "        [ 0.9639],\n",
       "        [ 1.3783],\n",
       "        [-1.0855],\n",
       "        [-1.2822],\n",
       "        [-1.9800],\n",
       "        [-0.1646],\n",
       "        [-0.0868],\n",
       "        [ 0.0839],\n",
       "        [ 0.3078],\n",
       "        [-0.5389],\n",
       "        [-1.5748],\n",
       "        [ 1.0163],\n",
       "        [ 1.0787],\n",
       "        [ 0.2210],\n",
       "        [ 1.7427],\n",
       "        [ 1.0057],\n",
       "        [ 0.2611],\n",
       "        [-1.3282],\n",
       "        [ 0.2328],\n",
       "        [ 1.2138],\n",
       "        [-0.1346],\n",
       "        [ 0.8453],\n",
       "        [-0.9759],\n",
       "        [-1.6318],\n",
       "        [-1.3088],\n",
       "        [ 1.2406],\n",
       "        [-1.5679],\n",
       "        [ 1.2091],\n",
       "        [-0.3745],\n",
       "        [-0.0359],\n",
       "        [-2.3297],\n",
       "        [ 0.4245],\n",
       "        [ 0.7032],\n",
       "        [ 0.7003],\n",
       "        [ 0.8502],\n",
       "        [ 0.1426],\n",
       "        [-0.0375],\n",
       "        [-0.2753],\n",
       "        [ 1.7730],\n",
       "        [-0.9235],\n",
       "        [ 0.7329],\n",
       "        [ 1.4699],\n",
       "        [-1.2937],\n",
       "        [ 0.9871],\n",
       "        [ 1.1401],\n",
       "        [ 2.3624],\n",
       "        [-0.7751],\n",
       "        [ 0.5253],\n",
       "        [-1.7892],\n",
       "        [ 0.7839],\n",
       "        [-0.2786]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bb99eeb-c4aa-4c6d-87a2-75ffa7e511e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  9.,  40.,  99., 148., 174., 156., 178., 131.,  53.,  12.]),\n",
       " array([-3.92286921, -3.15244029, -2.38201137, -1.61158245, -0.84115353,\n",
       "        -0.07072461,  0.69970431,  1.47013323,  2.24056215,  3.01099107,\n",
       "         3.78141999]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ppz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0759273d-e80a-413f-bd3c-7f6955419b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06320796876773238"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppz[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "99ad0512-a947-4f84-a473-3e2401fb9281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  0.,  4.,  2.,  3.,\n",
       "         2.,  1.,  3.,  0.,  0.,  3.,  2.,  8.,  5.,  4.,  4., 10.,  4.,\n",
       "         8.,  7., 10., 10., 12., 19., 16., 14., 17., 24., 27., 36., 35.,\n",
       "        26., 54., 32., 40., 44., 49., 45., 58., 54., 43., 31., 34., 22.,\n",
       "        29., 19., 16., 12., 19., 15., 12., 11.,  9.,  2.,  5.,  6.,  2.,\n",
       "         0.,  4.,  1.,  3.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.]),\n",
       " array([-2.11697197, -2.07361054, -2.03024936, -1.98688793, -1.94352663,\n",
       "        -1.90016532, -1.85680389, -1.81344259, -1.77008128, -1.72671998,\n",
       "        -1.68335855, -1.63999724, -1.59663594, -1.55327463, -1.50991321,\n",
       "        -1.4665519 , -1.42319059, -1.37982929, -1.33646786, -1.29310656,\n",
       "        -1.24974525, -1.20638394, -1.16302252, -1.11966121, -1.07629991,\n",
       "        -1.03293848, -0.98957717, -0.94621587, -0.9028545 , -0.8594932 ,\n",
       "        -0.81613183, -0.77277052, -0.72940916, -0.68604785, -0.64268649,\n",
       "        -0.59932518, -0.55596381, -0.51260251, -0.46924114, -0.42587981,\n",
       "        -0.38251847, -0.33915713, -0.2957958 , -0.25243446, -0.20907313,\n",
       "        -0.16571179, -0.12235046, -0.07898912, -0.03562779,  0.00773355,\n",
       "         0.05109489,  0.09445623,  0.13781756,  0.1811789 ,  0.22454023,\n",
       "         0.26790157,  0.31126291,  0.35462424,  0.39798558,  0.44134691,\n",
       "         0.48470825,  0.52806962,  0.57143092,  0.61479229,  0.65815359,\n",
       "         0.70151496,  0.74487627,  0.78823763,  0.83159894,  0.8749603 ,\n",
       "         0.91832161,  0.96168298,  1.00504434,  1.04840565,  1.09176695,\n",
       "         1.13512826,  1.17848969,  1.22185099,  1.2652123 ,  1.30857372,\n",
       "         1.35193503,  1.39529634,  1.43865764,  1.48201907,  1.52538037,\n",
       "         1.56874168,  1.61210299,  1.65546441,  1.69882572,  1.74218702,\n",
       "         1.78554833,  1.82890975,  1.87227106,  1.91563237,  1.95899367,\n",
       "         2.0023551 ,  2.04571629,  2.08907771,  2.13243914,  2.17580032,\n",
       "         2.21916175]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAGcCAYAAABwemJAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaR0lEQVR4nO3df2zc913H8ZfjuUahtat2axtnWR0LJdsqTYC60Kw1ii0kEAUTMVRCWfwPqI1Uddqi/WgkYCoEUvYHaxQGKEWo6ioo+1HLQx3LJORhR9wfQa3kgNaJqHZw3TZaaXWuaOs5veMPFKtOnNV279PzOY+H9JXmy/nr93Tz/NTne5/7ttXr9XoAAArZ1OwBAICNTWwAAEWJDQCgKLEBABQlNgCAosQGAFCU2AAAinpfswdIklqtlhdeeCHXXHNN2tramj0OALAC9Xo9r732Wnp6erJp0+XXL9ZFbLzwwgvZtm1bs8cAANZgZmYmH/zgBy/772uOjR/84AcZGRnJtm3bsnfv3lxzzTVrPdXi987MzKSrq2vN5wEA3jtzc3PZtm3bOzbAmmLjq1/9ap544ok88cQT2bp1a5Jkamoqf/7nf56f//mfz8mTJ/Mnf/Inufnmm1d0vguXTrq6usQGALSYd3oLxKpj48knn8wf//Ef5z/+4z/ygQ98IMn/v+diaGgoR48ezeDgYLZv3559+/alUqmsbWoAYMNY1W6U8+fP5zOf+Uw+97nPLYZGkpw4cSJnzpxJf39/kmRwcDCTk5M5depUY6cFAFrOqmJjfHw8MzMz+eEPf5i9e/fmIx/5SJ544olUKpX09fWlo6MjSdLe3p6+vr6MjY0te575+fnMzc0tOQCAjWlVl1EmJydz7bXX5stf/nKuu+66fPe7383Q0FAGBgYuea9Fd3d3Zmdnlz3PkSNH8uCDD659agCgZaxqZeONN97IRz7ykVx33XVJkl/5lV/JjTfemJMnTy6ualxQq9VSq9WWPc+hQ4dSrVYXj5mZmTWODwCsd6uKjZtuuin/+7//u+SxD37wg/nCF75wyaWQarW6uFPlYp2dnYs7T+xAAYCNbVWxcfvtt2d6ejrnz59ffOzNN99M8v9bX+v1epJkYWEh09PTGRgYaOCoAEArWlVs7NixIz/7sz+b733ve0mSV155JS+//HI+//nPp6enJxMTE0n+/42kvb292bVrV+MnBgBayqo/Z+NrX/taPve5z2VycjJTU1P5xje+kc2bN2d0dDSHDx/O6dOnU6lUMjIy4j4nAEDa6heufTTR3Nxcuru7U61WvX8DAFrESv9+u8U8AFCU2AAAihIbAEBRYgMAKEpsAABFrXrrK8BG0/vAU5c8Nv3QnU2YBDYmKxsAQFFiAwAoSmwAAEWJDQCgKLEBABQlNgCAosQGAFCU2AAAihIbAEBRYgMAKEpsAABFiQ0AoCixAQAUJTYAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICixAYAUNT7mj0AwHJ6H3jqksemH7qzaT//vfzZsNFY2QAAihIbAEBRYgMAKEpsAABFiQ0AoCixAQAUJTYAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICixAYAUJTYAACKEhsAQFHvKjZeeOGFRs0BAGxQq4qNer2eHTt2pK2tLW1tbfnUpz6VJJmamsqBAwdy/PjxDA8P5+zZs0WGBQBaz/tW8+R//ud/zqc//encdtttSZLt27enVqtlaGgoR48ezeDgYLZv3559+/alUqkUGRgAaC2rWtn4y7/8y7S3t+eGG27Irbfemuuvvz4nTpzImTNn0t/fnyQZHBzM5ORkTp06VWRgAKC1rDg2XnvttczPz+cP/uAPsn379tx///2p1+upVCrp6+tLR0dHkqS9vT19fX0ZGxu77Lnm5+czNze35AAANqYVX0a55ppr8i//8i9ZWFjI3/zN3+Szn/1sfuZnfibnzp1LV1fXkud2d3dndnb2suc6cuRIHnzwwbVPDQC0jFXvRuno6Mj999+fBx54IH//93+fjo6OxVWNC2q1Wmq12mXPcejQoVSr1cVjZmZm9ZMDAC1hzVtff+M3fiPVajVbtmy55DJItVrN1q1bL/u9nZ2d6erqWnIAABvTmmPjrbfeys6dO7Nnz55MTU2lXq8nSRYWFjI9PZ2BgYGGDQkAtK4Vx8bExEQef/zxxag4fvx4Pv/5z2f37t3p6enJxMREkmR8fDy9vb3ZtWtXmYkBgJay4jeIzszM5DOf+Uz+4R/+IbfddluGh4dzxx13JElGR0dz+PDhnD59OpVKJSMjI2lrays2NADQOlYcG3fffXfuvvvuZf9tx44deeyxx5Ik9913X2MmAwA2hFV9gihAq+l94KlLHpt+6M4mTAJXLnd9BQCKEhsAQFFiAwAoSmwAAEWJDQCgKLEBABQlNgCAosQGAFCU2AAAihIbAEBRYgMAKEpsAABFiQ0AoCixAQAUJTYAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICi3tfsAQDea70PPNXsEeCKYmUDAChKbAAARYkNAKAosQEAFCU2AICi7EYBNhQ7TWD9sbIBABQlNgCAosQGAFCU2AAAihIbAEBRdqMArMByu1ymH7qzCZNA67GyAQAUJTYAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICixAYAUJTYAACKEhsAQFFrio3XX389H/3oRzM9PZ0kmZqayoEDB3L8+PEMDw/n7NmzjZwRAGhha7rF/LFjx/KDH/wgSVKr1TI0NJSjR49mcHAw27dvz759+1KpVBo6KADQmla9sjE6OpqBgYHFr0+cOJEzZ86kv78/STI4OJjJycmcOnWqcVMCAC1rVbHx3//933nxxReza9euxccqlUr6+vrS0dGRJGlvb09fX1/GxsYue575+fnMzc0tOQCAjWnFl1HeeuutPPLII3nwwQeXPH7u3Ll0dXUteay7uzuzs7OXPdeRI0cuOQ/AavU+8FSzRwBWYMUrG1/96ldz7733ZtOmpd/S0dGxuKpxQa1WS61Wu+y5Dh06lGq1unjMzMyscmwAoFWseGXj2LFj+cIXvrDksZ07d6ZWq+WWW25Z8ni1Ws3WrVsve67Ozs50dnauclQAoBWtODb+67/+a8nXbW1t+eEPf5jZ2dn86q/+aur1etra2rKwsJDp6eklbyIFAK5c7/pDvXbv3p2enp5MTEwkScbHx9Pb27vkTaQAwJVrTZ+z8XabNm3K6OhoDh8+nNOnT6dSqWRkZCRtbW2NmA8AaHFrjo16vb74n3fs2JHHHnssSXLfffe9+6kAgA3DvVEAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICixAYAUJTYAACKEhsAQFFiAwAoSmwAAEWJDQCgKLEBABQlNgCAot7X7AGAja/3gaeWfD390J1NmgRoBisbAEBRYgMAKEpsAABFiQ0AoCixAQAUZTcK8J67eHdKYocKbGRWNgCAosQGAFCU2AAAihIbAEBRYgMAKEpsAABF2foK0CC29MLyrGwAAEWJDQCgKLEBABQlNgCAosQGAFCU2AAAihIbAEBRYgMAKEpsAABFiQ0AoCixAQAU5d4oQMtY7t4jwPpnZQMAKEpsAABFiQ0AoCixAQAUJTYAgKLsRgFYI7tjYGWsbAAARa06Np555pnccccdue666/JLv/RLefnll5MkU1NTOXDgQI4fP57h4eGcPXu24cMCAK1nVbHx5ptv5lvf+la+973vZWZmJq+//nr+4i/+IrVaLUNDQ7nrrrtyzz33ZP/+/dm3b1+pmQGAFrKq2KhWq/mjP/qjbN68OT/90z+d/v7+bNq0KSdOnMiZM2fS39+fJBkcHMzk5GROnTpVZGgAoHWsKjZuvPHGXHXVVUmSH//4x3nppZfy2c9+NpVKJX19feno6EiStLe3p6+vL2NjY8ueZ35+PnNzc0sOAGBjWtMbRL/zne/ktttuy9jYWP7zP/8z586dS1dX15LndHd3Z3Z2dtnvP3LkSLq7uxePbdu2rWUMAKAFrCk2fvmXfznf/OY384lPfCKf+tSn0tHRsbiqcUGtVkutVlv2+w8dOpRqtbp4zMzMrGUMAKAFrOlzNi5cJvm7v/u7XH/99fnABz5wyaWQarWarVu3Lvv9nZ2d6ezsXMuPBgBazLv6nI3Nmzfn/e9/f/bs2ZOpqanU6/UkycLCQqanpzMwMNCQIQGA1rWq2Pif//mf/NM//dNiVPzrv/5r9u/fn/7+/vT09GRiYiJJMj4+nt7e3uzatavxEwMALWVVl1Gmpqby+7//+9m5c2d+67d+K1dffXX+9E//NG1tbRkdHc3hw4dz+vTpVCqVjIyMpK2trdTcAECLWFVs3HrrrTl37tyy/7Zjx4489thjSZL77rvv3U8GsAEtdz+V6YfubMIk8N5xbxQAoCixAQAUJTYAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICiVnVvFIB3sty9P4Arm5UNAKAosQEAFCU2AICixAYAUJTYAACKshsFWBfsYoGNy8oGAFCU2AAAihIbAEBRYgMAKEpsAABFiQ0AoCixAQAUJTYAgKLEBgBQlNgAAIoSGwBAUe6NArwr7mkCvBMrGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICixAYAUJTYAACKEhsAQFFiAwAoSmwAAEW5NwpAQe4dA1Y2AIDCxAYAUJTYAACKEhsAQFFiAwAoSmwAAEWJDQCgqFXFxre//e3s3LkzXV1d+eQnP5lXXnklSTI1NZUDBw7k+PHjGR4eztmzZ4sMCwC0nhXHxnPPPZennnoqTz75ZB599NF8//vfzxe/+MXUarUMDQ3lrrvuyj333JP9+/dn3759JWcGAFrIij9B9OTJkzl27Fiuuuqq3HLLLZmcnMw3vvGNnDhxImfOnEl/f3+SZHBwMHv37s2pU6fy8Y9/vNjgAEBrWPHKxvDwcK666qrFr2+88cZ86EMfSqVSSV9fXzo6OpIk7e3t6evry9jYWOOnBQBazprfIPr000/n3nvvzblz59LV1bXk37q7uzM7O3vZ752fn8/c3NySAwDYmNYUGy+++GLOnz+fvXv3pqOjY3FV44JarZZarXbZ7z9y5Ei6u7sXj23btq1lDACgBaw6Nt566608/PDDOXbsWJJky5Ytl6xMVKvVbN269bLnOHToUKrV6uIxMzOz2jEAgBax6tj4yle+koMHD+bqq69Oktxxxx2ZmppKvV5PkiwsLGR6ejoDAwOXPUdnZ2e6urqWHADAxrTi3ShJ8vDDD2fHjh159dVX8+qrr+a5557L+fPn09PTk4mJifziL/5ixsfH09vbm127dpWaGQBoISuOja9//es5ePDg4gpGkmzevDkvvfRSRkdHc/jw4Zw+fTqVSiUjIyNpa2srMjAA0Fra6m+vhyaZm5tLd3d3qtWqSyrQYnofeKrZI7S86YfubPYIsCYr/fu9qssowMZxcST4g9c8a3ktlos8ryHrlRuxAQBFiQ0AoCixAQAUJTYAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFOXeKMCKuekasBZWNgCAosQGAFCU2AAAihIbAEBRYgMAKMpuFCDJ8jtNph+6swmT4LVgo7GyAQAUJTYAgKLEBgBQlNgAAIoSGwBAUXajAGwQF+9isYOF9cLKBgBQlNgAAIoSGwBAUWIDAChKbAAARdmNAlzWcvfoAFgtKxsAQFFiAwAoSmwAAEWJDQCgKLEBABQlNgCAosQGAFCU2AAAihIbAEBRYgMAKEpsAABFtdXr9Xqzh5ibm0t3d3eq1Wq6urqaPQ5sOO5xwgXTD93Z7BHYQFb699vKBgBQlNgAAIoSGwBAUWIDAChKbAAARb2v2QMA8N65eGeS3Sm8F9a8svHGG2+kWq02chYAYANadWzUarU8+uij2bFjR5555pnFx6empnLgwIEcP348w8PDOXv2bEMHBQBa06ovo7z88ssZGBjI888/v/hYrVbL0NBQjh49msHBwWzfvj379u1LpVJp6LAAQOtZ9crGDTfckJtvvnnJYydOnMiZM2fS39+fJBkcHMzk5GROnTrVmCkBgJbVkN0olUolfX196ejoSJK0t7enr68vY2Njyz5/fn4+c3NzSw4AYGNqyG6Uc+fOXfKZ6N3d3ZmdnV32+UeOHMmDDz7YiB8NLWu5+5WsZWeA+54A611DVjY6OjoWVzUuqNVqqdVqyz7/0KFDqVari8fMzEwjxgAA1qGGrGxs2bIlJ0+eXPJYtVrN1q1bl31+Z2dnOjs7G/GjAYB1riErG3v27MnU1FQu3K1+YWEh09PTGRgYaMTpAYAWtqbYuPjyyO7du9PT05OJiYkkyfj4eHp7e7Nr1653PyEA0NJWfRnlRz/6UR555JEkyeOPP56bbropH/7whzM6OprDhw/n9OnTqVQqGRkZSVtbW8MHBgBaS1v9wrWPJpqbm0t3d3eq1eolu1pgo7IbhfXK/VJYqZX+/XbXVwCgKLEBABQlNgCAosQGAFCU2AAAimrIJ4jClaRRu0jW288CKMXKBgBQlNgAAIoSGwBAUWIDAChKbAAARdmNAi3GvVCAVmNlAwAoSmwAAEWJDQCgKLEBABQlNgCAouxGAWDVLt4V5Z49/CRWNgCAosQGAFCU2AAAihIbAEBRYgMAKEpsAABF2foKDWAbIBuZm//xblnZAACKEhsAQFFiAwAoSmwAAEWJDQCgKLtRaKqNuotjre/e965/NrLl/ve9UX7n+cmsbAAARYkNAKAosQEAFCU2AICixAYAUJTdKKx7a30H+1p2uni3PDSf392Nx8oGAFCU2AAAihIbAEBRYgMAKEpsAABF2Y1CMY16R3mjzr1W7lfClWYt/5u3G4SfxMoGAFCU2AAAihIbAEBRYgMAKEpsAABFbfjdKCXfIX2lfH5/K+zGaIUZgUv53V29lfzteS937K2ElQ0AoKiGxcbU1FQOHDiQ48ePZ3h4OGfPnm3UqQGAFtaQyyi1Wi1DQ0M5evRoBgcHs3379uzbty+VSqURpwcAWlhDVjZOnDiRM2fOpL+/P0kyODiYycnJnDp1qhGnBwBaWENWNiqVSvr6+tLR0ZEkaW9vT19fX8bGxvLxj3/8kufPz89nfn5+8etqtZokmZuba8Q4S9TmX7/ksUb9nIvPvZLzlpynlOVmXovl/ns26txr/fkXey/ngY3u4t+5tf5+bdT/b12rlfztWcvfp7W4cN56vf6Tn1hvgHvuuad+2223LXns9ttvr3/6059e9vlf+tKX6kkcDofD4XBsgGNmZuYndkJDVjY6OjoWVzUuqNVqqdVqyz7/0KFDOXjw4JLnvvLKK7n++uvT1tbWiJHWZG5uLtu2bcvMzEy6urqaNgfL8/qsb16f9c3rs7616utTr9fz2muvpaen5yc+ryGxsWXLlpw8eXLJY9VqNVu3bl32+Z2dnens7Fzy2LXXXtuIURqiq6urpV7sK43XZ33z+qxvXp/1rRVfn+7u7nd8TkPeILpnz55MTU0tXrNZWFjI9PR0BgYGGnF6AKCFNSQ2du/enZ6enkxMTCRJxsfH09vbm127djXi9ABAC2vIZZRNmzZldHQ0hw8fzunTp1OpVDIyMtLU91+sRWdnZ770pS9dcomH9cHrs755fdY3r8/6ttFfn7b6O+5XAQBYO/dGAQCKEhsAQFFiAwAoSmzQ8l566aVmjwAt4Y033li8PQS8l8TGZczNzeV3f/d3c+2116avry//+I//2OyRuMjU1FTuvvvu3H333c0e5Yo3NTWVAwcO5Pjx4xkeHs7Zs2ebPRJvU6vV8uijj2bHjh155plnmj0OF/n2t7+dnTt3pqurK5/85CfzyiuvNHukhhMbl/Fnf/Zn+Z3f+Z2Mj4/nE5/4RPbv35+pqalmj8Xb1Ov1XHfddZf9WHzeG7VaLUNDQ7nrrrtyzz33ZP/+/dm3b1+zx+JtXn755QwMDOT5559v9ihc5LnnnstTTz2VJ598Mo8++mi+//3v54tf/GKzx2o4sbGMhYWFfPSjH82v/dqv5WMf+1j+9m//Nps2bcq///u/N3s03qavry/vf//7mz3GFe/EiRM5c+ZM+vv7kySDg4OZnJzMqVOnmjwZF9xwww25+eabmz0Gyzh58mSOHTuWW265Jb/5m7+Z+++/P//2b//W7LEaTmwso6OjI8PDw4tf/9RP/VS6u7vzoQ99qIlTwfpUqVTS19e3eDPG9vb29PX1ZWxsrMmTwfo3PDycq666avHrG2+8cUP+rREbK/D8889n69at+YVf+IVmjwLrzrlz5y65cVR3d3dmZ2ebNBG0rqeffjr33ntvs8doOLGxAn/913+d48ePN3sMWJc6OjoWVzUuqNVq3ksDq/Tiiy/m/Pnz2bt3b7NHabiG3Bul1bz44ov5uZ/7ucv++2//9m/n6NGjSZKxsbF87GMfy6233vpejXfFW83rQ/Nt2bIlJ0+eXPJYtVrN1q1bmzQRtJ633norDz/8cI4dO9bsUYq4ImNjy5YtK/pshmeffTbPPfdcfu/3fi9Jcv78+bS3t7fcDeZazUpfH9aHPXv25Mtf/nLq9Xra2tqysLCQ6enpDAwMNHs0aBlf+cpXcvDgwVx99dVJkh//+MdL3svR6lxGuYyXXnopf/VXf5Xbb789zz77bCYnJ3PkyJFmj8VFLNU33+7du9PT05OJiYkkyfj4eHp7e7Nr164mT8bb+V1Zvx5++OHs2LEjr776ap599tl85zvfyXe/+91mj9VQV+TKxjt58803c+edd+bpp59esqT1h3/4h1Y11pHx8fGMjo5mdnY2Tz75ZH7913/9kvcOUN6mTZsyOjqaw4cP5/Tp06lUKhkZGfG7so786Ec/yiOPPJIkefzxx3PTTTflwx/+cJOnIkm+/vWv5+DBg3n7Ddg3b9684VZ33WIeACjKZRQAoCixAQAUJTYAgKLEBgBQlNgAAIoSGwBAUWIDAChKbAAARYkNAKAosQEAFCU2AICi/g9g1ZIO9L0YCQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ypred, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a1553335-9913-49da-98d8-a97a4011f52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25293207"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fa5e5-408a-4f27-9be9-a518ba50d9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df362e62-98dd-4c6e-b6a0-aa2d3a9e7b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767868c1-c425-4f69-ac21-46cf0f0d1abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3081da-ea32-4c2e-8615-51ab626a8d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bd1ad-9c17-4846-8cbd-02b05a54a1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de267aa7-5079-4c00-8a83-9399ca59070f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d462c6f-db0c-4439-ac84-c41ff5704fb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0e937f99-22c8-4d7b-8db2-a2bba14b6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "e6fbfa63-3f7d-4619-a910-d39097ba69af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_distribution = D.normal.Normal(loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "75c1b13e-bef8-4ad1-9af5-acd09be079c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability of the sample in the base distribution:\n",
      "tensor([-90.7244])\n"
     ]
    }
   ],
   "source": [
    "# Sample from the base Gaussian distribution\n",
    "z_sample = torch.Tensor([-13.4019])\n",
    "\n",
    "# Compute log probability for a given sample\n",
    "log_prob_z = base_distribution.log_prob(z_sample)\n",
    "print(\"Log probability of the sample in the base distribution:\")\n",
    "print(log_prob_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b72d9238-ff08-4071-93e0-be175676dadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1044e+00,  5.3608e-01,  2.7223e-01, -4.0974e-01, -1.2630e+00,\n",
       "         2.7436e-01,  4.5724e-01,  7.1562e-01,  2.5585e-01, -1.7401e+00,\n",
       "         3.6557e-01,  1.9815e+00,  8.3127e-02, -2.8310e-02,  1.5944e-01,\n",
       "        -1.5677e+00,  8.7571e-01, -1.4026e+00,  9.6980e-01, -5.7682e-01,\n",
       "         2.6239e-01,  5.7484e-01, -5.7243e-01, -2.1229e-01,  6.5120e-01,\n",
       "        -4.3072e-01,  4.2276e-01,  7.4024e-01, -5.8200e-01, -1.1932e+00,\n",
       "        -7.1932e-01,  2.9953e-01,  1.0649e+00, -1.2855e-01,  3.5041e-01,\n",
       "         4.8927e-01,  6.0499e-01,  1.1458e+00, -6.1707e-01,  2.1834e+00,\n",
       "         9.1538e-01,  7.9090e-01,  6.4550e-01,  9.1880e-01,  3.5349e-01,\n",
       "         8.1434e-01,  1.1906e-01, -5.1368e-01, -8.0531e-01,  3.7218e-01,\n",
       "        -2.8462e-01,  5.1760e-01,  5.2333e-01, -1.1095e+00, -1.7090e-01,\n",
       "        -5.8244e-02, -3.0572e-01,  2.3676e-01,  4.0046e-01,  9.4080e-01,\n",
       "         1.8561e+00, -5.2990e-01,  5.1445e-02, -6.2367e-02,  5.9578e-01,\n",
       "         1.5714e-01,  6.5467e-01,  5.2255e-01, -5.2550e-01, -4.4779e-01,\n",
       "         9.3960e-01,  8.7942e-01,  5.3973e-02,  5.6147e-01, -1.0643e+00,\n",
       "        -5.3023e-01, -1.5540e+00, -1.2863e-01,  9.1037e-01, -4.5574e-01,\n",
       "        -2.3140e-02,  1.2005e-01,  1.5731e+00,  1.6403e-01,  8.3374e-01,\n",
       "        -3.0596e-01,  6.6844e-01, -1.3816e+00, -4.3452e-01, -2.6365e-01,\n",
       "        -8.3384e-01,  2.4906e-01, -1.1455e+00, -1.9485e-01, -2.0116e+00,\n",
       "         7.8016e-02, -9.5747e-01,  1.4937e-01, -2.5925e-01, -1.5145e+00,\n",
       "         3.1507e-01, -6.2256e-01,  1.5290e+00,  6.3536e-01,  4.4222e-01,\n",
       "        -1.1336e+00,  8.3353e-01, -1.3868e-02, -1.4489e+00, -4.7428e-01,\n",
       "         2.0545e+00, -4.0540e-01, -9.5925e-01,  1.4628e-01,  9.3416e-01,\n",
       "         1.2972e-01,  2.8183e+00, -9.3750e-01, -1.3405e+00,  9.7499e-01,\n",
       "         1.4499e+00,  2.3749e-01, -3.5307e-01,  1.7120e-01,  1.4509e+00,\n",
       "        -5.8411e-01, -1.6602e+00, -2.8575e-01, -8.8415e-01,  4.7089e-01,\n",
       "        -5.0467e-01,  2.1970e-01, -8.0355e-01, -6.7352e-01, -1.8423e+00,\n",
       "         1.3197e+00,  7.5914e-01, -1.4738e+00, -2.4028e-01,  2.0500e+00,\n",
       "         2.5433e-02,  1.3753e+00,  3.0881e-01, -7.2112e-01, -6.8736e-01,\n",
       "        -4.6874e-02, -1.4603e-01,  1.2329e+00, -4.2673e-01, -1.3867e+00,\n",
       "         1.6686e+00,  4.1296e-01, -6.9377e-01, -1.2500e+00, -4.4740e-01,\n",
       "        -1.5826e+00, -4.4897e-01,  1.3707e+00,  8.3077e-01,  6.6048e-02,\n",
       "         5.5191e-01, -2.4148e+00,  2.8505e-01, -2.8783e-01,  2.9958e-01,\n",
       "         6.1308e-01,  1.2282e+00,  4.6869e-01, -1.7393e+00, -3.4317e-01,\n",
       "        -7.1529e-01,  1.1789e+00, -8.7934e-01,  5.3453e-01,  3.5537e-01,\n",
       "        -1.2870e+00,  1.2393e+00,  1.3256e+00,  4.3599e-01,  7.0790e-01,\n",
       "        -2.9851e-01, -2.5265e+00,  1.3229e+00, -3.3910e-01, -1.8691e-01,\n",
       "        -6.6734e-02,  2.7792e-01, -8.5344e-01,  6.6565e-01, -8.7678e-01,\n",
       "        -3.7833e-01, -4.5579e-01, -1.7119e+00, -4.4352e-01, -1.4852e+00,\n",
       "        -7.9120e-01, -7.5951e-01, -9.6410e-01,  9.1380e-01, -5.1296e-01,\n",
       "        -6.8361e-01, -1.2658e+00,  2.4770e+00,  2.1466e-01, -3.8021e-01,\n",
       "        -1.2697e+00, -2.3289e-01, -1.1961e+00, -3.6432e-03, -1.5040e-01,\n",
       "         2.7548e-01, -1.7180e+00,  8.5279e-01,  1.6454e+00,  2.8954e-01,\n",
       "         2.3629e+00,  1.7833e+00, -7.7760e-01, -5.9648e-03,  1.7025e-01,\n",
       "        -9.8024e-03,  2.1076e-01, -3.6466e-01, -4.6469e-01,  1.2334e+00,\n",
       "        -6.9017e-01, -9.6955e-01, -2.1257e+00,  8.3989e-01, -1.1027e-01,\n",
       "        -6.6110e-01,  4.2615e-01, -1.1948e-01, -1.8326e+00, -2.4196e+00,\n",
       "        -2.0621e-01, -4.6798e-01, -4.8596e-01, -1.6059e+00, -8.3020e-01,\n",
       "        -2.2515e+00,  4.5996e-01,  1.5545e+00,  9.9873e-01,  3.9866e-01,\n",
       "         2.4323e-01,  1.3789e+00,  6.7689e-01, -2.6021e-02,  5.1618e-01,\n",
       "        -3.6143e-01, -9.7261e-01,  5.2108e-01, -8.0599e-01,  8.5165e-01,\n",
       "         1.4695e+00,  1.1753e+00, -3.0389e-01, -2.4687e-01,  2.5467e-01,\n",
       "        -3.8649e-01, -1.3182e+00,  7.6464e-01,  1.5648e+00,  3.0884e-01,\n",
       "         1.3561e-01, -9.0414e-01, -7.4026e-01, -4.4211e-01,  2.3025e-01,\n",
       "        -1.8918e-01, -4.2127e-01,  2.2499e-01,  5.1024e-01, -1.2259e+00,\n",
       "        -3.8215e-01,  3.7701e-01, -8.2513e-01,  1.2285e+00, -8.4012e-01,\n",
       "        -2.6810e-02, -4.3456e-01,  4.8574e-01, -1.0033e-01, -1.3786e+00,\n",
       "         1.0564e+00,  4.2144e-01,  6.4922e-01,  1.0002e+00, -1.7159e+00,\n",
       "         1.2216e+00, -8.9945e-03, -1.1934e+00, -2.8279e-01, -7.0490e-01,\n",
       "        -1.9081e-01, -9.7353e-01, -2.7479e+00, -4.5797e-01,  6.8582e-01,\n",
       "        -1.3553e+00,  2.7013e-01, -2.3826e-01, -1.6231e+00,  2.1371e-01,\n",
       "         5.6384e-01,  2.1960e-01,  2.4150e-01,  6.2061e-01, -1.0489e-01,\n",
       "         1.7517e+00,  6.7076e-01, -1.3978e-01, -8.2445e-02,  3.9530e-01,\n",
       "         5.1329e-01, -1.9216e-01,  1.1939e+00, -2.8467e+00,  1.5854e+00,\n",
       "         2.2636e-01,  1.0687e+00, -7.1430e-01,  1.1974e+00,  1.3638e-02,\n",
       "         1.1439e+00, -1.5609e+00, -4.2655e-01,  3.8575e-01,  7.7837e-01,\n",
       "        -4.0150e-01,  5.4693e-01,  1.9081e+00, -7.4662e-01, -4.7376e-01,\n",
       "         6.9946e-01, -1.9914e+00, -2.4258e-01, -9.3687e-01,  6.8541e-01,\n",
       "         1.0665e+00,  2.1555e-01, -2.4295e-01, -1.1957e+00,  6.0446e-01,\n",
       "         4.0213e-01,  1.4017e+00,  2.7869e-01, -9.5894e-01, -1.5638e+00,\n",
       "        -1.0110e-01,  4.6773e-01,  1.2827e+00, -9.1729e-01,  1.6297e+00,\n",
       "         4.5764e-01,  8.8216e-01,  2.2360e-01,  1.0108e+00,  7.2386e-01,\n",
       "         2.0961e+00,  2.4810e-01, -1.4284e+00,  5.6877e-01,  1.2502e+00,\n",
       "        -1.3409e-01, -3.0333e-01, -1.8817e-01, -2.2237e+00, -5.6682e-02,\n",
       "        -6.2232e-01, -4.6465e-01, -5.4288e-01,  7.8277e-01,  4.5549e-01,\n",
       "         5.9486e-01,  6.6038e-02,  1.5156e+00,  8.7062e-01, -4.8555e-01,\n",
       "         2.7363e-01, -2.2114e-01,  3.1821e-01,  6.6405e-01,  1.2378e+00,\n",
       "         4.2984e-01,  2.2287e-01, -1.2077e+00,  9.1887e-01,  3.2677e-01,\n",
       "        -7.3290e-01, -2.2486e+00,  8.0229e-01, -8.8998e-01, -1.9330e+00,\n",
       "         1.1855e+00,  6.3310e-01, -6.8345e-02, -1.1391e-01,  4.5725e-01,\n",
       "         1.6477e+00, -2.6330e-01,  5.4432e-01,  1.3206e+00,  2.0895e+00,\n",
       "         8.0402e-01, -1.7470e+00,  9.2834e-01, -8.7679e-01,  3.2405e-01,\n",
       "         6.0144e-01, -1.2082e+00,  1.2006e+00, -6.3478e-01,  9.0314e-01,\n",
       "        -1.3630e+00,  1.3225e+00, -3.9263e-02,  2.1543e-01, -5.2079e-01,\n",
       "         7.9209e-01,  2.1396e+00,  1.8184e+00, -1.5520e+00,  2.4373e-01,\n",
       "         4.9069e-01,  3.7375e-01,  5.0519e-01,  8.2489e-02, -8.1836e-01,\n",
       "        -3.7315e-01, -6.3905e-01,  1.4168e+00,  1.0993e+00, -4.3926e-01,\n",
       "         5.3011e-02, -2.4621e-01,  4.7379e-01, -3.1596e-01,  1.1466e+00,\n",
       "         1.9753e-02, -4.1847e-01, -1.6455e+00,  6.2989e-01,  3.0642e-01,\n",
       "        -3.4217e-01,  3.7732e-01,  5.7856e-01,  3.5808e-01, -6.4435e-01,\n",
       "        -1.2664e+00,  8.9284e-01,  8.6703e-01, -1.1849e+00,  2.2854e-01,\n",
       "        -8.9665e-01, -7.4685e-01, -8.3032e-01, -7.6299e-01,  6.6947e-01,\n",
       "        -6.4320e-01,  1.2720e+00, -4.8314e-01,  1.0369e+00, -8.2754e-02,\n",
       "        -3.6780e-01, -8.5765e-01, -2.6584e-01, -8.4592e-01,  2.0900e+00,\n",
       "        -3.3571e-01, -2.7365e-01,  4.2045e-01, -5.0189e-01,  2.1774e+00,\n",
       "         1.2684e+00,  1.0876e+00,  6.7469e-01,  1.8769e+00,  1.7561e+00,\n",
       "         6.3241e-02,  1.1177e-01, -6.9672e-01, -1.3041e-01, -1.5295e-01,\n",
       "         1.1734e+00,  2.0096e+00,  1.3440e+00,  4.1154e-01, -2.0337e+00,\n",
       "         2.9413e-01,  6.3165e-01,  6.6015e-02,  7.9139e-01, -4.2177e-01,\n",
       "        -9.5036e-01, -9.5504e-01, -2.5653e-01, -9.4473e-01, -1.0939e+00,\n",
       "         1.1041e-01, -5.3169e-01, -7.5699e-01,  5.3319e-01, -6.8033e-01,\n",
       "         5.3278e-01, -1.2020e+00, -1.9659e-01, -7.2072e-01,  3.5778e-01,\n",
       "         4.1554e-01, -1.4242e-01, -1.2204e+00,  1.3340e-01,  1.3910e+00,\n",
       "         2.8282e-02,  3.6570e-01,  5.4003e-01,  2.2741e+00,  7.6066e-02,\n",
       "        -1.1488e+00,  6.1675e-01, -9.8228e-01, -6.6162e-01,  1.6759e+00,\n",
       "        -1.1585e+00,  9.2543e-01, -4.0777e-01,  5.8472e-01,  5.7255e-01,\n",
       "         2.0956e+00,  1.6141e-01,  1.3108e+00, -4.2879e-01, -1.1836e-01,\n",
       "         3.0701e-01, -2.3178e+00,  1.8452e-01,  1.0042e+00, -1.0290e+00,\n",
       "         1.3379e+00, -7.2549e-01, -6.1016e-01, -1.3704e+00, -5.7965e-01,\n",
       "        -6.2787e-01,  4.3845e-02,  3.0263e-01, -2.3582e-01, -1.6874e+00,\n",
       "        -1.2794e-01, -6.3943e-01, -1.7925e+00,  4.9686e-01,  6.6856e-02,\n",
       "        -9.9491e-01,  1.2036e-01, -1.0225e+00, -4.4577e-01,  8.7954e-01,\n",
       "        -2.9278e-01,  1.1696e+00,  1.3101e-02, -2.4229e-01,  1.3554e+00,\n",
       "        -6.3189e-02,  5.8783e-01, -9.8299e-01,  9.6724e-01,  8.7076e-01,\n",
       "        -1.1199e+00, -1.5642e+00, -1.8559e+00, -8.2385e-01,  6.6140e-01,\n",
       "        -3.9254e-01,  6.1634e-01,  5.2854e-01,  1.0742e+00, -6.4902e-02,\n",
       "         5.3602e-02, -1.5826e+00,  1.0997e+00,  2.4608e-01, -3.8086e-01,\n",
       "         6.6185e-01, -9.8247e-01,  7.5218e-02, -4.1243e-02, -9.2047e-01,\n",
       "        -5.8027e-01,  6.8770e-01, -1.4893e+00, -9.4504e-01,  1.9702e-01,\n",
       "         6.6918e-01, -1.1709e+00, -7.0232e-01, -4.1784e-01,  8.1003e-01,\n",
       "        -2.7804e-01, -2.0031e-01,  7.0527e-01, -9.6937e-01,  1.1689e+00,\n",
       "         7.4006e-01, -2.5950e-01,  3.4501e-01, -2.3079e-01, -9.5328e-01,\n",
       "         4.5314e-02, -1.5330e+00, -3.3205e-01, -9.7520e-02, -1.1451e+00,\n",
       "         1.2243e+00,  4.0116e-01,  5.1238e-01,  1.0563e+00,  7.9822e-01,\n",
       "         3.2488e-04,  4.1915e-01, -1.8082e-01,  9.7909e-01, -5.6570e-02,\n",
       "         3.7949e-01, -4.4500e-02,  1.9252e-01, -9.0285e-01, -1.4165e+00,\n",
       "         6.6683e-01, -1.2207e+00,  2.6244e-01, -1.0841e+00, -3.8274e-01,\n",
       "         7.0959e-01,  9.6215e-01, -1.5444e+00,  4.9438e-03,  1.3579e+00,\n",
       "         4.7280e-01,  1.2425e+00,  2.5537e-01, -5.1405e-01,  6.0571e-02,\n",
       "        -3.0876e-01, -1.6462e+00, -1.1509e-01, -4.7552e-02, -2.6791e-01,\n",
       "        -4.3740e-01,  3.1958e-01, -1.2730e+00, -1.2231e+00, -1.4263e+00,\n",
       "         4.9323e-02, -1.3655e+00,  1.0031e+00,  2.3985e+00,  1.5285e+00,\n",
       "        -8.1026e-01,  3.5734e-01, -8.6429e-01,  1.7859e+00, -1.0850e+00,\n",
       "        -4.4293e-01,  1.5550e+00, -1.6774e-01,  1.4308e+00, -2.0135e+00,\n",
       "        -8.0758e-01,  6.4787e-01, -5.6576e-02, -3.9324e-01,  8.2594e-01,\n",
       "         5.1615e-01,  5.9828e-01,  2.7394e+00, -1.4974e+00,  1.3907e+00,\n",
       "         2.0605e+00,  1.7912e+00, -3.8680e-01, -1.8634e+00,  1.7960e-01,\n",
       "        -7.3744e-01, -3.6939e-01, -2.5540e-01, -3.5333e-01,  3.7214e-01,\n",
       "         1.9065e+00,  1.7407e+00,  1.2818e+00,  4.7474e-01, -1.1279e+00,\n",
       "        -1.3318e-01,  1.0841e+00,  1.1111e+00,  2.5677e-01,  9.6003e-02,\n",
       "         2.4728e+00, -6.9190e-01, -2.4013e+00, -1.9088e+00, -2.7976e-01,\n",
       "         2.8647e-01, -1.0314e+00,  4.9384e-01, -9.7575e-01,  2.4125e-01,\n",
       "        -4.5776e-01, -2.1729e+00,  6.8295e-01, -5.9783e-01, -7.6568e-01,\n",
       "        -6.8171e-02,  7.2051e-01,  1.0234e+00,  1.3814e+00,  1.0481e+00,\n",
       "         1.6249e+00,  1.4694e-02,  1.1664e+00, -8.8426e-01,  1.7891e-01,\n",
       "         8.7546e-02, -9.6954e-01,  3.5897e-01,  8.3710e-01, -1.1977e-01,\n",
       "         6.1722e-01,  2.1218e-01, -1.0700e+00, -1.0210e+00,  7.9014e-02,\n",
       "         6.6533e-01,  5.0188e-01,  3.6993e-01,  6.7140e-01, -2.1729e-01,\n",
       "         1.1855e+00,  9.7701e-01,  3.4555e-02,  5.2570e-02,  1.7804e+00,\n",
       "        -4.3345e-01, -9.1795e-01,  8.4220e-01, -5.2025e-01, -2.3512e+00,\n",
       "        -2.9133e-01,  1.8137e+00, -1.4506e-01,  7.7703e-01, -1.1247e+00,\n",
       "         1.0406e+00,  1.4300e-01, -3.2812e-01, -5.9724e-01, -1.5874e+00,\n",
       "         1.2625e+00,  2.5443e-01,  6.8958e-01, -1.5735e-01, -2.1421e+00,\n",
       "        -7.0841e-01, -4.5094e-01, -1.5603e+00,  7.7987e-01, -2.1264e+00,\n",
       "         1.3828e+00,  6.0764e-01, -7.3796e-01, -2.4417e-01, -7.1150e-01,\n",
       "         8.2985e-01, -7.7541e-02,  7.6857e-01,  3.1817e-01, -7.7757e-01,\n",
       "        -4.2241e-01,  6.5826e-01, -6.4465e-01, -5.9923e-01, -4.3871e-01,\n",
       "         7.2439e-02, -3.3670e-01,  1.5531e-01,  5.0956e-01,  6.4318e-01,\n",
       "         1.0634e-01, -5.2283e-02,  7.2015e-01,  1.1089e+00,  7.8364e-01,\n",
       "        -3.0195e-01, -3.6416e-01,  1.3103e+00, -4.6703e-01,  1.9210e+00,\n",
       "         2.4408e-01, -1.4922e+00,  5.6181e-01,  7.4204e-01,  9.7397e-01,\n",
       "         3.7613e-01,  1.2033e-03, -5.6261e-01,  1.3709e-01,  3.0908e-01,\n",
       "         4.3589e-01,  9.6681e-01,  5.2447e-01, -6.7887e-01,  1.3580e+00,\n",
       "         2.0728e-01,  1.3712e-01, -4.1442e-01,  9.4468e-02,  2.5556e-01,\n",
       "        -3.7138e-01,  1.5530e+00,  5.2251e-01,  4.6009e-01,  5.7644e-01,\n",
       "         1.6343e+00, -9.3365e-01, -1.7162e+00,  5.0816e-01,  1.9417e-02,\n",
       "        -3.2217e-01, -1.3262e+00, -7.3292e-01,  1.6861e+00, -8.8239e-02,\n",
       "        -1.5955e+00, -9.8843e-01,  1.5626e+00,  2.0443e-01, -3.3774e-01,\n",
       "        -8.3594e-01,  2.5315e-01,  1.6415e+00,  1.0761e-01,  4.3909e-02,\n",
       "        -9.7492e-02, -6.0125e-01, -1.0249e+00,  4.4471e-01,  8.7049e-01,\n",
       "        -2.8564e-01,  3.5769e-01, -6.4562e-01,  2.4414e-01,  4.4689e-01,\n",
       "         9.8379e-02, -2.0082e+00,  2.6912e+00, -2.9739e-02,  7.9058e-02,\n",
       "        -1.0467e+00,  1.2589e+00,  7.7871e-01,  1.6134e+00, -7.5795e-02,\n",
       "         2.4459e-01, -9.5959e-01, -1.0011e+00,  1.5439e+00,  8.5851e-01,\n",
       "         6.6582e-01,  8.2767e-02,  6.9187e-01, -8.3814e-02,  1.0908e+00,\n",
       "         5.5916e-01, -3.6419e-01,  2.0466e-01, -8.0997e-01,  1.4612e-01,\n",
       "        -5.8160e-01,  3.4547e-01, -2.0894e-01, -2.3499e+00, -2.0449e-01,\n",
       "         1.2959e-01, -1.0992e+00, -3.7099e-01,  5.7603e-01, -6.8101e-01,\n",
       "         7.4702e-01, -9.2913e-01, -5.2333e-01, -1.1561e+00,  1.0236e+00,\n",
       "         9.8274e-01,  1.8651e-01,  1.2910e-01,  7.5824e-01, -1.4217e+00,\n",
       "         1.5712e+00, -1.5876e+00, -5.8153e-01, -9.3299e-01, -8.1509e-01,\n",
       "         1.5932e+00,  1.6746e+00, -4.7422e-01, -7.5078e-01,  1.5711e+00,\n",
       "        -2.0033e-01,  1.1153e+00,  2.1060e+00,  5.4210e-01, -7.9347e-01,\n",
       "         4.7698e-01, -1.0490e+00, -1.9694e-01, -5.3319e-02,  5.9719e-01,\n",
       "        -2.7117e-01,  5.3615e-01,  3.0759e-01,  6.2320e-01,  1.4930e-01,\n",
       "         1.7076e+00,  9.3622e-02, -3.6664e-01,  1.7003e+00, -9.6456e-01,\n",
       "         2.9677e-01, -9.8081e-02,  1.1619e+00, -5.0380e-01, -1.2547e+00,\n",
       "        -1.6950e+00, -1.7951e-01, -9.8662e-02,  6.6682e-01,  9.5207e-01,\n",
       "        -1.6040e-01, -1.6418e+00, -8.9001e-01,  8.7406e-02, -7.2148e-01,\n",
       "         5.4667e-01, -1.5340e-01,  4.3919e-01,  2.4276e-01, -1.5593e+00,\n",
       "         2.0327e+00, -1.7976e+00, -1.0149e+00, -1.3462e+00, -2.3772e+00,\n",
       "         1.4590e+00, -3.0744e-01,  6.6394e-01,  4.7298e-02, -1.0030e+00,\n",
       "         7.8366e-01,  1.8789e-01,  4.3368e-02, -6.8919e-01, -3.3976e-01,\n",
       "        -6.4345e-01, -3.7919e-01,  1.0008e+00,  9.6835e-01, -4.1755e-01,\n",
       "        -1.7020e-03,  2.9330e-01,  5.1436e-01, -2.6706e+00, -1.9252e-01,\n",
       "        -1.1766e-01,  3.9349e-01,  1.3534e+00, -4.5487e-01, -1.5176e+00,\n",
       "         2.6356e-01, -2.7808e-01, -4.0173e-01, -7.6142e-01, -1.2565e+00,\n",
       "         2.6196e-01, -3.3675e-01,  6.0729e-01,  9.9332e-01, -7.8205e-02,\n",
       "        -5.5831e-01,  6.3940e-01, -9.9503e-01, -1.8994e-01,  2.5763e-01,\n",
       "        -6.7560e-01,  6.0766e-02,  2.5248e-01,  1.2508e+00,  9.3228e-01])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e66d6adc-0be2-45d0-8be1-adc1662c0374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `log_prob_z` not found.\n"
     ]
    }
   ],
   "source": [
    "log_prob_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26503a10-7780-485b-907c-54b03d1ee868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class InvertedPlanarTransform(nn.Module):\n",
    "    \"\"\"Implementation of the invertible transformation used in planar flow:\n",
    "        f(z) = z + u * h(dot(w.T, z) + b)\n",
    "    See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"Initialise weights and bias.\n",
    "        \n",
    "        Args:\n",
    "            dim: Dimensionality of the distribution to be estimated.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
    "        self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
    "        self.u = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            self.get_u_hat()\n",
    "\n",
    "        return z + self.u * nn.Tanh()(torch.mm(z, self.w.T) + self.b)\n",
    "    \n",
    "    def log_det_J(self, z: Tensor) -> Tensor:\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            self.get_u_hat()\n",
    "        a = torch.mm(z, self.w.T) + self.b\n",
    "        psi = (1 - nn.Tanh()(a) ** 2) * self.w\n",
    "        abs_det = (1 + torch.mm(self.u, psi.T)).abs()\n",
    "        log_det = torch.log(1e-4 + abs_det)\n",
    "\n",
    "        return log_det\n",
    "\n",
    "    def get_u_hat(self) -> None:\n",
    "        \"\"\"Enforce w^T u >= -1. When using h(.) = tanh(.), this is a sufficient condition \n",
    "        for invertibility of the transformation f(z). See Appendix A.1.\n",
    "        \"\"\"\n",
    "        wtu = torch.mm(self.u, self.w.T)\n",
    "        m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n",
    "        self.u.data = (\n",
    "            self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2\n",
    "        )\n",
    "\n",
    "    def inverse(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the inverse transformation.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Inverse transformed tensor.\n",
    "        \"\"\"\n",
    "        # Compute inverse of tanh using atanh (inverse hyperbolic tangent)\n",
    "        inverse_h = torch.atanh((x - self.u) / torch.mm(self.w, self.w.T))\n",
    "\n",
    "        # Compute the inverse transformation\n",
    "        inverse_z = torch.mm(inverse_h - self.b, torch.inverse(self.w))\n",
    "\n",
    "        return inverse_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9e463-5c22-428c-8548-08182fb77c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConditionalPlanarFlowModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, target_dim=1, num_flows=1):\n",
    "        super(ConditionalPlanarFlowModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.target_dim = target_dim\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        # Create a list to hold the InvertedPlanarTransform instances\n",
    "        self.flow_list = nn.ModuleList([InvertedPlanarTransform(dim=input_dim) for _ in range(num_flows)])\n",
    "\n",
    "        # Linear layer to predict the CPD for the target variable\n",
    "        self.linear_layer = nn.Linear(input_dim, target_dim)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        # Apply all the InvertedPlanarTransform instances sequentially\n",
    "        transformed_features = input_features\n",
    "        for flow in self.flow_list:\n",
    "            transformed_features = flow(transformed_features)\n",
    "\n",
    "        # Predict the CPD for the target variable\n",
    "        predicted_cpd = self.linear_layer(transformed_features)\n",
    "\n",
    "        return predicted_cpd\n",
    "\n",
    "\n",
    "def train_model(model, input_data, target_data, num_epochs=100, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_cpd = model(input_data)\n",
    "        loss = criterion(predicted_cpd, target_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Step 5: Inference\n",
    "def predict_cpd(model, input_features):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        transformed_features = model.feature_transform.inverse(input_features)\n",
    "        predicted_cpd = model.linear_layer(transformed_features)\n",
    "    return predicted_cpd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbcc9d-8661-41e6-a3e8-c31441e0491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define your loss function (e.g., negative log-likelihood)\n",
    "criterion = nn.NLLLoss()  # Use a suitable loss function based on the nature of your CPD\n",
    "\n",
    "# Step 4: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92ade13-b05c-44d2-a5fb-7cbe61dc63e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples=10\n",
    "input_dim=6\n",
    "target_dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa8e26a-007e-461c-8fb9-e4cd103e363a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have your input_data and target_data as torch tensors:\n",
    "input_data = torch.randn(num_samples, input_dim)  # Replace with your actual data\n",
    "target_data = torch.randn(num_samples, target_dim)  # Replace with your actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802c6a15-5109-4ad5-ad42-bec6dc3dd838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, input_data, target_data, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m predicted_cpd \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[0;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_cpd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/nn/modules/loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/torch/nn/functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "train_model(model, input_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7296cbf5-10f7-4dc6-a30a-96bf70b7e577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m target_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_samples, target_dim)  \u001b[38;5;66;03m# Replace with your actual data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 2: Create the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mConditionalPlanarFlowModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 4: Train the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_model(model, input_data, target_data)\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36mConditionalPlanarFlowModel.__init__\u001b[0;34m(self, input_dim, target_dim)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dim \u001b[38;5;241m=\u001b[39m target_dim\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Feature transformation using InvertedPlanarTransform\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_transform \u001b[38;5;241m=\u001b[39m \u001b[43mInvertedPlanarTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Linear layer to predict the CPD for the target variable\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(input_dim, target_dim)\n",
      "Cell \u001b[0;32mIn[34], line 19\u001b[0m, in \u001b[0;36mInvertedPlanarTransform.__init__\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialise weights and bias.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    dim: Dimensionality of the distribution to be estimated.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, dim)\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: randn(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2"
     ]
    }
   ],
   "source": [
    "# Assuming you have your input_data and target_data as torch tensors:\n",
    "input_data = torch.randn(num_samples, input_dim)  # Replace with your actual data\n",
    "target_data = torch.randn(num_samples, target_dim)  # Replace with your actual data\n",
    "\n",
    "# Step 2: Create the model\n",
    "\n",
    "# Step 4: Train the model\n",
    "\n",
    "\n",
    "# Step 5: Predict the CPD for new input features\n",
    "new_input_features = torch.randn(num_samples, input_dim)  # Replace with your new data\n",
    "predicted_cpd = predict_cpd(model, new_input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe0b227d-9aa5-4c99-91a4-0e558795328c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
       "\n",
       "\n",
       "Returns a tensor filled with random numbers from a normal distribution\n",
       "with mean `0` and variance `1` (also called the standard normal\n",
       "distribution).\n",
       "\n",
       ".. math::\n",
       "    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
       "\n",
       "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
       "\n",
       "Args:\n",
       "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
       "        Can be a variable number of arguments or a collection like a list or tuple.\n",
       "\n",
       "Keyword args:\n",
       "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
       "    out (Tensor, optional): the output tensor.\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
       "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
       "        Default: ``torch.strided``.\n",
       "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
       "        Default: if ``None``, uses the current device for the default tensor type\n",
       "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
       "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "    pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
       "        the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.randn(4)\n",
       "    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
       "    >>> torch.randn(2, 3)\n",
       "    tensor([[ 1.5954,  2.8929, -1.0923],\n",
       "            [ 1.1719, -0.4709, -0.1996]])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e45c5-79fa-43bc-b4a5-3bb633880230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9af6a-e0e9-4171-b46f-ca21255e253d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd133-21b6-40b1-a264-c1ba88cc8b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8c8a08-854d-44f7-83eb-23a6a9e0cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, dim: int = 2, K: int = 6):\n",
    "        \"\"\"Make a planar flow by stacking planar transformations in sequence.\n",
    "\n",
    "        Args:\n",
    "            dim: Dimensionality of the distribution to be estimated.\n",
    "            K: Number of transformations in the flow. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = [InvertedPlanarTransform(dim) for _ in range(K)]\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n",
    "        log_det_J = 0\n",
    "\n",
    "        for layer in self.layers:\n",
    "            log_det_J += layer.log_det_J(z)\n",
    "            z = layer(z)\n",
    "\n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "789351d3-262f-43b8-8132-89d053681ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PlanarFlow(dim=2, K=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf722c0-6d93-437e-9679-1a4704831d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlanarFlow(\n",
       "  (model): Sequential(\n",
       "    (0): InvertedPlanarTransform()\n",
       "    (1): InvertedPlanarTransform()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a9b31f-f3e3-49d7-8b1a-6cfe426bbbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m identity_flow \u001b[38;5;241m=\u001b[39m \u001b[43mIdentityFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EUCLID/INSIGHT/NF/flows/IdentityFlow.py:14\u001b[0m, in \u001b[0;36mIdentityFlow.__init__\u001b[0;34m(self, params, n_dims, name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, n_dims, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdentityFlow\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    :param params: shape (?, 1), this will become alpha and define the slow of ReLU for x < 0\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    :param n_dims: Dimension of the distribution that's being transformed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIdentityFlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d50e1e-5adc-4bdd-87cb-32c78c883b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "planar_flow = InvertedPlanarFlow(params=torch.zeros((1, 2 * 1 + 1)), n_dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb981f0f-113e-45f2-a027-2e003065bdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvertedPlanarFlow()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planar_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74688079-9a3f-4f87-b93f-4a191325e6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29cd37-0b15-4cf9-a55c-da73f38d67ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9fbda-6377-4df2-9caa-3fe741ec3bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d73f2-51a8-4775-850a-da4981fd7d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1bd67f-8253-4ebd-b69e-16a1418c6e31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 07:26:44.817779: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-03 07:26:47.725659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 07:27:10.847442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-03 07:27:53.690091: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class BaseEstimator(tf.keras.Sequential):\n",
    "    x_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "    y_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "    def __init__(self, layers, noise_fn_type=\"fixed_rate\", noise_scale_factor=0.0, random_seed=22):\n",
    "        tf.random.set_seed(random_seed)\n",
    "        self.noise_fn_type = noise_fn_type\n",
    "        self.noise_scale_factor = noise_scale_factor\n",
    "\n",
    "        super().__init__(layers)\n",
    "\n",
    "    def fit(self, x, y, batch_size=None, epochs=None, verbose=1, **kwargs):\n",
    "        self._assign_data_normalization(x, y)\n",
    "        assert len(x.shape) == len(y.shape) == 2, \"Please pass a matrix not a vector\"\n",
    "        self._assign_noise_regularisation(n_dims=x.shape[1] + y.shape[1], n_datapoints=x.shape[0])\n",
    "        super().fit(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            callbacks=[tf.keras.callbacks.TerminateOnNaN()],\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _assign_noise_regularisation(self, n_dims, n_datapoints):\n",
    "        assert self.noise_fn_type in [\"rule_of_thumb\", \"fixed_rate\"]\n",
    "        if self.noise_fn_type == \"rule_of_thumb\":\n",
    "            noise_std = self.noise_scale_factor * (n_datapoints + 1) ** (-1 / (4 + n_dims))\n",
    "            self.x_noise_std.assign(noise_std)\n",
    "            self.y_noise_std.assign(noise_std)\n",
    "        elif self.noise_fn_type == \"fixed_rate\":\n",
    "            self.x_noise_std.assign(self.noise_scale_factor)\n",
    "            self.y_noise_std.assign(self.noise_scale_factor)\n",
    "\n",
    "    def score(self, x_data, y_data):\n",
    "        x_data = x_data.astype(np.float32)\n",
    "        y_data = y_data.astype(np.float32)\n",
    "        nll = self._get_neg_log_likelihood()\n",
    "        return -nll(y_data, self.call(x_data, training=False)).numpy().mean()\n",
    "\n",
    "    def _assign_data_normalization(self, x, y):\n",
    "        self.x_mean = np.mean(x, axis=0, dtype=np.float32)\n",
    "        self.y_mean = np.mean(y, axis=0, dtype=np.float32)\n",
    "        self.x_std = np.std(x, axis=0, dtype=np.float32)\n",
    "        self.y_std = np.std(y, axis=0, dtype=np.float32)\n",
    "\n",
    "    def _get_neg_log_likelihood(self):\n",
    "        y_input_model = self._get_input_model()\n",
    "        return lambda y, p_y: -p_y.log_prob(y_input_model(y)) + tf.reduce_sum(\n",
    "            tf.math.log(self.y_std)\n",
    "        )\n",
    "\n",
    "    def _get_input_model(self):\n",
    "        y_input_model = tf.keras.Sequential()\n",
    "        # add data normalization layer\n",
    "        y_input_model.add(\n",
    "            tf.keras.layers.Lambda(lambda y: (y - tf.ones_like(y) * self.y_mean) / self.y_std)\n",
    "        )\n",
    "        # noise will be switched on during training and switched off otherwise automatically\n",
    "        y_input_model.add(tf.keras.layers.GaussianNoise(self.y_noise_std))\n",
    "        return y_input_model\n",
    "\n",
    "    def pdf(self, x, y):\n",
    "        assert x.shape == y.shape\n",
    "        output = self(x)\n",
    "        y_circ = (y - tf.ones_like(y) * self.y_mean) / self.y_std\n",
    "        return output.prob(y_circ) / tf.reduce_prod(self.y_std)\n",
    "\n",
    "    def log_pdf(self, x, y):\n",
    "        x = x.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        assert x.shape == y.shape\n",
    "\n",
    "        output = self(x)\n",
    "        assert output.event_shape == y.shape[-1]\n",
    "\n",
    "        y_circ = (y - tf.ones_like(y) * self.y_mean) / self.y_std\n",
    "        return output.log_prob(y_circ) - tf.reduce_sum(tf.math.log(self.y_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84797c3a-3973-40d8-991d-cf375a88a416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python import tf2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "if not tf2.enabled():\n",
    "    import tensorflow.compat.v2 as tf\n",
    "\n",
    "    tf.enable_v2_behavior()\n",
    "    assert tf2.enabled()\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class MeanFieldLayer(tfp.layers.DistributionLambda):\n",
    "    def __init__(self, n_dims, scale=None, map_mode=False, dtype=None):\n",
    "        \"\"\"\n",
    "        A subclass of Distribution Lambda. A layer that uses it's input to parametrize n_dims-many indepentent normal\n",
    "        distributions (aka mean field)\n",
    "        Requires input size n_dims for fixed scale, 2*n_dims for trainable scale\n",
    "        Mean Field also works for scalars\n",
    "        The input tensors for this layer should be initialized to Zero for a standard normal distribution\n",
    "        :param n_dims: Dimension of the distribution that's being output by the Layer\n",
    "        :param scale: (float) None if scale should be trainable. If not None, specifies the fixed scale of the\n",
    "            independent normals. If map mode is activated, this is ignored and set to 1.0\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.scale = scale\n",
    "\n",
    "        if map_mode:\n",
    "            self.scale = 1.0\n",
    "        convert_ttf = tfd.Distribution.mean if map_mode else tfd.Distribution.sample\n",
    "\n",
    "        make_dist_fn = self._get_distribution_fn(self.n_dims, self.scale)\n",
    "\n",
    "        super().__init__(\n",
    "            make_distribution_fn=make_dist_fn, convert_to_tensor_fn=convert_ttf, dtype=dtype\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_distribution_fn(n_dims, scale=None):\n",
    "        if scale is None:\n",
    "\n",
    "            def dist_fn(t):\n",
    "                assert t.shape[-1] == 2 * n_dims\n",
    "                return tfd.Independent(\n",
    "                    tfd.Normal(\n",
    "                        loc=t[..., 0:n_dims],\n",
    "                        scale=1e-3\n",
    "                        + tf.nn.softplus(\n",
    "                            tf.math.log(tf.math.expm1(1.0)) + 0.05 * t[..., n_dims : 2 * n_dims]\n",
    "                        ),\n",
    "                    ),\n",
    "                    reinterpreted_batch_ndims=1,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            assert scale > 0.0\n",
    "\n",
    "            def dist_fn(t):\n",
    "                assert t.shape[-1] == n_dims\n",
    "                return tfd.Independent(\n",
    "                    tfd.Normal(loc=t[..., 0:n_dims], scale=scale), reinterpreted_batch_ndims=1\n",
    "                )\n",
    "\n",
    "        return dist_fn\n",
    "\n",
    "    def get_total_param_size(self):\n",
    "        return 2 * self.n_dims if self.scale is None else self.n_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af8bca53-9191-4f66-aa78-64747187df98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class AffineFlow(tfp.experimental.joint_distribution_layers.Affine):\n",
    "    def __init__(self, t, n_dims, name=\"AffineFlow\"):\n",
    "        assert t.shape[-1] == 2 * n_dims\n",
    "        super(AffineFlow, self).__init__(\n",
    "            shift=t[..., 0:n_dims], scale_diag=1.0 + t[..., n_dims : 2 * n_dims], name=name\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(n_dims):\n",
    "        \"\"\"\n",
    "        :param n_dims:  The dimension of the distribution to be transformed by the flow\n",
    "        :return: (int) The dimension of the parameter space for the flow\n",
    "        \"\"\"\n",
    "        return 2 * n_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5561b844-a362-4b0c-8ad2-002c68cff427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'out_units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_radial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 46\u001b[0m, in \u001b[0;36mtest_radial\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_radial\u001b[39m():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mflow_dimension_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mradial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 17\u001b[0m, in \u001b[0;36mflow_dimension_testing\u001b[0;34m(flow_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)), dim)\n\u001b[1;32m     16\u001b[0m flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n\u001b[0;32m---> 17\u001b[0m reference \u001b[38;5;241m=\u001b[39m \u001b[43mAffineFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_tensors \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim], [[\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim] \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mforward_min_event_ndims \u001b[38;5;241m==\u001b[39m reference\u001b[38;5;241m.\u001b[39mforward_min_event_ndims\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py:342\u001b[0m, in \u001b[0;36m_DistributionMeta.__new__.<locals>.wrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to have things set in `self` before `__init__` is\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# called, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m \u001b[43mdefault_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to override things set in `self` by subclass\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# `__init__`, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m   \u001b[38;5;66;03m# We prefer subclasses will set `parameters = dict(locals())` because\u001b[39;00m\n\u001b[1;32m    347\u001b[0m   \u001b[38;5;66;03m# this has nearly zero overhead. However, failing to do this, we will\u001b[39;00m\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;66;03m# resolve the input arguments dynamically and only when needed.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m, in \u001b[0;36mAffineFlow.__init__\u001b[0;34m(self, t, n_dims, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, n_dims, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAffineFlow\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n_dims\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_diag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/decorator.py:231\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m--> 231\u001b[0m         args, kw \u001b[38;5;241m=\u001b[39m \u001b[43mfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/decorator.py:203\u001b[0m, in \u001b[0;36mfix\u001b[0;34m(args, kwargs, sig)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfix\u001b[39m(args, kwargs, sig):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Fix args and kwargs to be consistent with the signature\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     ba \u001b[38;5;241m=\u001b[39m \u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     ba\u001b[38;5;241m.\u001b[39mapply_defaults()  \u001b[38;5;66;03m# needed for test_dan_schult\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ba\u001b[38;5;241m.\u001b[39margs, ba\u001b[38;5;241m.\u001b[39mkwargs\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/inspect.py:3185\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/inspect.py:3100\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3098\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3099\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 3100\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3102\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[1;32m   3103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'out_units'"
     ]
    }
   ],
   "source": [
    "test_radial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "727a821f-0fc9-4ec9-8ba1-9451c3e2bda7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class PlanarFlow(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Implements a bijector x = y + u * tanh(w_t * y + b)\n",
    "\n",
    "    Args:\n",
    "        params: Tensor shape (?, 2*n_dims+1). This will be split into the parameters\n",
    "            u (?, n_dims), w (?, n_dims), b (?, 1).\n",
    "            Furthermore u will be constrained to assure the invertability of the flow\n",
    "        n_dims: The dimension of the distribution that will be transformed\n",
    "        name: The name to give this particular flow\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _u, _w, _b = None, None, None\n",
    "\n",
    "    def __init__(self, t, n_dims, name=\"Inverted_Planar_Flow\"):\n",
    "        super().__init__(validate_args=False, name=name, inverse_min_event_ndims=1)\n",
    "        assert t.shape[-1] == 2 * n_dims + 1\n",
    "        u, w, b = (\n",
    "            t[..., 0:n_dims],\n",
    "            # initialize w to 1.0\n",
    "            t[..., n_dims : 2 * n_dims] + 1,\n",
    "            t[..., 2 * n_dims : 2 * n_dims + 1],\n",
    "        )\n",
    "\n",
    "        # constrain u before assigning it\n",
    "        self._u = self._u_circ(u, w)\n",
    "        self._w = w\n",
    "        self._b = b\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(n_dims):\n",
    "        \"\"\"\n",
    "        :param n_dims: The dimension of the distribution to be transformed by the flow\n",
    "        :return: (int array) The dimension of the parameter space for this flow, n_dims + n_dims + 1\n",
    "        \"\"\"\n",
    "        return n_dims + n_dims + 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _u_circ(u, w):\n",
    "        \"\"\"\n",
    "        To ensure invertibility of the flow, the following condition needs to hold: w_t * u >= -1\n",
    "        :return: The transformed u\n",
    "        \"\"\"\n",
    "        wtu = tf.math.reduce_sum(w * u, 1, keepdims=True)\n",
    "        # add constant to make it more numerically stable\n",
    "        m_wtu = -1.0 + tf.nn.softplus(wtu) + 1e-5\n",
    "        norm_w_squared = tf.math.reduce_sum(w ** 2, 1, keepdims=True) + 1e-9\n",
    "        return u + (m_wtu - wtu) * (w / norm_w_squared)\n",
    "\n",
    "    def _wzb(self, z):\n",
    "        \"\"\"\n",
    "        Computes w_t * z + b\n",
    "        \"\"\"\n",
    "        return tf.math.reduce_sum(self._w * z, 1, keepdims=True) + self._b\n",
    "\n",
    "    @staticmethod\n",
    "    def _der_tanh(z):\n",
    "        \"\"\"\n",
    "        Computes the derivative of hyperbolic tangent\n",
    "        \"\"\"\n",
    "        return 1.0 - tf.math.tanh(z) ** 2\n",
    "\n",
    "    def _forward(self, z):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the bijector\n",
    "        \"\"\"\n",
    "        return z + self._u * tf.math.tanh(self._wzb(z))\n",
    "\n",
    "    def _forward_log_det_jacobian(self, z):\n",
    "        \"\"\"\n",
    "        Computes the ln of the absolute determinant of the jacobian\n",
    "        \"\"\"\n",
    "        psi = self._der_tanh(self._wzb(z)) * self._w\n",
    "        det_grad = 1.0 + tf.math.reduce_sum(self._u * psi, 1)\n",
    "        return tf.math.log(tf.math.abs(det_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3aa80cb-8d33-429a-bf44-17228f745289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class RadialFlow(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Implements a bijector x = y + (alpha * beta * (y - y_0)) / (alpha + abs(y - y_0)).\n",
    "    Args:\n",
    "        params: Tensor shape (?, n_dims+2). This will be split into the parameters\n",
    "            alpha (?, 1), beta (?, 1), gamma (?, n_dims).\n",
    "            Furthermore alpha will be constrained to assure the invertability of the flow\n",
    "        n_dims: The dimension of the distribution that will be transformed\n",
    "        name: The name to give this particular flow\n",
    "    \"\"\"\n",
    "\n",
    "    _alpha = None\n",
    "    _beta = None\n",
    "    _gamma = None\n",
    "\n",
    "    def __init__(self, t, n_dims, name='RadialFlow'):\n",
    "        super().__init__(validate_args=False, name=name, inverse_min_event_ndims=1)\n",
    "\n",
    "        assert t.shape[-1] == n_dims + 2\n",
    "        alpha = t[..., 0:1]\n",
    "        beta = t[..., 1:2]\n",
    "        gamma = t[..., 2 : n_dims + 2]\n",
    "\n",
    "        # constraining the parameters before they are assigned to ensure invertibility.\n",
    "        # slightly shift alpha, softplus(zero centered input - 2) = small\n",
    "        self._alpha = self._alpha_circ(0.3 * alpha - 2.0)\n",
    "        # slightly shift beta, softplus(zero centered input + ln(e - 1)) = 0\n",
    "        self._beta = self._beta_circ(0.1 * beta + tf.math.log(tf.math.expm1(1.0)))\n",
    "        self._gamma = gamma\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(n_dims):\n",
    "        \"\"\"\n",
    "        :param n_dims:  The dimension of the distribution to be transformed by the flow\n",
    "        :return: (int) The dimension of the parameter space for the flow\n",
    "        \"\"\"\n",
    "        return 1 + 1 + n_dims\n",
    "\n",
    "    def _r(self, z):\n",
    "        return tf.math.reduce_sum(tf.abs(z - self._gamma), 1, keepdims=True)\n",
    "\n",
    "    def _h(self, r):\n",
    "        return 1.0 / (self._alpha + r)\n",
    "\n",
    "    def _forward(self, z):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the bijector\n",
    "        \"\"\"\n",
    "        r = self._r(z)\n",
    "        h = self._h(r)\n",
    "        return z + (self._alpha * self._beta * h) * (z - self._gamma)\n",
    "\n",
    "    def _forward_log_det_jacobian(self, z):\n",
    "        \"\"\"\n",
    "        Computes the ln of the absolute determinant of the jacobian\n",
    "        \"\"\"\n",
    "        r = self._r(z)\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(r)\n",
    "            h = self._h(r)\n",
    "        der_h = g.gradient(h, r)\n",
    "        ab = self._alpha * self._beta\n",
    "        det = (1.0 + ab * h) ** (self.n_dims - 1) * (1.0 + ab * h + ab * der_h * r)\n",
    "        det = tf.squeeze(det, axis=-1)\n",
    "        return tf.math.log(det)\n",
    "\n",
    "    @staticmethod\n",
    "    def _alpha_circ(alpha):\n",
    "        \"\"\"\n",
    "        Method for constraining the alpha parameter to meet the invertibility requirements\n",
    "        \"\"\"\n",
    "        return tf.nn.softplus(alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def _beta_circ(beta):\n",
    "        \"\"\"\n",
    "        Method for constraining the beta parameter to meet the invertibility requirements\n",
    "        \"\"\"\n",
    "        return tf.nn.softplus(beta) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f74ef9a1-83b6-45ba-ac25-5f0f5ec5665c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class BayesianNNEstimator(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dist_layer,\n",
    "        kl_weight_scale,\n",
    "        kl_use_exact=True,\n",
    "        hidden_sizes=(10,),\n",
    "        activation=\"tanh\",\n",
    "        learning_rate=3e-2,\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        trainable_prior=False,\n",
    "        map_mode=False,\n",
    "        prior_scale=1.0,\n",
    "        random_seed=22,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A bayesian net parametrizing a normalizing flow distribution\n",
    "        :param dist_layer: A Tfp Distribution Lambda Layer that converts the neural net output into a distribution\n",
    "        :param kl_weight_scale: Scales how much KL(posterior|prior) influences the loss\n",
    "        :param hidden_sizes: size and depth of net\n",
    "        :param noise_reg: Tuple with (type_of_reg, scale_factor)\n",
    "        :param trainable_prior: empirical bayes\n",
    "        :param map_mode: If true, will use the mean of the posterior instead of a sample. Default False\n",
    "        :param prior_scale: The scale of the zero centered priors\n",
    "\n",
    "        A note on kl_weight_scale: Keras calculates the loss per sample and not for the full dataset. Therefore,\n",
    "        we need to scale the KL(q||p) loss down to a single sample, which means setting kl_weight_scale = 1/n_datapoints\n",
    "        \"\"\"\n",
    "        self.x_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "        self.y_noise_std = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False)\n",
    "        self.map_mode = map_mode\n",
    "\n",
    "        posterior = self._get_posterior_fn(map_mode=map_mode)\n",
    "        prior = self._get_prior_fn(trainable_prior, prior_scale)\n",
    "        dense_layers = self._get_dense_layers(\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=dist_layer.get_total_param_size(),\n",
    "            posterior=posterior,\n",
    "            prior=prior,\n",
    "            kl_weight_scale=kl_weight_scale,\n",
    "            kl_use_exact=kl_use_exact,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            dense_layers + [dist_layer],\n",
    "            noise_fn_type=noise_reg[0],\n",
    "            noise_scale_factor=noise_reg[1],\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "\n",
    "        self.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate), loss=self._get_neg_log_likelihood()\n",
    "        )\n",
    "\n",
    "    def score(self, x_data, y_data):\n",
    "        x_data = x_data.astype(np.float32)\n",
    "        y_data = y_data.astype(np.float32)\n",
    "\n",
    "        scores = None\n",
    "        nll = self._get_neg_log_likelihood()\n",
    "        posterior_draws = 1 if self.map_mode else 50\n",
    "        for _ in range(posterior_draws):\n",
    "            res = tf.expand_dims(-nll(y_data, self.call(x_data, training=False)), axis=0)\n",
    "            scores = res if scores is None else tf.concat([scores, res], axis=0)\n",
    "        logsumexp = tf.math.reduce_logsumexp(scores, axis=0).numpy() - np.log(posterior_draws)\n",
    "        return logsumexp.mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_prior_fn(trainable=False, prior_scale=1.0):\n",
    "        def prior_fn(kernel_size, bias_size=0, dtype=None):\n",
    "            size = kernel_size + bias_size\n",
    "            layers = [\n",
    "                tfp.layers.VariableLayer(\n",
    "                    shape=size, initializer=\"zeros\", dtype=dtype, trainable=trainable\n",
    "                ),\n",
    "                MeanFieldLayer(size, scale=prior_scale, map_mode=False, dtype=dtype),\n",
    "            ]\n",
    "            return tf.keras.Sequential(layers)\n",
    "\n",
    "        return prior_fn\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_posterior_fn(map_mode=False):\n",
    "        def posterior_fn(kernel_size, bias_size=0, dtype=None):\n",
    "            size = kernel_size + bias_size\n",
    "            layers = [\n",
    "                tfp.layers.VariableLayer(\n",
    "                    size if map_mode else 2 * size,\n",
    "                    initializer=\"normal\",\n",
    "                    dtype=dtype,\n",
    "                    trainable=True,\n",
    "                ),\n",
    "                MeanFieldLayer(size, scale=None, map_mode=map_mode, dtype=dtype),\n",
    "            ]\n",
    "            return tf.keras.Sequential(layers)\n",
    "\n",
    "        return posterior_fn\n",
    "\n",
    "    def _get_dense_layers(\n",
    "        self,\n",
    "        hidden_sizes,\n",
    "        output_size,\n",
    "        posterior,\n",
    "        prior,\n",
    "        kl_weight_scale=1.0,\n",
    "        kl_use_exact=True,\n",
    "        activation=\"relu\",\n",
    "    ):\n",
    "        assert type(hidden_sizes) == tuple or type(hidden_sizes) == list\n",
    "        assert kl_weight_scale <= 1.0\n",
    "\n",
    "        # these values are assigned once fit is called\n",
    "        normalization = [tf.keras.layers.Lambda(lambda x: (x - self.x_mean) / (self.x_std + 1e-8))]\n",
    "        noise_reg = [tf.keras.layers.GaussianNoise(self.x_noise_std)]\n",
    "        hidden = [\n",
    "            tfp.layers.DenseVariational(\n",
    "                units=size,\n",
    "                make_posterior_fn=posterior,\n",
    "                make_prior_fn=prior,\n",
    "                kl_weight=kl_weight_scale,\n",
    "                kl_use_exact=kl_use_exact,\n",
    "                activation=activation,\n",
    "            )\n",
    "            for size in hidden_sizes\n",
    "        ]\n",
    "        output = [\n",
    "            tfp.layers.DenseVariational(\n",
    "                units=output_size,\n",
    "                make_posterior_fn=posterior,\n",
    "                make_prior_fn=prior,\n",
    "                kl_weight=kl_weight_scale,\n",
    "                kl_use_exact=kl_use_exact,\n",
    "                activation=\"linear\",\n",
    "            )\n",
    "        ]\n",
    "        return normalization + noise_reg + hidden + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881c0b13-71fc-4a27-88a6-590de016cdea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InverseNormalizingFlowLayer(tfp.layers.DistributionLambda):\n",
    "    _flow_types = None\n",
    "    _trainable_base_dist = None\n",
    "    _n_dims = None\n",
    "\n",
    "    def __init__(self, flow_types, n_dims, trainable_base_dist=False):\n",
    "        \"\"\"\n",
    "        Subclass of a DistributionLambda. A layer that uses it's input to parametrize a normalizing flow\n",
    "        that transforms a base normal distribution. The Normalizing flows are inverted to enable fast likelihood\n",
    "        calculation of externally provided data. This is useful for density estimation.\n",
    "        As a result, sampling from this layer is not possible.\n",
    "        This layer does not work for scalars!\n",
    "        :param flow_types: Types of flows to use, applied in order from base_dist -> transformed_dist\n",
    "        :param n_dims: dimension of the underlying distribution being transformed\n",
    "        :param trainable_base_dist: whether the base normal distribution should have trainable loc and scale diag\n",
    "        \"\"\"\n",
    "        assert all([flow_type in FLOWS for flow_type in flow_types])\n",
    "\n",
    "        self._flow_types = flow_types\n",
    "        self._trainable_base_dist = trainable_base_dist\n",
    "        self._n_dims = n_dims\n",
    "\n",
    "        # as keras transforms tensors, this layer needs to have an tensor-like output\n",
    "        # therefore a function needs to be provided that transforms a distribution into a tensor\n",
    "        # per default the .sample() function is used, but our reversed flows cannot perform that operation\n",
    "        convert_ttfn = lambda d: tf.constant([0.0])\n",
    "        make_flow_dist = self._get_distribution_fn(n_dims, flow_types, trainable_base_dist)\n",
    "        super().__init__(make_distribution_fn=make_flow_dist, convert_to_tensor_fn=convert_ttfn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_distribution_fn(n_dims, flow_types, trainable_base_dist):\n",
    "        return lambda t: tfd.TransformedDistribution(\n",
    "            distribution=InverseNormalizingFlowLayer._get_base_dist(\n",
    "                t, n_dims, trainable_base_dist\n",
    "            ),\n",
    "            bijector=tfp.bijectors.Invert(\n",
    "                InverseNormalizingFlowLayer._get_bijector(\n",
    "                    (t[..., 2 * n_dims :] if trainable_base_dist else t), flow_types, n_dims\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_total_param_size(self):\n",
    "        \"\"\"\n",
    "        :return: The total number of parameters to specify this distribution\n",
    "        \"\"\"\n",
    "        num_flow_params = sum(\n",
    "            [FLOWS[flow_type].get_param_size(self._n_dims) for flow_type in self._flow_types]\n",
    "        )\n",
    "        base_dist_params = 2 * self._n_dims if self._trainable_base_dist else 0\n",
    "        return num_flow_params + base_dist_params\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_bijector(t, flow_types, n_dims):\n",
    "        # intuitively, we want to flows to go from base_dist -> transformed dist\n",
    "        flow_types = list(reversed(flow_types))\n",
    "        param_sizes = [FLOWS[flow_type].get_param_size(n_dims) for flow_type in flow_types]\n",
    "        assert sum(param_sizes) == t.shape[-1]\n",
    "        split_beginnings = [sum(param_sizes[0:i]) for i in range(len(param_sizes))]\n",
    "        chain = [\n",
    "            FLOWS[flow_type](t[..., begin : begin + size], n_dims)\n",
    "            for begin, size, flow_type in zip(split_beginnings, param_sizes, flow_types)\n",
    "        ]\n",
    "        return tfp.bijectors.Chain(chain)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_base_dist(t, n_dims, trainable):\n",
    "        if trainable:\n",
    "            return tfd.MultivariateNormalDiag(\n",
    "                loc=t[..., 0:n_dims],\n",
    "                scale_diag=1e-3\n",
    "                + tf.math.softplus(\n",
    "                    tf.math.log(tf.math.expm1(1.0)) + 0.1 * t[..., n_dims : 2 * n_dims]\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # we still need to know the batch size, therefore we need t for reference\n",
    "            return tfd.MultivariateNormalDiag(\n",
    "                loc=tf.zeros_like(t[..., 0:n_dims]), scale_diag=tf.ones_like(t[..., 0:n_dims])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b62f5ad-f94a-4a4d-955a-76f75adaee76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class BayesNormalizingFlowNetwork(BayesianNNEstimator):\n",
    "    def __init__(self, n_dims, kl_weight_scale, n_flows=2, trainable_base_dist=True, **kwargs):\n",
    "        \"\"\"\n",
    "        A bayesian net parametrizing a normalizing flow distribution\n",
    "        :param n_dims: The dimension of the output distribution\n",
    "        :param kl_weight_scale: Scales how much KL(posterior|prior) influences the loss\n",
    "        :param n_flows: The number of flows to use\n",
    "        :param hidden_sizes: size and depth of net\n",
    "        :param trainable_base_dist: whether to train the base normal dist\n",
    "        :param noise_reg: Tuple with (type_of_reg, scale_factor)\n",
    "        :param trainable_prior: empirical bayes\n",
    "        :param map_mode: If true, will use the mean of the posterior instead of a sample. Default False\n",
    "        :param prior_scale: The scale of the zero centered priors\n",
    "\n",
    "        A note on kl_weight_scale: Keras calculates the loss per sample and not for the full dataset. Therefore,\n",
    "        we need to scale the KL(q||p) loss down to a single sample, which means setting kl_weight_scale = 1/n_datapoints\n",
    "        \"\"\"\n",
    "        dist_layer = InverseNormalizingFlowLayer(\n",
    "            flow_types=[\"radial\"] * n_flows, n_dims=n_dims, trainable_base_dist=trainable_base_dist\n",
    "        )\n",
    "        super().__init__(dist_layer, kl_weight_scale, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_function(\n",
    "        n_dims,\n",
    "        kl_weight_scale,\n",
    "        n_flows=2,\n",
    "        trainable_base_dist=True,\n",
    "        kl_use_exact=True,\n",
    "        hidden_sizes=(10,),\n",
    "        activation=\"tanh\",\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        learning_rate=2e-2,\n",
    "        trainable_prior=False,\n",
    "        map_mode=False,\n",
    "        prior_scale=1.0,\n",
    "    ):\n",
    "        # this is necessary, else there'll be processes hanging around hogging memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        return BayesNormalizingFlowNetwork(\n",
    "            n_dims=n_dims,\n",
    "            kl_weight_scale=kl_weight_scale,\n",
    "            n_flows=n_flows,\n",
    "            trainable_base_dist=trainable_base_dist,\n",
    "            kl_use_exact=kl_use_exact,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            activation=activation,\n",
    "            noise_reg=noise_reg,\n",
    "            learning_rate=learning_rate,\n",
    "            trainable_prior=trainable_prior,\n",
    "            map_mode=map_mode,\n",
    "            prior_scale=prior_scale,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65992896-1ca4-4c63-8775-7485fe274321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaximumLikelihoodNNEstimator(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dist_layer,\n",
    "        hidden_sizes=(16, 16),\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        learning_rate=3e-3,\n",
    "        activation=\"relu\",\n",
    "        random_seed=22,\n",
    "    ):\n",
    "        assert len(noise_reg) == 2\n",
    "\n",
    "        dense_layers = self._get_dense_layers(\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=dist_layer.get_total_param_size(),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            dense_layers + [dist_layer],\n",
    "            noise_fn_type=noise_reg[0],\n",
    "            noise_scale_factor=noise_reg[1],\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "\n",
    "        self.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate), loss=self._get_neg_log_likelihood()\n",
    "        )\n",
    "\n",
    "    def _get_dense_layers(self, hidden_sizes, output_size, activation):\n",
    "        assert type(hidden_sizes) == tuple or type(hidden_sizes) == list\n",
    "        # the data normalization values are assigned once fit is called\n",
    "        normalization = [tf.keras.layers.Lambda(lambda x: (x - self.x_mean) / (self.x_std + 1e-8))]\n",
    "        noise_reg = [tf.keras.layers.GaussianNoise(self.x_noise_std)]\n",
    "        hidden = [tf.keras.layers.Dense(size, activation=activation) for size in hidden_sizes]\n",
    "        output = [tf.keras.layers.Dense(output_size, activation=\"linear\")]\n",
    "        return normalization + noise_reg + hidden + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a97995fc-130a-4b21-a8dd-98b39f940647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class NormalizingFlowNetwork(MaximumLikelihoodNNEstimator):\n",
    "    def __init__(self, n_dims, n_flows=10, trainable_base_dist=True, **kwargs):\n",
    "        \"\"\"\n",
    "        :param n_dims: Dimensionsion of Y. The dimension of X is automatically inferred from the data\n",
    "        :param n_flows: The number of radial flows to use.\n",
    "        :param trainable_base_dist: Whether the standard normal base dist has trainable mean + diag. convariance\n",
    "        \"\"\"\n",
    "        dist_layer = InverseNormalizingFlowLayer(\n",
    "            flow_types=[\"radial\"] * n_flows, n_dims=n_dims, trainable_base_dist=trainable_base_dist\n",
    "        )\n",
    "        super().__init__(dist_layer, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_function(\n",
    "        n_dims=1,\n",
    "        n_flows=3,\n",
    "        hidden_sizes=(16, 16),\n",
    "        trainable_base_dist=True,\n",
    "        noise_reg=(\"fixed_rate\", 0.0),\n",
    "        learning_rate=3e-3,\n",
    "        activation=\"tanh\",\n",
    "    ):\n",
    "        # this is necessary, else there'll be processes hanging around hogging memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        return NormalizingFlowNetwork(\n",
    "            n_dims=n_dims,\n",
    "            n_flows=n_flows,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            trainable_base_dist=trainable_base_dist,\n",
    "            noise_reg=noise_reg,\n",
    "            learning_rate=learning_rate,\n",
    "            activation=activation,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024be78-1775-48f0-a82b-c35059acab44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94580e6-639e-4e81-8e00-5d05fdf42067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d66b5d-ffc9-42e6-82e3-909245a0907b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "640b9386-fbe4-488a-a15c-7cf1596d369f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import pytest\n",
    "\n",
    "\n",
    "def flow_dimension_testing(flow_name):\n",
    "    FLOWS = {\"planar\": PlanarFlow, \"radial\": RadialFlow, \"affine\": AffineFlow}\n",
    "    # all are tested against Affine Flow, since that's the reference implementation\n",
    "    batch_size = 10\n",
    "    for dim in [1, 4]:\n",
    "        flow_class = FLOWS[flow_name]\n",
    "        # test dimension of parameter space\n",
    "        with pytest.raises(AssertionError):\n",
    "            flow = flow_class(tf.ones((batch_size, flow_class.get_param_size(dim) + 1)), dim)\n",
    "\n",
    "        flow = flow_class(tf.ones((batch_size, flow_class.get_param_size(dim))), dim)\n",
    "        reference = AffineFlow(tf.ones((batch_size, AffineFlow.get_param_size(dim))), dim)\n",
    "\n",
    "        test_tensors = [[[0.0] * dim], [[1.0] * dim] * batch_size]\n",
    "        assert flow.forward_min_event_ndims == reference.forward_min_event_ndims\n",
    "        for tensor in test_tensors:\n",
    "            assert flow.forward(tensor).shape == reference.forward(tensor).shape\n",
    "            assert (\n",
    "                flow._forward_log_det_jacobian(tensor).shape\n",
    "                == reference._forward_log_det_jacobian(tensor).shape\n",
    "            )\n",
    "\n",
    "        tensor = [[1.0] * dim] + ([[0.0] * dim] * (batch_size - 2)) + [[1.0] * dim]\n",
    "        res = flow.forward(tensor).numpy()\n",
    "        assert res[0] == pytest.approx(res[-1], rel=1e-5)\n",
    "        assert res[1] == pytest.approx(res[-2], rel=1e-5)\n",
    "        assert not all(res[0] == res[1])\n",
    "\n",
    "        tensor = [[1.0] * dim] + ([[0.0] * dim] * (batch_size - 2)) + [[1.0] * dim]\n",
    "        res = flow._forward_log_det_jacobian(tensor).numpy()\n",
    "        assert res[0] == pytest.approx(res[-1], rel=1e-5)\n",
    "        assert res[1] == pytest.approx(res[-2], rel=1e-5)\n",
    "        assert not res[0] == pytest.approx(res[1])\n",
    "\n",
    "\n",
    "def test_planar():\n",
    "    flow_dimension_testing(\"planar\")\n",
    "\n",
    "\n",
    "def test_radial():\n",
    "    flow_dimension_testing(\"radial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d08e49a-50a5-4f01-8af9-c61d07a7630a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Affine.__init__() got an unexpected keyword argument 'shift'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_radial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 46\u001b[0m, in \u001b[0;36mtest_radial\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_radial\u001b[39m():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mflow_dimension_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mradial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 17\u001b[0m, in \u001b[0;36mflow_dimension_testing\u001b[0;34m(flow_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)), dim)\n\u001b[1;32m     16\u001b[0m flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n\u001b[0;32m---> 17\u001b[0m reference \u001b[38;5;241m=\u001b[39m \u001b[43mAffineFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_tensors \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim], [[\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;241m*\u001b[39m dim] \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mforward_min_event_ndims \u001b[38;5;241m==\u001b[39m reference\u001b[38;5;241m.\u001b[39mforward_min_event_ndims\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mAffineFlow.__init__\u001b[0;34m(self, t, n_dims, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, n_dims, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAffineFlow\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n_dims\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAffineFlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_diag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Affine.__init__() got an unexpected keyword argument 'shift'"
     ]
    }
   ],
   "source": [
    "test_radial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97946900-bc4f-4bca-8faf-e7cfbf238d61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "name for name_scope must be a string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mflow_dimension_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPlanarFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m, in \u001b[0;36mflow_dimension_testing\u001b[0;34m(flow_class)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# test dimension of parameter space\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pytest\u001b[38;5;241m.\u001b[39mraises(\u001b[38;5;167;01mAssertionError\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m         flow \u001b[38;5;241m=\u001b[39m \u001b[43mflow_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     flow \u001b[38;5;241m=\u001b[39m flow_class(tf\u001b[38;5;241m.\u001b[39mones((batch_size, flow_class\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n\u001b[1;32m     15\u001b[0m     reference \u001b[38;5;241m=\u001b[39m AffineFlow(tf\u001b[38;5;241m.\u001b[39mones((batch_size, AffineFlow\u001b[38;5;241m.\u001b[39mget_param_size(dim))), dim)\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:913\u001b[0m, in \u001b[0;36mBijector.__call__\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Bijector):\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mChain([\u001b[38;5;28mself\u001b[39m, value], name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:1326\u001b[0m, in \u001b[0;36mBijector.forward\u001b[0;34m(self, x, name, **kwargs)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the forward `Bijector` evaluation, i.e., X = g(Y).\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;124;03m    NotImplementedError: if `_forward` is not implemented.\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:1300\u001b[0m, in \u001b[0;36mBijector._call_forward\u001b[0;34m(self, x, name, **kwargs)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Wraps call to _forward, allowing extra shared logic.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1300\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(name):\n\u001b[1;32m   1301\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_dtype(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1302\u001b[0m     x \u001b[38;5;241m=\u001b[39m nest_util\u001b[38;5;241m.\u001b[39mconvert_to_nested_tensor(\n\u001b[1;32m   1303\u001b[0m         x, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype_hint\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1304\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m SKIP_DTYPE_CHECKS \u001b[38;5;28;01melse\u001b[39;00m dtype,\n\u001b[1;32m   1305\u001b[0m         allow_packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow_probability/python/bijectors/bijector.py:1816\u001b[0m, in \u001b[0;36mBijector._name_and_control_scope\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to standardize op scope.\"\"\"\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m name_util\u001b[38;5;241m.\u001b[39minstance_scope(\n\u001b[1;32m   1814\u001b[0m     instance_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1815\u001b[0m     constructor_name_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_name_scope):\n\u001b[0;32m-> 1816\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m name_scope:\n\u001b[1;32m   1817\u001b[0m     deps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defer_all_assertions:\n",
      "File \u001b[0;32m/data/astro/scratch/lcabayol/anaconda3/envs/NFlow/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:6423\u001b[0m, in \u001b[0;36mname_scope_v2.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6414\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the context manager.\u001b[39;00m\n\u001b[1;32m   6415\u001b[0m \n\u001b[1;32m   6416\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6420\u001b[0m \u001b[38;5;124;03m  ValueError: If name is not a string.\u001b[39;00m\n\u001b[1;32m   6421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 6423\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname for name_scope must be a string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m   6425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exit_fns \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: name for name_scope must be a string."
     ]
    }
   ],
   "source": [
    "flow_dimension_testing(PlanarFlow(t=t, n_dims=ndims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c58f187a-4d63-45e9-bd80-8349e94edd63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = tf.random.normal(shape=(1, 5))\n",
    "ndims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41869a9e-26ea-4729-a19f-3463eca5f4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6c787-4142-4992-8a39-4f358aa43bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Implements a bijector x = y + (alpha * beta * (y - y_0)) / (alpha + abs(y - y_0)).\n",
    "    Args:\n",
    "        params: Tensor shape (?, n_dims+2). This will be split into the parameters\n",
    "            alpha (?, 1), beta (?, 1), gamma (?, n_dims).\n",
    "            Furthermore alpha will be constrained to assure the invertability of the flow\n",
    "        n_dims: The dimension of the distribution that will be transformed\n",
    "        name: The name to give this particular flow\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215fb29-b609-4def-afa5-a610b188fadf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFlows",
   "language": "python",
   "name": "nflows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
